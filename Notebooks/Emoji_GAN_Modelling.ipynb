{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2302aa94befd47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T19:32:27.904954Z",
     "start_time": "2025-03-20T16:03:22.338189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Mac GPU (MPS)\n",
      "Epoch [1/5000] | D Loss: 0.4286 | G Loss: 3.0351\n",
      "Epoch [2/5000] | D Loss: 0.4216 | G Loss: 3.3006\n",
      "Epoch [3/5000] | D Loss: 0.4255 | G Loss: 3.5396\n",
      "Epoch [4/5000] | D Loss: 0.4117 | G Loss: 2.8643\n",
      "Epoch [5/5000] | D Loss: 0.4165 | G Loss: 3.2905\n",
      "Epoch [6/5000] | D Loss: 0.6215 | G Loss: 2.0187\n",
      "Epoch [7/5000] | D Loss: 0.7661 | G Loss: 1.6057\n",
      "Epoch [8/5000] | D Loss: 0.6466 | G Loss: 2.0053\n",
      "Epoch [9/5000] | D Loss: 0.5729 | G Loss: 1.6640\n",
      "Epoch [10/5000] | D Loss: 0.7564 | G Loss: 1.8983\n",
      "Epoch [11/5000] | D Loss: 0.5181 | G Loss: 2.0073\n",
      "Epoch [12/5000] | D Loss: 0.6223 | G Loss: 2.6089\n",
      "Epoch [13/5000] | D Loss: 0.9152 | G Loss: 0.9754\n",
      "Epoch [14/5000] | D Loss: 0.5698 | G Loss: 2.1877\n",
      "Epoch [15/5000] | D Loss: 0.6705 | G Loss: 2.1140\n",
      "Epoch [16/5000] | D Loss: 0.4597 | G Loss: 2.3484\n",
      "Epoch [17/5000] | D Loss: 0.7280 | G Loss: 1.7560\n",
      "Epoch [18/5000] | D Loss: 0.5915 | G Loss: 1.7719\n",
      "Epoch [19/5000] | D Loss: 0.4946 | G Loss: 2.9721\n",
      "Epoch [20/5000] | D Loss: 0.6752 | G Loss: 2.8391\n",
      "Epoch [21/5000] | D Loss: 0.7490 | G Loss: 0.9424\n",
      "Epoch [22/5000] | D Loss: 0.4589 | G Loss: 2.7115\n",
      "Epoch [23/5000] | D Loss: 0.6714 | G Loss: 1.4150\n",
      "Epoch [24/5000] | D Loss: 0.5097 | G Loss: 2.5712\n",
      "Epoch [25/5000] | D Loss: 0.8309 | G Loss: 2.5995\n",
      "Epoch [26/5000] | D Loss: 0.7878 | G Loss: 1.5265\n",
      "Epoch [27/5000] | D Loss: 0.5840 | G Loss: 2.3386\n",
      "Epoch [28/5000] | D Loss: 1.1540 | G Loss: 0.9167\n",
      "Epoch [29/5000] | D Loss: 0.4968 | G Loss: 1.7566\n",
      "Epoch [30/5000] | D Loss: 1.5445 | G Loss: 1.1053\n",
      "Epoch [31/5000] | D Loss: 0.5593 | G Loss: 2.2172\n",
      "Epoch [32/5000] | D Loss: 0.5969 | G Loss: 1.4389\n",
      "Epoch [33/5000] | D Loss: 0.5127 | G Loss: 1.8642\n",
      "Epoch [34/5000] | D Loss: 0.8240 | G Loss: 2.0029\n",
      "Epoch [35/5000] | D Loss: 0.5695 | G Loss: 2.7386\n",
      "Epoch [36/5000] | D Loss: 0.7283 | G Loss: 1.0115\n",
      "Epoch [37/5000] | D Loss: 0.5728 | G Loss: 2.1569\n",
      "Epoch [38/5000] | D Loss: 0.7602 | G Loss: 2.2366\n",
      "Epoch [39/5000] | D Loss: 0.6960 | G Loss: 2.2339\n",
      "Epoch [40/5000] | D Loss: 0.7427 | G Loss: 1.1749\n",
      "Epoch [41/5000] | D Loss: 0.6062 | G Loss: 2.5387\n",
      "Epoch [42/5000] | D Loss: 0.7181 | G Loss: 2.7478\n",
      "Epoch [43/5000] | D Loss: 0.5107 | G Loss: 2.9439\n",
      "Epoch [44/5000] | D Loss: 0.6622 | G Loss: 1.3098\n",
      "Epoch [45/5000] | D Loss: 0.5850 | G Loss: 1.5619\n",
      "Epoch [46/5000] | D Loss: 0.5131 | G Loss: 2.2134\n",
      "Epoch [47/5000] | D Loss: 1.0314 | G Loss: 0.6073\n",
      "Epoch [48/5000] | D Loss: 0.9508 | G Loss: 0.4279\n",
      "Epoch [49/5000] | D Loss: 0.9654 | G Loss: 0.8897\n",
      "Epoch [50/5000] | D Loss: 0.5847 | G Loss: 1.9162\n",
      "Epoch [51/5000] | D Loss: 0.5657 | G Loss: 2.1792\n",
      "Epoch [52/5000] | D Loss: 0.5691 | G Loss: 1.5100\n",
      "Epoch [53/5000] | D Loss: 1.1219 | G Loss: 2.9262\n",
      "Epoch [54/5000] | D Loss: 0.6874 | G Loss: 2.3984\n",
      "Epoch [55/5000] | D Loss: 0.4358 | G Loss: 2.7593\n",
      "Epoch [56/5000] | D Loss: 0.5533 | G Loss: 2.3527\n",
      "Epoch [57/5000] | D Loss: 0.6099 | G Loss: 1.6347\n",
      "Epoch [58/5000] | D Loss: 0.7044 | G Loss: 1.7787\n",
      "Epoch [59/5000] | D Loss: 0.5075 | G Loss: 2.1604\n",
      "Epoch [60/5000] | D Loss: 0.6299 | G Loss: 2.7395\n",
      "Epoch [61/5000] | D Loss: 1.5309 | G Loss: 1.1711\n",
      "Epoch [62/5000] | D Loss: 0.4781 | G Loss: 2.3601\n",
      "Epoch [63/5000] | D Loss: 0.4838 | G Loss: 2.9225\n",
      "Epoch [64/5000] | D Loss: 0.8985 | G Loss: 3.5175\n",
      "Epoch [65/5000] | D Loss: 0.9861 | G Loss: 0.4374\n",
      "Epoch [66/5000] | D Loss: 0.4976 | G Loss: 2.9495\n",
      "Epoch [67/5000] | D Loss: 0.6688 | G Loss: 1.4307\n",
      "Epoch [68/5000] | D Loss: 0.4504 | G Loss: 3.1168\n",
      "Epoch [69/5000] | D Loss: 0.4817 | G Loss: 2.9994\n",
      "Epoch [70/5000] | D Loss: 0.5046 | G Loss: 2.4889\n",
      "Epoch [71/5000] | D Loss: 0.5156 | G Loss: 2.8646\n",
      "Epoch [72/5000] | D Loss: 0.6793 | G Loss: 2.1605\n",
      "Epoch [73/5000] | D Loss: 0.4565 | G Loss: 3.5017\n",
      "Epoch [74/5000] | D Loss: 0.4702 | G Loss: 3.7565\n",
      "Epoch [75/5000] | D Loss: 0.4726 | G Loss: 3.0086\n",
      "Epoch [76/5000] | D Loss: 0.5509 | G Loss: 1.4084\n",
      "Epoch [77/5000] | D Loss: 0.5518 | G Loss: 2.5437\n",
      "Epoch [78/5000] | D Loss: 0.4667 | G Loss: 2.0052\n",
      "Epoch [79/5000] | D Loss: 0.5622 | G Loss: 3.7964\n",
      "Epoch [80/5000] | D Loss: 0.7415 | G Loss: 3.2531\n",
      "Epoch [81/5000] | D Loss: 0.5871 | G Loss: 2.6011\n",
      "Epoch [82/5000] | D Loss: 0.6328 | G Loss: 1.4486\n",
      "Epoch [83/5000] | D Loss: 0.8196 | G Loss: 1.2652\n",
      "Epoch [84/5000] | D Loss: 0.6739 | G Loss: 2.1272\n",
      "Epoch [85/5000] | D Loss: 0.7518 | G Loss: 1.4164\n",
      "Epoch [86/5000] | D Loss: 0.4619 | G Loss: 3.0067\n",
      "Epoch [87/5000] | D Loss: 0.6587 | G Loss: 1.1231\n",
      "Epoch [88/5000] | D Loss: 0.4648 | G Loss: 2.3201\n",
      "Epoch [89/5000] | D Loss: 0.7499 | G Loss: 2.9812\n",
      "Epoch [90/5000] | D Loss: 0.4600 | G Loss: 3.0761\n",
      "Epoch [91/5000] | D Loss: 0.5191 | G Loss: 2.0269\n",
      "Epoch [92/5000] | D Loss: 0.4781 | G Loss: 2.9684\n",
      "Epoch [93/5000] | D Loss: 1.0533 | G Loss: 0.2696\n",
      "Epoch [94/5000] | D Loss: 0.6216 | G Loss: 2.2390\n",
      "Epoch [95/5000] | D Loss: 0.5365 | G Loss: 2.1075\n",
      "Epoch [96/5000] | D Loss: 0.6120 | G Loss: 1.3269\n",
      "Epoch [97/5000] | D Loss: 0.4665 | G Loss: 2.4429\n",
      "Epoch [98/5000] | D Loss: 0.5818 | G Loss: 3.1292\n",
      "Epoch [99/5000] | D Loss: 0.9039 | G Loss: 0.5469\n",
      "Epoch [100/5000] | D Loss: 0.6024 | G Loss: 1.9416\n",
      "Epoch 100 FID Score: 253.6074\n",
      "Epoch [101/5000] | D Loss: 0.5338 | G Loss: 1.8764\n",
      "Epoch [102/5000] | D Loss: 0.4953 | G Loss: 2.8299\n",
      "Epoch [103/5000] | D Loss: 0.8397 | G Loss: 1.1681\n",
      "Epoch [104/5000] | D Loss: 0.9042 | G Loss: 0.3768\n",
      "Epoch [105/5000] | D Loss: 0.6828 | G Loss: 1.1330\n",
      "Epoch [106/5000] | D Loss: 0.7141 | G Loss: 1.5725\n",
      "Epoch [107/5000] | D Loss: 1.0568 | G Loss: 0.6672\n",
      "Epoch [108/5000] | D Loss: 0.6658 | G Loss: 3.2656\n",
      "Epoch [109/5000] | D Loss: 0.5418 | G Loss: 3.0686\n",
      "Epoch [110/5000] | D Loss: 0.8172 | G Loss: 0.9031\n",
      "Epoch [111/5000] | D Loss: 0.5208 | G Loss: 1.9898\n",
      "Epoch [112/5000] | D Loss: 0.5298 | G Loss: 2.0583\n",
      "Epoch [113/5000] | D Loss: 0.4971 | G Loss: 2.1225\n",
      "Epoch [114/5000] | D Loss: 0.6226 | G Loss: 4.0174\n",
      "Epoch [115/5000] | D Loss: 0.4966 | G Loss: 2.2013\n",
      "Epoch [116/5000] | D Loss: 1.1281 | G Loss: 0.3260\n",
      "Epoch [117/5000] | D Loss: 0.5093 | G Loss: 2.8093\n",
      "Epoch [118/5000] | D Loss: 0.7682 | G Loss: 2.4677\n",
      "Epoch [119/5000] | D Loss: 0.5972 | G Loss: 1.9185\n",
      "Epoch [120/5000] | D Loss: 0.5773 | G Loss: 3.1579\n",
      "Epoch [121/5000] | D Loss: 0.4668 | G Loss: 2.9413\n",
      "Epoch [122/5000] | D Loss: 0.4498 | G Loss: 2.9283\n",
      "Epoch [123/5000] | D Loss: 0.7719 | G Loss: 0.7255\n",
      "Epoch [124/5000] | D Loss: 0.4616 | G Loss: 2.9057\n",
      "Epoch [125/5000] | D Loss: 0.5529 | G Loss: 3.7475\n",
      "Epoch [126/5000] | D Loss: 0.4881 | G Loss: 2.3958\n",
      "Epoch [127/5000] | D Loss: 0.5468 | G Loss: 1.1001\n",
      "Epoch [128/5000] | D Loss: 0.7795 | G Loss: 1.1639\n",
      "Epoch [129/5000] | D Loss: 0.6173 | G Loss: 1.5264\n",
      "Epoch [130/5000] | D Loss: 0.4687 | G Loss: 2.6637\n",
      "Epoch [131/5000] | D Loss: 0.4702 | G Loss: 2.3057\n",
      "Epoch [132/5000] | D Loss: 0.7448 | G Loss: 0.7780\n",
      "Epoch [133/5000] | D Loss: 0.5816 | G Loss: 3.7861\n",
      "Epoch [134/5000] | D Loss: 0.5075 | G Loss: 3.5023\n",
      "Epoch [135/5000] | D Loss: 0.5178 | G Loss: 2.1811\n",
      "Epoch [136/5000] | D Loss: 0.5252 | G Loss: 2.4224\n",
      "Epoch [137/5000] | D Loss: 0.4990 | G Loss: 2.5565\n",
      "Epoch [138/5000] | D Loss: 0.6142 | G Loss: 1.0771\n",
      "Epoch [139/5000] | D Loss: 0.5225 | G Loss: 2.8016\n",
      "Epoch [140/5000] | D Loss: 0.4718 | G Loss: 3.3139\n",
      "Epoch [141/5000] | D Loss: 0.5680 | G Loss: 1.1242\n",
      "Epoch [142/5000] | D Loss: 0.7465 | G Loss: 3.6432\n",
      "Epoch [143/5000] | D Loss: 0.6690 | G Loss: 1.4969\n",
      "Epoch [144/5000] | D Loss: 0.6128 | G Loss: 2.7949\n",
      "Epoch [145/5000] | D Loss: 0.5575 | G Loss: 3.6592\n",
      "Epoch [146/5000] | D Loss: 1.1955 | G Loss: 0.6469\n",
      "Epoch [147/5000] | D Loss: 0.5294 | G Loss: 1.9427\n",
      "Epoch [148/5000] | D Loss: 0.5875 | G Loss: 3.2457\n",
      "Epoch [149/5000] | D Loss: 0.5848 | G Loss: 2.3500\n",
      "Epoch [150/5000] | D Loss: 0.4586 | G Loss: 2.6095\n",
      "Epoch [151/5000] | D Loss: 0.5980 | G Loss: 1.3678\n",
      "Epoch [152/5000] | D Loss: 0.4570 | G Loss: 3.4649\n",
      "Epoch [153/5000] | D Loss: 0.6467 | G Loss: 0.9834\n",
      "Epoch [154/5000] | D Loss: 0.5462 | G Loss: 1.5247\n",
      "Epoch [155/5000] | D Loss: 0.4751 | G Loss: 2.5381\n",
      "Epoch [156/5000] | D Loss: 0.5006 | G Loss: 4.6871\n",
      "Epoch [157/5000] | D Loss: 1.0035 | G Loss: 0.3614\n",
      "Epoch [158/5000] | D Loss: 0.4987 | G Loss: 2.2791\n",
      "Epoch [159/5000] | D Loss: 0.5193 | G Loss: 1.5741\n",
      "Epoch [160/5000] | D Loss: 0.4519 | G Loss: 2.6508\n",
      "Epoch [161/5000] | D Loss: 0.5899 | G Loss: 3.3961\n",
      "Epoch [162/5000] | D Loss: 0.4438 | G Loss: 2.8002\n",
      "Epoch [163/5000] | D Loss: 0.7978 | G Loss: 1.3400\n",
      "Epoch [164/5000] | D Loss: 0.4649 | G Loss: 2.2470\n",
      "Epoch [165/5000] | D Loss: 0.8626 | G Loss: 0.4532\n",
      "Epoch [166/5000] | D Loss: 0.7036 | G Loss: 1.5315\n",
      "Epoch [167/5000] | D Loss: 0.4760 | G Loss: 2.4667\n",
      "Epoch [168/5000] | D Loss: 0.5903 | G Loss: 1.7645\n",
      "Epoch [169/5000] | D Loss: 0.4510 | G Loss: 2.9163\n",
      "Epoch [170/5000] | D Loss: 0.4646 | G Loss: 3.5134\n",
      "Epoch [171/5000] | D Loss: 0.5068 | G Loss: 2.9964\n",
      "Epoch [172/5000] | D Loss: 0.5309 | G Loss: 1.9219\n",
      "Epoch [173/5000] | D Loss: 0.4510 | G Loss: 3.5786\n",
      "Epoch [174/5000] | D Loss: 0.4573 | G Loss: 3.7462\n",
      "Epoch [175/5000] | D Loss: 0.6202 | G Loss: 1.4364\n",
      "Epoch [176/5000] | D Loss: 0.7309 | G Loss: 0.4138\n",
      "Epoch [177/5000] | D Loss: 0.4925 | G Loss: 2.0121\n",
      "Epoch [178/5000] | D Loss: 0.5246 | G Loss: 2.1068\n",
      "Epoch [179/5000] | D Loss: 0.5665 | G Loss: 3.3117\n",
      "Epoch [180/5000] | D Loss: 0.6027 | G Loss: 3.4082\n",
      "Epoch [181/5000] | D Loss: 0.4583 | G Loss: 2.5925\n",
      "Epoch [182/5000] | D Loss: 0.4487 | G Loss: 2.9023\n",
      "Epoch [183/5000] | D Loss: 0.4476 | G Loss: 2.7362\n",
      "Epoch [184/5000] | D Loss: 0.4653 | G Loss: 3.1324\n",
      "Epoch [185/5000] | D Loss: 0.7278 | G Loss: 1.2469\n",
      "Epoch [186/5000] | D Loss: 0.4589 | G Loss: 2.7291\n",
      "Epoch [187/5000] | D Loss: 0.4405 | G Loss: 2.5583\n",
      "Epoch [188/5000] | D Loss: 0.4792 | G Loss: 2.5127\n",
      "Epoch [189/5000] | D Loss: 0.4464 | G Loss: 1.9742\n",
      "Epoch [190/5000] | D Loss: 0.5034 | G Loss: 2.8940\n",
      "Epoch [191/5000] | D Loss: 1.0472 | G Loss: 0.2683\n",
      "Epoch [192/5000] | D Loss: 0.9501 | G Loss: 0.9683\n",
      "Epoch [193/5000] | D Loss: 0.5560 | G Loss: 1.4437\n",
      "Epoch [194/5000] | D Loss: 0.4522 | G Loss: 2.8866\n",
      "Epoch [195/5000] | D Loss: 0.6058 | G Loss: 1.4488\n",
      "Epoch [196/5000] | D Loss: 0.6432 | G Loss: 1.3144\n",
      "Epoch [197/5000] | D Loss: 0.4542 | G Loss: 2.7894\n",
      "Epoch [198/5000] | D Loss: 0.5063 | G Loss: 2.0577\n",
      "Epoch [199/5000] | D Loss: 0.5287 | G Loss: 1.8066\n",
      "Epoch [200/5000] | D Loss: 0.4824 | G Loss: 2.8783\n",
      "Epoch 200 FID Score: 211.7093\n",
      "Epoch [201/5000] | D Loss: 0.5066 | G Loss: 2.6144\n",
      "Epoch [202/5000] | D Loss: 0.4522 | G Loss: 3.5054\n",
      "Epoch [203/5000] | D Loss: 0.4867 | G Loss: 2.2388\n",
      "Epoch [204/5000] | D Loss: 0.5708 | G Loss: 2.4172\n",
      "Epoch [205/5000] | D Loss: 0.4784 | G Loss: 2.3570\n",
      "Epoch [206/5000] | D Loss: 0.6510 | G Loss: 0.7683\n",
      "Epoch [207/5000] | D Loss: 0.4554 | G Loss: 2.9284\n",
      "Epoch [208/5000] | D Loss: 0.4543 | G Loss: 3.1908\n",
      "Epoch [209/5000] | D Loss: 0.4493 | G Loss: 3.2162\n",
      "Epoch [210/5000] | D Loss: 0.4640 | G Loss: 2.6398\n",
      "Epoch [211/5000] | D Loss: 0.5202 | G Loss: 3.1551\n",
      "Epoch [212/5000] | D Loss: 0.4993 | G Loss: 3.2845\n",
      "Epoch [213/5000] | D Loss: 0.5074 | G Loss: 1.8526\n",
      "Epoch [214/5000] | D Loss: 0.8701 | G Loss: 0.5018\n",
      "Epoch [215/5000] | D Loss: 0.4932 | G Loss: 2.4409\n",
      "Epoch [216/5000] | D Loss: 0.6423 | G Loss: 1.4145\n",
      "Epoch [217/5000] | D Loss: 0.5072 | G Loss: 1.9089\n",
      "Epoch [218/5000] | D Loss: 0.4852 | G Loss: 2.0727\n",
      "Epoch [219/5000] | D Loss: 0.4629 | G Loss: 3.8075\n",
      "Epoch [220/5000] | D Loss: 0.5515 | G Loss: 2.4180\n",
      "Epoch [221/5000] | D Loss: 0.4729 | G Loss: 3.4251\n",
      "Epoch [222/5000] | D Loss: 0.9282 | G Loss: 0.3601\n",
      "Epoch [223/5000] | D Loss: 0.4486 | G Loss: 3.4981\n",
      "Epoch [224/5000] | D Loss: 1.1413 | G Loss: 0.2926\n",
      "Epoch [225/5000] | D Loss: 0.4571 | G Loss: 2.9542\n",
      "Epoch [226/5000] | D Loss: 0.4956 | G Loss: 2.8925\n",
      "Epoch [227/5000] | D Loss: 0.4723 | G Loss: 3.2431\n",
      "Epoch [228/5000] | D Loss: 0.5328 | G Loss: 2.3210\n",
      "Epoch [229/5000] | D Loss: 0.4647 | G Loss: 2.6622\n",
      "Epoch [230/5000] | D Loss: 0.4620 | G Loss: 2.4539\n",
      "Epoch [231/5000] | D Loss: 0.4564 | G Loss: 2.7322\n",
      "Epoch [232/5000] | D Loss: 0.4881 | G Loss: 3.5457\n",
      "Epoch [233/5000] | D Loss: 0.4440 | G Loss: 3.3480\n",
      "Epoch [234/5000] | D Loss: 0.4346 | G Loss: 2.9199\n",
      "Epoch [235/5000] | D Loss: 0.6193 | G Loss: 0.9108\n",
      "Epoch [236/5000] | D Loss: 0.4969 | G Loss: 1.9299\n",
      "Epoch [237/5000] | D Loss: 0.4876 | G Loss: 3.2078\n",
      "Epoch [238/5000] | D Loss: 0.4391 | G Loss: 3.0497\n",
      "Epoch [239/5000] | D Loss: 0.7521 | G Loss: 1.2015\n",
      "Epoch [240/5000] | D Loss: 0.4942 | G Loss: 3.2236\n",
      "Epoch [241/5000] | D Loss: 0.5913 | G Loss: 1.3508\n",
      "Epoch [242/5000] | D Loss: 0.5746 | G Loss: 2.2099\n",
      "Epoch [243/5000] | D Loss: 0.7051 | G Loss: 1.1005\n",
      "Epoch [244/5000] | D Loss: 0.4704 | G Loss: 2.5276\n",
      "Epoch [245/5000] | D Loss: 0.4452 | G Loss: 3.1077\n",
      "Epoch [246/5000] | D Loss: 0.5628 | G Loss: 2.7230\n",
      "Epoch [247/5000] | D Loss: 0.4861 | G Loss: 1.6762\n",
      "Epoch [248/5000] | D Loss: 0.4703 | G Loss: 3.7997\n",
      "Epoch [249/5000] | D Loss: 0.6305 | G Loss: 1.4706\n",
      "Epoch [250/5000] | D Loss: 0.4437 | G Loss: 3.2410\n",
      "Epoch [251/5000] | D Loss: 0.4679 | G Loss: 3.0843\n",
      "Epoch [252/5000] | D Loss: 0.4670 | G Loss: 2.9236\n",
      "Epoch [253/5000] | D Loss: 0.6049 | G Loss: 1.6472\n",
      "Epoch [254/5000] | D Loss: 0.4901 | G Loss: 2.5171\n",
      "Epoch [255/5000] | D Loss: 0.5720 | G Loss: 2.4559\n",
      "Epoch [256/5000] | D Loss: 0.5265 | G Loss: 1.8962\n",
      "Epoch [257/5000] | D Loss: 0.4727 | G Loss: 3.4158\n",
      "Epoch [258/5000] | D Loss: 0.5147 | G Loss: 1.5891\n",
      "Epoch [259/5000] | D Loss: 0.6294 | G Loss: 0.9356\n",
      "Epoch [260/5000] | D Loss: 0.4516 | G Loss: 2.8363\n",
      "Epoch [261/5000] | D Loss: 0.4923 | G Loss: 2.6010\n",
      "Epoch [262/5000] | D Loss: 0.5607 | G Loss: 1.8223\n",
      "Epoch [263/5000] | D Loss: 0.6934 | G Loss: 1.1604\n",
      "Epoch [264/5000] | D Loss: 0.4919 | G Loss: 1.6440\n",
      "Epoch [265/5000] | D Loss: 0.4868 | G Loss: 3.6560\n",
      "Epoch [266/5000] | D Loss: 0.4700 | G Loss: 2.8960\n",
      "Epoch [267/5000] | D Loss: 0.7615 | G Loss: 0.7789\n",
      "Epoch [268/5000] | D Loss: 0.4476 | G Loss: 3.4647\n",
      "Epoch [269/5000] | D Loss: 0.6216 | G Loss: 1.3439\n",
      "Epoch [270/5000] | D Loss: 0.6236 | G Loss: 1.3792\n",
      "Epoch [271/5000] | D Loss: 0.4818 | G Loss: 3.9302\n",
      "Epoch [272/5000] | D Loss: 0.5693 | G Loss: 1.4621\n",
      "Epoch [273/5000] | D Loss: 0.5321 | G Loss: 2.1860\n",
      "Epoch [274/5000] | D Loss: 0.5672 | G Loss: 3.3436\n",
      "Epoch [275/5000] | D Loss: 0.5529 | G Loss: 2.3552\n",
      "Epoch [276/5000] | D Loss: 0.5167 | G Loss: 3.3617\n",
      "Epoch [277/5000] | D Loss: 0.9227 | G Loss: 0.6233\n",
      "Epoch [278/5000] | D Loss: 1.0883 | G Loss: 0.4880\n",
      "Epoch [279/5000] | D Loss: 0.7516 | G Loss: 1.5858\n",
      "Epoch [280/5000] | D Loss: 0.5194 | G Loss: 2.5122\n",
      "Epoch [281/5000] | D Loss: 0.5051 | G Loss: 1.8926\n",
      "Epoch [282/5000] | D Loss: 0.4932 | G Loss: 3.0562\n",
      "Epoch [283/5000] | D Loss: 0.5385 | G Loss: 2.7017\n",
      "Epoch [284/5000] | D Loss: 0.4644 | G Loss: 2.6067\n",
      "Epoch [285/5000] | D Loss: 0.5510 | G Loss: 3.6188\n",
      "Epoch [286/5000] | D Loss: 0.4417 | G Loss: 2.7247\n",
      "Epoch [287/5000] | D Loss: 0.4425 | G Loss: 3.3346\n",
      "Epoch [288/5000] | D Loss: 0.4893 | G Loss: 2.6349\n",
      "Epoch [289/5000] | D Loss: 0.4574 | G Loss: 2.3311\n",
      "Epoch [290/5000] | D Loss: 0.4716 | G Loss: 2.9399\n",
      "Epoch [291/5000] | D Loss: 0.4440 | G Loss: 2.9799\n",
      "Epoch [292/5000] | D Loss: 0.4525 | G Loss: 3.6028\n",
      "Epoch [293/5000] | D Loss: 0.6153 | G Loss: 2.6103\n",
      "Epoch [294/5000] | D Loss: 0.4623 | G Loss: 3.6242\n",
      "Epoch [295/5000] | D Loss: 0.7333 | G Loss: 0.8743\n",
      "Epoch [296/5000] | D Loss: 0.6076 | G Loss: 1.3439\n",
      "Epoch [297/5000] | D Loss: 0.4617 | G Loss: 2.7692\n",
      "Epoch [298/5000] | D Loss: 0.4580 | G Loss: 2.5605\n",
      "Epoch [299/5000] | D Loss: 0.4526 | G Loss: 3.1699\n",
      "Epoch [300/5000] | D Loss: 0.4541 | G Loss: 3.5234\n",
      "Epoch 300 FID Score: 221.2568\n",
      "Epoch [301/5000] | D Loss: 0.4922 | G Loss: 2.8371\n",
      "Epoch [302/5000] | D Loss: 0.5952 | G Loss: 4.0204\n",
      "Epoch [303/5000] | D Loss: 0.9488 | G Loss: 0.3143\n",
      "Epoch [304/5000] | D Loss: 0.5487 | G Loss: 4.0151\n",
      "Epoch [305/5000] | D Loss: 0.4817 | G Loss: 3.5425\n",
      "Epoch [306/5000] | D Loss: 0.4515 | G Loss: 3.1027\n",
      "Epoch [307/5000] | D Loss: 1.2674 | G Loss: 0.3543\n",
      "Epoch [308/5000] | D Loss: 0.4515 | G Loss: 2.8268\n",
      "Epoch [309/5000] | D Loss: 0.4575 | G Loss: 2.5588\n",
      "Epoch [310/5000] | D Loss: 0.8071 | G Loss: 0.4528\n",
      "Epoch [311/5000] | D Loss: 0.4580 | G Loss: 3.0810\n",
      "Epoch [312/5000] | D Loss: 0.4775 | G Loss: 2.5175\n",
      "Epoch [313/5000] | D Loss: 0.4718 | G Loss: 2.7929\n",
      "Epoch [314/5000] | D Loss: 0.5220 | G Loss: 2.1126\n",
      "Epoch [315/5000] | D Loss: 0.4533 | G Loss: 3.2780\n",
      "Epoch [316/5000] | D Loss: 0.5381 | G Loss: 2.0915\n",
      "Epoch [317/5000] | D Loss: 0.4409 | G Loss: 3.6617\n",
      "Epoch [318/5000] | D Loss: 0.4770 | G Loss: 3.6074\n",
      "Epoch [319/5000] | D Loss: 0.4782 | G Loss: 3.3512\n",
      "Epoch [320/5000] | D Loss: 0.4637 | G Loss: 3.3603\n",
      "Epoch [321/5000] | D Loss: 0.5817 | G Loss: 1.1159\n",
      "Epoch [322/5000] | D Loss: 0.4837 | G Loss: 3.7795\n",
      "Epoch [323/5000] | D Loss: 0.4817 | G Loss: 2.4187\n",
      "Epoch [324/5000] | D Loss: 0.4936 | G Loss: 2.3530\n",
      "Epoch [325/5000] | D Loss: 0.4477 | G Loss: 2.3640\n",
      "Epoch [326/5000] | D Loss: 0.4684 | G Loss: 3.1606\n",
      "Epoch [327/5000] | D Loss: 0.4728 | G Loss: 3.4217\n",
      "Epoch [328/5000] | D Loss: 0.5470 | G Loss: 1.9885\n",
      "Epoch [329/5000] | D Loss: 1.1493 | G Loss: 0.3808\n",
      "Epoch [330/5000] | D Loss: 0.5150 | G Loss: 2.2453\n",
      "Epoch [331/5000] | D Loss: 0.4599 | G Loss: 2.4557\n",
      "Epoch [332/5000] | D Loss: 0.4743 | G Loss: 3.1849\n",
      "Epoch [333/5000] | D Loss: 0.4692 | G Loss: 3.1032\n",
      "Epoch [334/5000] | D Loss: 0.7239 | G Loss: 0.8454\n",
      "Epoch [335/5000] | D Loss: 0.5037 | G Loss: 3.3073\n",
      "Epoch [336/5000] | D Loss: 0.4573 | G Loss: 2.4671\n",
      "Epoch [337/5000] | D Loss: 0.5572 | G Loss: 2.3144\n",
      "Epoch [338/5000] | D Loss: 0.5784 | G Loss: 1.2970\n",
      "Epoch [339/5000] | D Loss: 0.4766 | G Loss: 3.2319\n",
      "Epoch [340/5000] | D Loss: 0.5476 | G Loss: 3.5797\n",
      "Epoch [341/5000] | D Loss: 0.4465 | G Loss: 2.4958\n",
      "Epoch [342/5000] | D Loss: 0.4924 | G Loss: 3.5862\n",
      "Epoch [343/5000] | D Loss: 0.6444 | G Loss: 1.6429\n",
      "Epoch [344/5000] | D Loss: 0.5038 | G Loss: 2.2080\n",
      "Epoch [345/5000] | D Loss: 0.5140 | G Loss: 3.2039\n",
      "Epoch [346/5000] | D Loss: 0.4642 | G Loss: 2.2843\n",
      "Epoch [347/5000] | D Loss: 0.4430 | G Loss: 2.7992\n",
      "Epoch [348/5000] | D Loss: 0.5603 | G Loss: 1.5620\n",
      "Epoch [349/5000] | D Loss: 0.4512 | G Loss: 3.0773\n",
      "Epoch [350/5000] | D Loss: 0.5723 | G Loss: 2.3272\n",
      "Epoch [351/5000] | D Loss: 0.4495 | G Loss: 3.1918\n",
      "Epoch [352/5000] | D Loss: 0.4671 | G Loss: 3.2684\n",
      "Epoch [353/5000] | D Loss: 0.5226 | G Loss: 3.3668\n",
      "Epoch [354/5000] | D Loss: 0.4553 | G Loss: 3.0472\n",
      "Epoch [355/5000] | D Loss: 0.5501 | G Loss: 3.7794\n",
      "Epoch [356/5000] | D Loss: 0.4802 | G Loss: 2.4985\n",
      "Epoch [357/5000] | D Loss: 0.6397 | G Loss: 0.8315\n",
      "Epoch [358/5000] | D Loss: 0.5021 | G Loss: 3.0761\n",
      "Epoch [359/5000] | D Loss: 0.5241 | G Loss: 3.3700\n",
      "Epoch [360/5000] | D Loss: 0.7618 | G Loss: 1.2074\n",
      "Epoch [361/5000] | D Loss: 0.4585 | G Loss: 2.9444\n",
      "Epoch [362/5000] | D Loss: 0.4934 | G Loss: 2.6771\n",
      "Epoch [363/5000] | D Loss: 0.5369 | G Loss: 4.0345\n",
      "Epoch [364/5000] | D Loss: 0.4752 | G Loss: 3.3583\n",
      "Epoch [365/5000] | D Loss: 0.4868 | G Loss: 3.0713\n",
      "Epoch [366/5000] | D Loss: 0.6309 | G Loss: 3.3925\n",
      "Epoch [367/5000] | D Loss: 0.4737 | G Loss: 2.2127\n",
      "Epoch [368/5000] | D Loss: 0.4554 | G Loss: 2.7746\n",
      "Epoch [369/5000] | D Loss: 0.6290 | G Loss: 4.1624\n",
      "Epoch [370/5000] | D Loss: 0.4833 | G Loss: 2.4842\n",
      "Epoch [371/5000] | D Loss: 1.5368 | G Loss: 0.2563\n",
      "Epoch [372/5000] | D Loss: 0.5170 | G Loss: 2.1768\n",
      "Epoch [373/5000] | D Loss: 0.4478 | G Loss: 2.5826\n",
      "Epoch [374/5000] | D Loss: 0.6200 | G Loss: 2.6254\n",
      "Epoch [375/5000] | D Loss: 0.6313 | G Loss: 1.7198\n",
      "Epoch [376/5000] | D Loss: 0.4534 | G Loss: 3.7717\n",
      "Epoch [377/5000] | D Loss: 1.1453 | G Loss: 0.2528\n",
      "Epoch [378/5000] | D Loss: 0.4624 | G Loss: 2.6323\n",
      "Epoch [379/5000] | D Loss: 0.4728 | G Loss: 3.3191\n",
      "Epoch [380/5000] | D Loss: 0.5008 | G Loss: 2.1649\n",
      "Epoch [381/5000] | D Loss: 0.5251 | G Loss: 2.7038\n",
      "Epoch [382/5000] | D Loss: 0.5061 | G Loss: 3.1913\n",
      "Epoch [383/5000] | D Loss: 0.4665 | G Loss: 2.5598\n",
      "Epoch [384/5000] | D Loss: 0.4462 | G Loss: 2.7650\n",
      "Epoch [385/5000] | D Loss: 0.4867 | G Loss: 2.1097\n",
      "Epoch [386/5000] | D Loss: 0.5141 | G Loss: 2.2387\n",
      "Epoch [387/5000] | D Loss: 0.4503 | G Loss: 3.2635\n",
      "Epoch [388/5000] | D Loss: 0.4472 | G Loss: 2.6686\n",
      "Epoch [389/5000] | D Loss: 0.7085 | G Loss: 0.6706\n",
      "Epoch [390/5000] | D Loss: 0.8182 | G Loss: 0.6479\n",
      "Epoch [391/5000] | D Loss: 0.4519 | G Loss: 2.9912\n",
      "Epoch [392/5000] | D Loss: 0.4768 | G Loss: 3.8867\n",
      "Epoch [393/5000] | D Loss: 0.4700 | G Loss: 3.0085\n",
      "Epoch [394/5000] | D Loss: 0.4952 | G Loss: 2.2474\n",
      "Epoch [395/5000] | D Loss: 0.4964 | G Loss: 2.5109\n",
      "Epoch [396/5000] | D Loss: 0.4506 | G Loss: 2.7008\n",
      "Epoch [397/5000] | D Loss: 0.4524 | G Loss: 3.3116\n",
      "Epoch [398/5000] | D Loss: 0.4655 | G Loss: 2.9131\n",
      "Epoch [399/5000] | D Loss: 0.4788 | G Loss: 2.9818\n",
      "Epoch [400/5000] | D Loss: 0.6029 | G Loss: 1.7098\n",
      "Epoch 400 FID Score: 215.6394\n",
      "Epoch [401/5000] | D Loss: 0.4806 | G Loss: 2.6826\n",
      "Epoch [402/5000] | D Loss: 0.4858 | G Loss: 2.2558\n",
      "Epoch [403/5000] | D Loss: 0.4600 | G Loss: 3.0312\n",
      "Epoch [404/5000] | D Loss: 0.5526 | G Loss: 3.3076\n",
      "Epoch [405/5000] | D Loss: 1.0208 | G Loss: 0.5470\n",
      "Epoch [406/5000] | D Loss: 0.4618 | G Loss: 2.6819\n",
      "Epoch [407/5000] | D Loss: 0.4619 | G Loss: 2.9700\n",
      "Epoch [408/5000] | D Loss: 0.5178 | G Loss: 2.3241\n",
      "Epoch [409/5000] | D Loss: 0.4436 | G Loss: 3.2724\n",
      "Epoch [410/5000] | D Loss: 0.4752 | G Loss: 2.8366\n",
      "Epoch [411/5000] | D Loss: 0.4586 | G Loss: 3.2868\n",
      "Epoch [412/5000] | D Loss: 0.5440 | G Loss: 4.3348\n",
      "Epoch [413/5000] | D Loss: 0.4596 | G Loss: 2.7378\n",
      "Epoch [414/5000] | D Loss: 0.4589 | G Loss: 3.4733\n",
      "Epoch [415/5000] | D Loss: 0.5002 | G Loss: 1.9344\n",
      "Epoch [416/5000] | D Loss: 0.4687 | G Loss: 2.5586\n",
      "Epoch [417/5000] | D Loss: 0.5694 | G Loss: 2.6299\n",
      "Epoch [418/5000] | D Loss: 0.4851 | G Loss: 3.7468\n",
      "Epoch [419/5000] | D Loss: 0.4583 | G Loss: 3.4464\n",
      "Epoch [420/5000] | D Loss: 0.5188 | G Loss: 3.8458\n",
      "Epoch [421/5000] | D Loss: 0.5347 | G Loss: 3.0796\n",
      "Epoch [422/5000] | D Loss: 0.4810 | G Loss: 3.0286\n",
      "Epoch [423/5000] | D Loss: 0.4750 | G Loss: 2.5973\n",
      "Epoch [424/5000] | D Loss: 0.4672 | G Loss: 3.2366\n",
      "Epoch [425/5000] | D Loss: 0.4559 | G Loss: 3.6160\n",
      "Epoch [426/5000] | D Loss: 0.4482 | G Loss: 2.6115\n",
      "Epoch [427/5000] | D Loss: 0.6610 | G Loss: 0.9804\n",
      "Epoch [428/5000] | D Loss: 1.0496 | G Loss: 0.3660\n",
      "Epoch [429/5000] | D Loss: 0.4508 | G Loss: 3.6973\n",
      "Epoch [430/5000] | D Loss: 0.4863 | G Loss: 2.6367\n",
      "Epoch [431/5000] | D Loss: 0.4571 | G Loss: 3.6982\n",
      "Epoch [432/5000] | D Loss: 0.4654 | G Loss: 3.3102\n",
      "Epoch [433/5000] | D Loss: 0.4444 | G Loss: 2.5743\n",
      "Epoch [434/5000] | D Loss: 0.4689 | G Loss: 3.2132\n",
      "Epoch [435/5000] | D Loss: 0.4623 | G Loss: 3.5189\n",
      "Epoch [436/5000] | D Loss: 0.5323 | G Loss: 1.9740\n",
      "Epoch [437/5000] | D Loss: 0.4976 | G Loss: 2.6148\n",
      "Epoch [438/5000] | D Loss: 0.4716 | G Loss: 2.7416\n",
      "Epoch [439/5000] | D Loss: 0.4661 | G Loss: 2.5581\n",
      "Epoch [440/5000] | D Loss: 0.5598 | G Loss: 1.4645\n",
      "Epoch [441/5000] | D Loss: 0.5404 | G Loss: 2.8058\n",
      "Epoch [442/5000] | D Loss: 0.6077 | G Loss: 1.1080\n",
      "Epoch [443/5000] | D Loss: 0.4790 | G Loss: 2.6333\n",
      "Epoch [444/5000] | D Loss: 0.4637 | G Loss: 3.4779\n",
      "Epoch [445/5000] | D Loss: 0.4781 | G Loss: 2.9169\n",
      "Epoch [446/5000] | D Loss: 0.4453 | G Loss: 2.6481\n",
      "Epoch [447/5000] | D Loss: 0.4350 | G Loss: 3.2449\n",
      "Epoch [448/5000] | D Loss: 0.5774 | G Loss: 1.7054\n",
      "Epoch [449/5000] | D Loss: 0.4682 | G Loss: 2.5125\n",
      "Epoch [450/5000] | D Loss: 0.4374 | G Loss: 2.4635\n",
      "Epoch [451/5000] | D Loss: 0.4763 | G Loss: 3.6288\n",
      "Epoch [452/5000] | D Loss: 0.4988 | G Loss: 2.7654\n",
      "Epoch [453/5000] | D Loss: 0.9167 | G Loss: 0.3136\n",
      "Epoch [454/5000] | D Loss: 0.4672 | G Loss: 3.0891\n",
      "Epoch [455/5000] | D Loss: 0.5361 | G Loss: 2.5349\n",
      "Epoch [456/5000] | D Loss: 0.4665 | G Loss: 3.4900\n",
      "Epoch [457/5000] | D Loss: 0.4970 | G Loss: 2.0086\n",
      "Epoch [458/5000] | D Loss: 0.4814 | G Loss: 2.5104\n",
      "Epoch [459/5000] | D Loss: 0.4934 | G Loss: 2.7703\n",
      "Epoch [460/5000] | D Loss: 0.4410 | G Loss: 3.2426\n",
      "Epoch [461/5000] | D Loss: 0.4609 | G Loss: 2.5281\n",
      "Epoch [462/5000] | D Loss: 0.5438 | G Loss: 2.8770\n",
      "Epoch [463/5000] | D Loss: 0.5636 | G Loss: 4.2787\n",
      "Epoch [464/5000] | D Loss: 0.4692 | G Loss: 3.5651\n",
      "Epoch [465/5000] | D Loss: 0.4657 | G Loss: 3.0615\n",
      "Epoch [466/5000] | D Loss: 0.5097 | G Loss: 3.5172\n",
      "Epoch [467/5000] | D Loss: 0.4953 | G Loss: 2.1305\n",
      "Epoch [468/5000] | D Loss: 0.4832 | G Loss: 2.5879\n",
      "Epoch [469/5000] | D Loss: 0.8165 | G Loss: 0.9044\n",
      "Epoch [470/5000] | D Loss: 0.4540 | G Loss: 3.0306\n",
      "Epoch [471/5000] | D Loss: 0.6234 | G Loss: 3.6927\n",
      "Epoch [472/5000] | D Loss: 0.4921 | G Loss: 2.3024\n",
      "Epoch [473/5000] | D Loss: 0.4543 | G Loss: 2.9684\n",
      "Epoch [474/5000] | D Loss: 0.4519 | G Loss: 2.8044\n",
      "Epoch [475/5000] | D Loss: 0.4975 | G Loss: 2.0992\n",
      "Epoch [476/5000] | D Loss: 0.4839 | G Loss: 3.2304\n",
      "Epoch [477/5000] | D Loss: 0.4708 | G Loss: 2.9580\n",
      "Epoch [478/5000] | D Loss: 0.4445 | G Loss: 3.1811\n",
      "Epoch [479/5000] | D Loss: 0.4750 | G Loss: 3.4948\n",
      "Epoch [480/5000] | D Loss: 0.5244 | G Loss: 2.5325\n",
      "Epoch [481/5000] | D Loss: 0.4580 | G Loss: 2.8738\n",
      "Epoch [482/5000] | D Loss: 0.4641 | G Loss: 3.2229\n",
      "Epoch [483/5000] | D Loss: 0.4367 | G Loss: 2.9723\n",
      "Epoch [484/5000] | D Loss: 0.7231 | G Loss: 1.3703\n",
      "Epoch [485/5000] | D Loss: 0.4877 | G Loss: 3.9207\n",
      "Epoch [486/5000] | D Loss: 0.6978 | G Loss: 0.8876\n",
      "Epoch [487/5000] | D Loss: 0.5200 | G Loss: 3.5298\n",
      "Epoch [488/5000] | D Loss: 0.4608 | G Loss: 3.6913\n",
      "Epoch [489/5000] | D Loss: 0.4537 | G Loss: 2.8132\n",
      "Epoch [490/5000] | D Loss: 0.4576 | G Loss: 2.9827\n",
      "Epoch [491/5000] | D Loss: 0.4478 | G Loss: 3.3509\n",
      "Epoch [492/5000] | D Loss: 0.5146 | G Loss: 3.2959\n",
      "Epoch [493/5000] | D Loss: 1.5177 | G Loss: 0.3993\n",
      "Epoch [494/5000] | D Loss: 0.4720 | G Loss: 2.6022\n",
      "Epoch [495/5000] | D Loss: 0.4803 | G Loss: 3.6151\n",
      "Epoch [496/5000] | D Loss: 0.4829 | G Loss: 1.8602\n",
      "Epoch [497/5000] | D Loss: 1.0038 | G Loss: 0.4127\n",
      "Epoch [498/5000] | D Loss: 0.4519 | G Loss: 2.8124\n",
      "Epoch [499/5000] | D Loss: 0.8752 | G Loss: 0.5719\n",
      "Epoch [500/5000] | D Loss: 0.5083 | G Loss: 2.1030\n",
      "Epoch 500 FID Score: 206.4712\n",
      "Epoch [501/5000] | D Loss: 0.4820 | G Loss: 2.9926\n",
      "Epoch [502/5000] | D Loss: 0.4741 | G Loss: 2.0212\n",
      "Epoch [503/5000] | D Loss: 0.4776 | G Loss: 3.0468\n",
      "Epoch [504/5000] | D Loss: 0.4838 | G Loss: 3.3130\n",
      "Epoch [505/5000] | D Loss: 0.7970 | G Loss: 1.1442\n",
      "Epoch [506/5000] | D Loss: 0.4483 | G Loss: 2.7221\n",
      "Epoch [507/5000] | D Loss: 0.4786 | G Loss: 3.5174\n",
      "Epoch [508/5000] | D Loss: 0.4677 | G Loss: 2.6126\n",
      "Epoch [509/5000] | D Loss: 0.6448 | G Loss: 2.1593\n",
      "Epoch [510/5000] | D Loss: 0.5217 | G Loss: 2.3703\n",
      "Epoch [511/5000] | D Loss: 0.4714 | G Loss: 2.2746\n",
      "Epoch [512/5000] | D Loss: 0.8081 | G Loss: 0.6271\n",
      "Epoch [513/5000] | D Loss: 0.4965 | G Loss: 3.4453\n",
      "Epoch [514/5000] | D Loss: 0.4611 | G Loss: 3.7784\n",
      "Epoch [515/5000] | D Loss: 0.4701 | G Loss: 3.3277\n",
      "Epoch [516/5000] | D Loss: 0.6115 | G Loss: 3.6602\n",
      "Epoch [517/5000] | D Loss: 0.5510 | G Loss: 3.6769\n",
      "Epoch [518/5000] | D Loss: 0.5013 | G Loss: 2.1826\n",
      "Epoch [519/5000] | D Loss: 0.4628 | G Loss: 3.1306\n",
      "Epoch [520/5000] | D Loss: 0.4467 | G Loss: 2.7866\n",
      "Epoch [521/5000] | D Loss: 0.9311 | G Loss: 0.6879\n",
      "Epoch [522/5000] | D Loss: 0.4918 | G Loss: 3.7487\n",
      "Epoch [523/5000] | D Loss: 0.5602 | G Loss: 1.3445\n",
      "Epoch [524/5000] | D Loss: 0.4571 | G Loss: 3.5099\n",
      "Epoch [525/5000] | D Loss: 0.4506 | G Loss: 2.9980\n",
      "Epoch [526/5000] | D Loss: 0.8129 | G Loss: 0.5406\n",
      "Epoch [527/5000] | D Loss: 0.6887 | G Loss: 1.2487\n",
      "Epoch [528/5000] | D Loss: 0.4739 | G Loss: 3.7841\n",
      "Epoch [529/5000] | D Loss: 0.6026 | G Loss: 3.3238\n",
      "Epoch [530/5000] | D Loss: 1.3063 | G Loss: 0.2979\n",
      "Epoch [531/5000] | D Loss: 0.4745 | G Loss: 4.0520\n",
      "Epoch [532/5000] | D Loss: 0.5158 | G Loss: 2.6640\n",
      "Epoch [533/5000] | D Loss: 0.4608 | G Loss: 3.6354\n",
      "Epoch [534/5000] | D Loss: 0.5114 | G Loss: 2.8142\n",
      "Epoch [535/5000] | D Loss: 0.9652 | G Loss: 0.7197\n",
      "Epoch [536/5000] | D Loss: 0.4840 | G Loss: 2.8301\n",
      "Epoch [537/5000] | D Loss: 0.6304 | G Loss: 3.0894\n",
      "Epoch [538/5000] | D Loss: 0.5509 | G Loss: 2.3993\n",
      "Epoch [539/5000] | D Loss: 0.4660 | G Loss: 3.5481\n",
      "Epoch [540/5000] | D Loss: 0.4748 | G Loss: 2.6157\n",
      "Epoch [541/5000] | D Loss: 0.4570 | G Loss: 2.2990\n",
      "Epoch [542/5000] | D Loss: 0.7399 | G Loss: 1.3383\n",
      "Epoch [543/5000] | D Loss: 0.8413 | G Loss: 0.9412\n",
      "Epoch [544/5000] | D Loss: 0.5459 | G Loss: 2.0628\n",
      "Epoch [545/5000] | D Loss: 0.5262 | G Loss: 2.9922\n",
      "Epoch [546/5000] | D Loss: 0.6371 | G Loss: 1.7461\n",
      "Epoch [547/5000] | D Loss: 0.4983 | G Loss: 2.9058\n",
      "Epoch [548/5000] | D Loss: 0.4588 | G Loss: 2.2792\n",
      "Epoch [549/5000] | D Loss: 0.4619 | G Loss: 3.2467\n",
      "Epoch [550/5000] | D Loss: 0.5282 | G Loss: 3.1738\n",
      "Epoch [551/5000] | D Loss: 0.4888 | G Loss: 3.8145\n",
      "Epoch [552/5000] | D Loss: 0.4856 | G Loss: 2.4907\n",
      "Epoch [553/5000] | D Loss: 0.4826 | G Loss: 2.4969\n",
      "Epoch [554/5000] | D Loss: 0.4769 | G Loss: 3.1733\n",
      "Epoch [555/5000] | D Loss: 0.5458 | G Loss: 1.7208\n",
      "Epoch [556/5000] | D Loss: 0.4665 | G Loss: 2.0927\n",
      "Epoch [557/5000] | D Loss: 0.4886 | G Loss: 3.2897\n",
      "Epoch [558/5000] | D Loss: 0.4692 | G Loss: 2.2569\n",
      "Epoch [559/5000] | D Loss: 0.5701 | G Loss: 1.3058\n",
      "Epoch [560/5000] | D Loss: 0.4788 | G Loss: 3.9065\n",
      "Epoch [561/5000] | D Loss: 0.5860 | G Loss: 3.6905\n",
      "Epoch [562/5000] | D Loss: 0.4620 | G Loss: 3.4641\n",
      "Epoch [563/5000] | D Loss: 0.4512 | G Loss: 2.8367\n",
      "Epoch [564/5000] | D Loss: 0.4589 | G Loss: 2.7125\n",
      "Epoch [565/5000] | D Loss: 0.4645 | G Loss: 3.0801\n",
      "Epoch [566/5000] | D Loss: 1.3988 | G Loss: 0.3852\n",
      "Epoch [567/5000] | D Loss: 0.4763 | G Loss: 2.4888\n",
      "Epoch [568/5000] | D Loss: 0.5478 | G Loss: 2.1931\n",
      "Epoch [569/5000] | D Loss: 0.6002 | G Loss: 1.8048\n",
      "Epoch [570/5000] | D Loss: 0.6813 | G Loss: 1.1767\n",
      "Epoch [571/5000] | D Loss: 0.4551 | G Loss: 2.7015\n",
      "Epoch [572/5000] | D Loss: 0.4452 | G Loss: 3.2051\n",
      "Epoch [573/5000] | D Loss: 1.2500 | G Loss: 0.4023\n",
      "Epoch [574/5000] | D Loss: 1.4323 | G Loss: 0.6122\n",
      "Epoch [575/5000] | D Loss: 0.4826 | G Loss: 3.1283\n",
      "Epoch [576/5000] | D Loss: 0.4975 | G Loss: 3.3395\n",
      "Epoch [577/5000] | D Loss: 0.4958 | G Loss: 3.0713\n",
      "Epoch [578/5000] | D Loss: 0.5022 | G Loss: 2.8761\n",
      "Epoch [579/5000] | D Loss: 0.4504 | G Loss: 3.8004\n",
      "Epoch [580/5000] | D Loss: 0.8912 | G Loss: 0.8699\n",
      "Epoch [581/5000] | D Loss: 1.2442 | G Loss: 0.2726\n",
      "Epoch [582/5000] | D Loss: 0.5646 | G Loss: 1.8256\n",
      "Epoch [583/5000] | D Loss: 0.4590 | G Loss: 3.5395\n",
      "Epoch [584/5000] | D Loss: 0.6068 | G Loss: 1.1066\n",
      "Epoch [585/5000] | D Loss: 0.4688 | G Loss: 3.3096\n",
      "Epoch [586/5000] | D Loss: 0.7539 | G Loss: 1.8590\n",
      "Epoch [587/5000] | D Loss: 0.4900 | G Loss: 3.8872\n",
      "Epoch [588/5000] | D Loss: 0.4799 | G Loss: 2.7810\n",
      "Epoch [589/5000] | D Loss: 0.4759 | G Loss: 2.6873\n",
      "Epoch [590/5000] | D Loss: 0.4903 | G Loss: 2.1047\n",
      "Epoch [591/5000] | D Loss: 0.5538 | G Loss: 2.7646\n",
      "Epoch [592/5000] | D Loss: 0.5053 | G Loss: 1.9242\n",
      "Epoch [593/5000] | D Loss: 0.5983 | G Loss: 1.0287\n",
      "Epoch [594/5000] | D Loss: 0.4663 | G Loss: 3.0683\n",
      "Epoch [595/5000] | D Loss: 0.5266 | G Loss: 2.2735\n",
      "Epoch [596/5000] | D Loss: 1.2420 | G Loss: 0.3626\n",
      "Epoch [597/5000] | D Loss: 0.4753 | G Loss: 3.4089\n",
      "Epoch [598/5000] | D Loss: 0.5558 | G Loss: 1.9688\n",
      "Epoch [599/5000] | D Loss: 0.4965 | G Loss: 3.0073\n",
      "Epoch [600/5000] | D Loss: 0.4831 | G Loss: 2.4265\n",
      "Epoch 600 FID Score: 192.3241\n",
      "Epoch [601/5000] | D Loss: 1.4529 | G Loss: 0.3494\n",
      "Epoch [602/5000] | D Loss: 0.4610 | G Loss: 3.7855\n",
      "Epoch [603/5000] | D Loss: 0.7492 | G Loss: 0.9125\n",
      "Epoch [604/5000] | D Loss: 0.4700 | G Loss: 2.9642\n",
      "Epoch [605/5000] | D Loss: 0.4980 | G Loss: 1.8342\n",
      "Epoch [606/5000] | D Loss: 1.8338 | G Loss: 0.7240\n",
      "Epoch [607/5000] | D Loss: 0.4625 | G Loss: 2.8143\n",
      "Epoch [608/5000] | D Loss: 0.4730 | G Loss: 2.7669\n",
      "Epoch [609/5000] | D Loss: 0.6756 | G Loss: 1.0517\n",
      "Epoch [610/5000] | D Loss: 0.4952 | G Loss: 3.1673\n",
      "Epoch [611/5000] | D Loss: 0.4587 | G Loss: 3.1508\n",
      "Epoch [612/5000] | D Loss: 0.5203 | G Loss: 4.3881\n",
      "Epoch [613/5000] | D Loss: 0.6124 | G Loss: 3.8094\n",
      "Epoch [614/5000] | D Loss: 0.4729 | G Loss: 3.2648\n",
      "Epoch [615/5000] | D Loss: 1.2513 | G Loss: 0.7941\n",
      "Epoch [616/5000] | D Loss: 0.4903 | G Loss: 2.5251\n",
      "Epoch [617/5000] | D Loss: 0.4577 | G Loss: 2.8388\n",
      "Epoch [618/5000] | D Loss: 1.6526 | G Loss: 0.4085\n",
      "Epoch [619/5000] | D Loss: 0.4630 | G Loss: 3.1289\n",
      "Epoch [620/5000] | D Loss: 0.4623 | G Loss: 3.6193\n",
      "Epoch [621/5000] | D Loss: 0.4701 | G Loss: 3.4121\n",
      "Epoch [622/5000] | D Loss: 0.4739 | G Loss: 3.6769\n",
      "Epoch [623/5000] | D Loss: 0.4802 | G Loss: 3.1663\n",
      "Epoch [624/5000] | D Loss: 0.5012 | G Loss: 2.9995\n",
      "Epoch [625/5000] | D Loss: 0.4449 | G Loss: 2.5998\n",
      "Epoch [626/5000] | D Loss: 0.4801 | G Loss: 2.7470\n",
      "Epoch [627/5000] | D Loss: 0.9403 | G Loss: 0.9059\n",
      "Epoch [628/5000] | D Loss: 0.7882 | G Loss: 0.7759\n",
      "Epoch [629/5000] | D Loss: 0.5898 | G Loss: 3.5390\n",
      "Epoch [630/5000] | D Loss: 0.6528 | G Loss: 0.8140\n",
      "Epoch [631/5000] | D Loss: 0.8493 | G Loss: 0.8533\n",
      "Epoch [632/5000] | D Loss: 0.4922 | G Loss: 2.9299\n",
      "Epoch [633/5000] | D Loss: 0.4964 | G Loss: 3.6369\n",
      "Epoch [634/5000] | D Loss: 0.5432 | G Loss: 3.1350\n",
      "Epoch [635/5000] | D Loss: 1.3640 | G Loss: 0.7701\n",
      "Epoch [636/5000] | D Loss: 0.6499 | G Loss: 1.2066\n",
      "Epoch [637/5000] | D Loss: 0.5139 | G Loss: 2.9663\n",
      "Epoch [638/5000] | D Loss: 0.5670 | G Loss: 2.0454\n",
      "Epoch [639/5000] | D Loss: 0.4705 | G Loss: 2.4492\n",
      "Epoch [640/5000] | D Loss: 0.6129 | G Loss: 2.3413\n",
      "Epoch [641/5000] | D Loss: 0.8390 | G Loss: 0.7529\n",
      "Epoch [642/5000] | D Loss: 0.9194 | G Loss: 0.3317\n",
      "Epoch [643/5000] | D Loss: 0.5300 | G Loss: 1.7842\n",
      "Epoch [644/5000] | D Loss: 0.4887 | G Loss: 2.2566\n",
      "Epoch [645/5000] | D Loss: 0.9105 | G Loss: 0.9979\n",
      "Epoch [646/5000] | D Loss: 0.9524 | G Loss: 0.8618\n",
      "Epoch [647/5000] | D Loss: 1.4038 | G Loss: 0.3832\n",
      "Epoch [648/5000] | D Loss: 0.5338 | G Loss: 3.1573\n",
      "Epoch [649/5000] | D Loss: 0.6097 | G Loss: 3.0091\n",
      "Epoch [650/5000] | D Loss: 0.4488 | G Loss: 3.2291\n",
      "Epoch [651/5000] | D Loss: 0.6600 | G Loss: 1.1000\n",
      "Epoch [652/5000] | D Loss: 0.4737 | G Loss: 2.9707\n",
      "Epoch [653/5000] | D Loss: 0.5230 | G Loss: 3.6097\n",
      "Epoch [654/5000] | D Loss: 0.4686 | G Loss: 3.5783\n",
      "Epoch [655/5000] | D Loss: 0.5262 | G Loss: 1.4392\n",
      "Epoch [656/5000] | D Loss: 0.4612 | G Loss: 2.6423\n",
      "Epoch [657/5000] | D Loss: 1.3566 | G Loss: 0.4962\n",
      "Epoch [658/5000] | D Loss: 0.5350 | G Loss: 2.9797\n",
      "Epoch [659/5000] | D Loss: 0.5133 | G Loss: 3.1733\n",
      "Epoch [660/5000] | D Loss: 0.7340 | G Loss: 1.4911\n",
      "Epoch [661/5000] | D Loss: 1.0417 | G Loss: 0.7466\n",
      "Epoch [662/5000] | D Loss: 0.5292 | G Loss: 2.7091\n",
      "Epoch [663/5000] | D Loss: 0.4623 | G Loss: 2.9050\n",
      "Epoch [664/5000] | D Loss: 0.5213 | G Loss: 2.3823\n",
      "Epoch [665/5000] | D Loss: 0.5573 | G Loss: 4.4688\n",
      "Epoch [666/5000] | D Loss: 1.1835 | G Loss: 0.4078\n",
      "Epoch [667/5000] | D Loss: 0.5518 | G Loss: 2.0317\n",
      "Epoch [668/5000] | D Loss: 2.0448 | G Loss: 0.8027\n",
      "Epoch [669/5000] | D Loss: 0.5228 | G Loss: 3.7386\n",
      "Epoch [670/5000] | D Loss: 0.4719 | G Loss: 3.2484\n",
      "Epoch [671/5000] | D Loss: 0.9481 | G Loss: 0.4371\n",
      "Epoch [672/5000] | D Loss: 0.5223 | G Loss: 3.9174\n",
      "Epoch [673/5000] | D Loss: 0.4835 | G Loss: 2.9900\n",
      "Epoch [674/5000] | D Loss: 0.4551 | G Loss: 2.6037\n",
      "Epoch [675/5000] | D Loss: 0.7806 | G Loss: 1.0728\n",
      "Epoch [676/5000] | D Loss: 1.4640 | G Loss: 0.5257\n",
      "Epoch [677/5000] | D Loss: 0.7962 | G Loss: 1.8019\n",
      "Epoch [678/5000] | D Loss: 0.4734 | G Loss: 3.9926\n",
      "Epoch [679/5000] | D Loss: 0.4858 | G Loss: 3.6709\n",
      "Epoch [680/5000] | D Loss: 0.5425 | G Loss: 2.9053\n",
      "Epoch [681/5000] | D Loss: 0.4483 | G Loss: 3.2809\n",
      "Epoch [682/5000] | D Loss: 0.5111 | G Loss: 1.7817\n",
      "Epoch [683/5000] | D Loss: 0.4759 | G Loss: 4.0229\n",
      "Epoch [684/5000] | D Loss: 0.4686 | G Loss: 2.6516\n",
      "Epoch [685/5000] | D Loss: 0.5538 | G Loss: 2.2840\n",
      "Epoch [686/5000] | D Loss: 0.4907 | G Loss: 4.0218\n",
      "Epoch [687/5000] | D Loss: 0.4742 | G Loss: 3.9725\n",
      "Epoch [688/5000] | D Loss: 0.4820 | G Loss: 4.1238\n",
      "Epoch [689/5000] | D Loss: 0.7211 | G Loss: 2.6738\n",
      "Epoch [690/5000] | D Loss: 0.8144 | G Loss: 2.2720\n",
      "Epoch [691/5000] | D Loss: 0.5126 | G Loss: 3.5618\n",
      "Epoch [692/5000] | D Loss: 0.6579 | G Loss: 1.5371\n",
      "Epoch [693/5000] | D Loss: 0.4688 | G Loss: 3.0935\n",
      "Epoch [694/5000] | D Loss: 1.5879 | G Loss: 0.4075\n",
      "Epoch [695/5000] | D Loss: 0.4569 | G Loss: 2.6516\n",
      "Epoch [696/5000] | D Loss: 0.6696 | G Loss: 1.7626\n",
      "Epoch [697/5000] | D Loss: 0.4514 | G Loss: 3.2275\n",
      "Epoch [698/5000] | D Loss: 0.5253 | G Loss: 3.3435\n",
      "Epoch [699/5000] | D Loss: 0.6185 | G Loss: 3.3823\n",
      "Epoch [700/5000] | D Loss: 0.5136 | G Loss: 2.7333\n",
      "Epoch 700 FID Score: 183.4016\n",
      "Epoch [701/5000] | D Loss: 0.6172 | G Loss: 3.4653\n",
      "Epoch [702/5000] | D Loss: 0.5675 | G Loss: 5.4379\n",
      "Epoch [703/5000] | D Loss: 0.5130 | G Loss: 3.1508\n",
      "Epoch [704/5000] | D Loss: 0.5033 | G Loss: 4.1663\n",
      "Epoch [705/5000] | D Loss: 0.4617 | G Loss: 2.9125\n",
      "Epoch [706/5000] | D Loss: 1.1760 | G Loss: 1.0462\n",
      "Epoch [707/5000] | D Loss: 0.5896 | G Loss: 1.6206\n",
      "Epoch [708/5000] | D Loss: 0.5004 | G Loss: 3.4877\n",
      "Epoch [709/5000] | D Loss: 1.6756 | G Loss: 0.6874\n",
      "Epoch [710/5000] | D Loss: 0.4601 | G Loss: 3.2310\n",
      "Epoch [711/5000] | D Loss: 0.6821 | G Loss: 2.2092\n",
      "Epoch [712/5000] | D Loss: 0.9140 | G Loss: 1.2451\n",
      "Epoch [713/5000] | D Loss: 0.4715 | G Loss: 2.5658\n",
      "Epoch [714/5000] | D Loss: 0.4908 | G Loss: 2.8357\n",
      "Epoch [715/5000] | D Loss: 0.4869 | G Loss: 4.1126\n",
      "Epoch [716/5000] | D Loss: 0.5479 | G Loss: 1.5812\n",
      "Epoch [717/5000] | D Loss: 0.4503 | G Loss: 3.3645\n",
      "Epoch [718/5000] | D Loss: 0.5156 | G Loss: 1.9466\n",
      "Epoch [719/5000] | D Loss: 0.5925 | G Loss: 2.4175\n",
      "Epoch [720/5000] | D Loss: 2.5181 | G Loss: 0.2892\n",
      "Epoch [721/5000] | D Loss: 0.4999 | G Loss: 2.6579\n",
      "Epoch [722/5000] | D Loss: 0.8154 | G Loss: 1.2667\n",
      "Epoch [723/5000] | D Loss: 0.7462 | G Loss: 1.2305\n",
      "Epoch [724/5000] | D Loss: 1.3849 | G Loss: 0.5941\n",
      "Epoch [725/5000] | D Loss: 1.4242 | G Loss: 0.3531\n",
      "Epoch [726/5000] | D Loss: 0.6046 | G Loss: 1.8389\n",
      "Epoch [727/5000] | D Loss: 0.4678 | G Loss: 3.1276\n",
      "Epoch [728/5000] | D Loss: 0.4878 | G Loss: 3.1833\n",
      "Epoch [729/5000] | D Loss: 0.8372 | G Loss: 0.9419\n",
      "Epoch [730/5000] | D Loss: 0.5300 | G Loss: 3.0082\n",
      "Epoch [731/5000] | D Loss: 0.5979 | G Loss: 1.7314\n",
      "Epoch [732/5000] | D Loss: 0.5395 | G Loss: 3.4588\n",
      "Epoch [733/5000] | D Loss: 0.4573 | G Loss: 2.5779\n",
      "Epoch [734/5000] | D Loss: 0.4838 | G Loss: 3.6191\n",
      "Epoch [735/5000] | D Loss: 0.4868 | G Loss: 2.8032\n",
      "Epoch [736/5000] | D Loss: 0.4898 | G Loss: 3.4881\n",
      "Epoch [737/5000] | D Loss: 0.6984 | G Loss: 1.2671\n",
      "Epoch [738/5000] | D Loss: 0.8419 | G Loss: 0.7692\n",
      "Epoch [739/5000] | D Loss: 2.9849 | G Loss: 2.9952\n",
      "Epoch [740/5000] | D Loss: 0.5225 | G Loss: 3.1568\n",
      "Epoch [741/5000] | D Loss: 0.8693 | G Loss: 1.3545\n",
      "Epoch [742/5000] | D Loss: 0.5376 | G Loss: 3.1987\n",
      "Epoch [743/5000] | D Loss: 0.5766 | G Loss: 1.6843\n",
      "Epoch [744/5000] | D Loss: 0.5542 | G Loss: 2.2055\n",
      "Epoch [745/5000] | D Loss: 0.5382 | G Loss: 2.4917\n",
      "Epoch [746/5000] | D Loss: 0.9409 | G Loss: 0.5227\n",
      "Epoch [747/5000] | D Loss: 0.5090 | G Loss: 3.7341\n",
      "Epoch [748/5000] | D Loss: 1.6210 | G Loss: 1.2862\n",
      "Epoch [749/5000] | D Loss: 0.4554 | G Loss: 3.0237\n",
      "Epoch [750/5000] | D Loss: 1.0329 | G Loss: 0.8111\n",
      "Epoch [751/5000] | D Loss: 2.0675 | G Loss: 0.7476\n",
      "Epoch [752/5000] | D Loss: 0.7003 | G Loss: 2.6297\n",
      "Epoch [753/5000] | D Loss: 0.5547 | G Loss: 2.4295\n",
      "Epoch [754/5000] | D Loss: 1.0764 | G Loss: 0.8291\n",
      "Epoch [755/5000] | D Loss: 1.6188 | G Loss: 0.7515\n",
      "Epoch [756/5000] | D Loss: 0.8993 | G Loss: 0.5303\n",
      "Epoch [757/5000] | D Loss: 0.4778 | G Loss: 3.4018\n",
      "Epoch [758/5000] | D Loss: 0.5206 | G Loss: 2.6010\n",
      "Epoch [759/5000] | D Loss: 0.6176 | G Loss: 1.1038\n",
      "Epoch [760/5000] | D Loss: 0.4806 | G Loss: 2.4856\n",
      "Epoch [761/5000] | D Loss: 0.5354 | G Loss: 1.9640\n",
      "Epoch [762/5000] | D Loss: 0.5202 | G Loss: 3.1601\n",
      "Epoch [763/5000] | D Loss: 1.0304 | G Loss: 0.5460\n",
      "Epoch [764/5000] | D Loss: 0.5494 | G Loss: 2.8542\n",
      "Epoch [765/5000] | D Loss: 0.6100 | G Loss: 2.0304\n",
      "Epoch [766/5000] | D Loss: 0.6247 | G Loss: 3.1931\n",
      "Epoch [767/5000] | D Loss: 0.5598 | G Loss: 3.5675\n",
      "Epoch [768/5000] | D Loss: 0.4723 | G Loss: 2.7747\n",
      "Epoch [769/5000] | D Loss: 1.0071 | G Loss: 0.8494\n",
      "Epoch [770/5000] | D Loss: 0.6488 | G Loss: 2.7876\n",
      "Epoch [771/5000] | D Loss: 0.4776 | G Loss: 3.3073\n",
      "Epoch [772/5000] | D Loss: 0.5648 | G Loss: 1.7254\n",
      "Epoch [773/5000] | D Loss: 1.5925 | G Loss: 0.2788\n",
      "Epoch [774/5000] | D Loss: 0.5588 | G Loss: 1.9221\n",
      "Epoch [775/5000] | D Loss: 0.4619 | G Loss: 2.8361\n",
      "Epoch [776/5000] | D Loss: 1.0151 | G Loss: 0.8445\n",
      "Epoch [777/5000] | D Loss: 0.4639 | G Loss: 2.0500\n",
      "Epoch [778/5000] | D Loss: 1.0750 | G Loss: 0.8968\n",
      "Epoch [779/5000] | D Loss: 0.6303 | G Loss: 2.5304\n",
      "Epoch [780/5000] | D Loss: 0.4951 | G Loss: 2.8757\n",
      "Epoch [781/5000] | D Loss: 0.5912 | G Loss: 1.9739\n",
      "Epoch [782/5000] | D Loss: 0.4987 | G Loss: 4.1176\n",
      "Epoch [783/5000] | D Loss: 0.4659 | G Loss: 3.9636\n",
      "Epoch [784/5000] | D Loss: 0.6566 | G Loss: 1.6215\n",
      "Epoch [785/5000] | D Loss: 0.5893 | G Loss: 1.8762\n",
      "Epoch [786/5000] | D Loss: 0.5785 | G Loss: 2.2011\n",
      "Epoch [787/5000] | D Loss: 0.4858 | G Loss: 2.7190\n",
      "Epoch [788/5000] | D Loss: 0.7918 | G Loss: 2.6813\n",
      "Epoch [789/5000] | D Loss: 0.5168 | G Loss: 2.9688\n",
      "Epoch [790/5000] | D Loss: 0.7501 | G Loss: 3.0837\n",
      "Epoch [791/5000] | D Loss: 2.1344 | G Loss: 1.0755\n",
      "Epoch [792/5000] | D Loss: 0.5457 | G Loss: 2.0056\n",
      "Epoch [793/5000] | D Loss: 1.5738 | G Loss: 0.3510\n",
      "Epoch [794/5000] | D Loss: 1.1235 | G Loss: 0.6432\n",
      "Epoch [795/5000] | D Loss: 0.4801 | G Loss: 2.6103\n",
      "Epoch [796/5000] | D Loss: 0.8382 | G Loss: 1.0890\n",
      "Epoch [797/5000] | D Loss: 1.2821 | G Loss: 0.4915\n",
      "Epoch [798/5000] | D Loss: 0.5537 | G Loss: 2.2848\n",
      "Epoch [799/5000] | D Loss: 1.1139 | G Loss: 0.8780\n",
      "Epoch [800/5000] | D Loss: 0.4712 | G Loss: 3.1524\n",
      "Epoch 800 FID Score: 165.0877\n",
      "Epoch [801/5000] | D Loss: 0.5270 | G Loss: 3.0363\n",
      "Epoch [802/5000] | D Loss: 0.5130 | G Loss: 3.1472\n",
      "Epoch [803/5000] | D Loss: 1.7035 | G Loss: 0.6194\n",
      "Epoch [804/5000] | D Loss: 0.7108 | G Loss: 3.0637\n",
      "Epoch [805/5000] | D Loss: 0.4818 | G Loss: 3.1413\n",
      "Epoch [806/5000] | D Loss: 0.5352 | G Loss: 1.8187\n",
      "Epoch [807/5000] | D Loss: 0.6191 | G Loss: 2.7442\n",
      "Epoch [808/5000] | D Loss: 1.5259 | G Loss: 0.4276\n",
      "Epoch [809/5000] | D Loss: 2.0253 | G Loss: 0.7490\n",
      "Epoch [810/5000] | D Loss: 1.3103 | G Loss: 0.5477\n",
      "Epoch [811/5000] | D Loss: 0.4943 | G Loss: 3.1125\n",
      "Epoch [812/5000] | D Loss: 1.7060 | G Loss: 0.7561\n",
      "Epoch [813/5000] | D Loss: 0.4805 | G Loss: 2.9891\n",
      "Epoch [814/5000] | D Loss: 0.5252 | G Loss: 4.9949\n",
      "Epoch [815/5000] | D Loss: 0.5546 | G Loss: 2.2016\n",
      "Epoch [816/5000] | D Loss: 0.5000 | G Loss: 2.8176\n",
      "Epoch [817/5000] | D Loss: 0.9959 | G Loss: 1.5213\n",
      "Epoch [818/5000] | D Loss: 0.5474 | G Loss: 3.2875\n",
      "Epoch [819/5000] | D Loss: 0.9158 | G Loss: 0.8878\n",
      "Epoch [820/5000] | D Loss: 1.7578 | G Loss: 0.3627\n",
      "Epoch [821/5000] | D Loss: 0.7994 | G Loss: 0.8954\n",
      "Epoch [822/5000] | D Loss: 1.1465 | G Loss: 2.3664\n",
      "Epoch [823/5000] | D Loss: 0.6314 | G Loss: 1.6174\n",
      "Epoch [824/5000] | D Loss: 2.6416 | G Loss: 0.7140\n",
      "Epoch [825/5000] | D Loss: 1.2335 | G Loss: 0.8378\n",
      "Epoch [826/5000] | D Loss: 0.5252 | G Loss: 4.7103\n",
      "Epoch [827/5000] | D Loss: 0.6617 | G Loss: 2.0331\n",
      "Epoch [828/5000] | D Loss: 0.5736 | G Loss: 2.3281\n",
      "Epoch [829/5000] | D Loss: 1.7960 | G Loss: 0.7752\n",
      "Epoch [830/5000] | D Loss: 1.3470 | G Loss: 0.9738\n",
      "Epoch [831/5000] | D Loss: 0.4914 | G Loss: 3.7640\n",
      "Epoch [832/5000] | D Loss: 1.0442 | G Loss: 2.7946\n",
      "Epoch [833/5000] | D Loss: 0.6442 | G Loss: 2.6881\n",
      "Epoch [834/5000] | D Loss: 2.8442 | G Loss: 0.8204\n",
      "Epoch [835/5000] | D Loss: 0.5623 | G Loss: 2.6255\n",
      "Epoch [836/5000] | D Loss: 0.4833 | G Loss: 3.2872\n",
      "Epoch [837/5000] | D Loss: 0.5085 | G Loss: 3.7689\n",
      "Epoch [838/5000] | D Loss: 0.5496 | G Loss: 2.4171\n",
      "Epoch [839/5000] | D Loss: 0.5759 | G Loss: 3.8230\n",
      "Epoch [840/5000] | D Loss: 0.5629 | G Loss: 1.8666\n",
      "Epoch [841/5000] | D Loss: 0.5547 | G Loss: 2.5958\n",
      "Epoch [842/5000] | D Loss: 0.5990 | G Loss: 2.1352\n",
      "Epoch [843/5000] | D Loss: 0.5376 | G Loss: 1.5794\n",
      "Epoch [844/5000] | D Loss: 1.0714 | G Loss: 1.4458\n",
      "Epoch [845/5000] | D Loss: 0.4743 | G Loss: 4.1013\n",
      "Epoch [846/5000] | D Loss: 0.5067 | G Loss: 2.6133\n",
      "Epoch [847/5000] | D Loss: 0.7122 | G Loss: 3.1918\n",
      "Epoch [848/5000] | D Loss: 0.5168 | G Loss: 3.1388\n",
      "Epoch [849/5000] | D Loss: 0.6317 | G Loss: 1.2454\n",
      "Epoch [850/5000] | D Loss: 0.6235 | G Loss: 2.1742\n",
      "Epoch [851/5000] | D Loss: 0.5351 | G Loss: 2.4199\n",
      "Epoch [852/5000] | D Loss: 2.5633 | G Loss: 1.0506\n",
      "Epoch [853/5000] | D Loss: 0.5579 | G Loss: 3.8820\n",
      "Epoch [854/5000] | D Loss: 2.9278 | G Loss: 0.9783\n",
      "Epoch [855/5000] | D Loss: 1.3280 | G Loss: 0.6424\n",
      "Epoch [856/5000] | D Loss: 0.5290 | G Loss: 2.2614\n",
      "Epoch [857/5000] | D Loss: 0.6660 | G Loss: 2.8387\n",
      "Epoch [858/5000] | D Loss: 0.7424 | G Loss: 1.4156\n",
      "Epoch [859/5000] | D Loss: 1.3064 | G Loss: 1.1583\n",
      "Epoch [860/5000] | D Loss: 0.5115 | G Loss: 2.9345\n",
      "Epoch [861/5000] | D Loss: 0.6775 | G Loss: 2.6795\n",
      "Epoch [862/5000] | D Loss: 0.6312 | G Loss: 2.7743\n",
      "Epoch [863/5000] | D Loss: 0.7345 | G Loss: 1.6430\n",
      "Epoch [864/5000] | D Loss: 1.2341 | G Loss: 0.5009\n",
      "Epoch [865/5000] | D Loss: 0.4936 | G Loss: 2.5793\n",
      "Epoch [866/5000] | D Loss: 0.6577 | G Loss: 1.8367\n",
      "Epoch [867/5000] | D Loss: 0.4906 | G Loss: 4.1548\n",
      "Epoch [868/5000] | D Loss: 0.4592 | G Loss: 2.7281\n",
      "Epoch [869/5000] | D Loss: 2.8828 | G Loss: 0.8529\n",
      "Epoch [870/5000] | D Loss: 1.8568 | G Loss: 0.2712\n",
      "Epoch [871/5000] | D Loss: 1.0113 | G Loss: 2.0901\n",
      "Epoch [872/5000] | D Loss: 0.5519 | G Loss: 2.3558\n",
      "Epoch [873/5000] | D Loss: 0.6248 | G Loss: 1.1551\n",
      "Epoch [874/5000] | D Loss: 0.5301 | G Loss: 4.1506\n",
      "Epoch [875/5000] | D Loss: 0.4796 | G Loss: 3.1254\n",
      "Epoch [876/5000] | D Loss: 0.6393 | G Loss: 2.2128\n",
      "Epoch [877/5000] | D Loss: 0.6115 | G Loss: 3.9819\n",
      "Epoch [878/5000] | D Loss: 1.7531 | G Loss: 0.7636\n",
      "Epoch [879/5000] | D Loss: 0.4757 | G Loss: 2.8072\n",
      "Epoch [880/5000] | D Loss: 0.6543 | G Loss: 2.2329\n",
      "Epoch [881/5000] | D Loss: 0.5212 | G Loss: 1.6280\n",
      "Epoch [882/5000] | D Loss: 0.5097 | G Loss: 3.0978\n",
      "Epoch [883/5000] | D Loss: 0.6246 | G Loss: 1.4693\n",
      "Epoch [884/5000] | D Loss: 0.6447 | G Loss: 2.6536\n",
      "Epoch [885/5000] | D Loss: 1.8514 | G Loss: 1.3082\n",
      "Epoch [886/5000] | D Loss: 0.6302 | G Loss: 0.9330\n",
      "Epoch [887/5000] | D Loss: 1.7617 | G Loss: 0.6739\n",
      "Epoch [888/5000] | D Loss: 0.5126 | G Loss: 3.4300\n",
      "Epoch [889/5000] | D Loss: 3.1185 | G Loss: 1.8884\n",
      "Epoch [890/5000] | D Loss: 0.6283 | G Loss: 2.2541\n",
      "Epoch [891/5000] | D Loss: 0.5032 | G Loss: 2.2011\n",
      "Epoch [892/5000] | D Loss: 0.9862 | G Loss: 0.6034\n",
      "Epoch [893/5000] | D Loss: 0.5216 | G Loss: 4.6859\n",
      "Epoch [894/5000] | D Loss: 2.1443 | G Loss: 0.2313\n",
      "Epoch [895/5000] | D Loss: 0.5958 | G Loss: 5.0243\n",
      "Epoch [896/5000] | D Loss: 0.6675 | G Loss: 1.9684\n",
      "Epoch [897/5000] | D Loss: 0.6776 | G Loss: 2.3067\n",
      "Epoch [898/5000] | D Loss: 0.4949 | G Loss: 2.6895\n",
      "Epoch [899/5000] | D Loss: 2.3275 | G Loss: 0.4338\n",
      "Epoch [900/5000] | D Loss: 0.6722 | G Loss: 2.6943\n",
      "Epoch 900 FID Score: 158.6578\n",
      "Epoch [901/5000] | D Loss: 0.5658 | G Loss: 1.5083\n",
      "Epoch [902/5000] | D Loss: 0.7289 | G Loss: 1.9281\n",
      "Epoch [903/5000] | D Loss: 0.9374 | G Loss: 1.1877\n",
      "Epoch [904/5000] | D Loss: 1.4300 | G Loss: 0.6926\n",
      "Epoch [905/5000] | D Loss: 0.5325 | G Loss: 2.8457\n",
      "Epoch [906/5000] | D Loss: 0.4852 | G Loss: 2.7920\n",
      "Epoch [907/5000] | D Loss: 0.5726 | G Loss: 2.4430\n",
      "Epoch [908/5000] | D Loss: 0.6227 | G Loss: 2.0250\n",
      "Epoch [909/5000] | D Loss: 0.7064 | G Loss: 1.1973\n",
      "Epoch [910/5000] | D Loss: 0.7401 | G Loss: 1.1749\n",
      "Epoch [911/5000] | D Loss: 0.5334 | G Loss: 2.8836\n",
      "Epoch [912/5000] | D Loss: 0.9847 | G Loss: 0.7997\n",
      "Epoch [913/5000] | D Loss: 0.4539 | G Loss: 3.3445\n",
      "Epoch [914/5000] | D Loss: 0.8390 | G Loss: 0.9838\n",
      "Epoch [915/5000] | D Loss: 0.4963 | G Loss: 1.8872\n",
      "Epoch [916/5000] | D Loss: 0.5024 | G Loss: 3.3710\n",
      "Epoch [917/5000] | D Loss: 0.8005 | G Loss: 2.6367\n",
      "Epoch [918/5000] | D Loss: 1.4251 | G Loss: 0.7702\n",
      "Epoch [919/5000] | D Loss: 3.1999 | G Loss: 4.3880\n",
      "Epoch [920/5000] | D Loss: 0.4799 | G Loss: 2.8294\n",
      "Epoch [921/5000] | D Loss: 1.7468 | G Loss: 0.2604\n",
      "Epoch [922/5000] | D Loss: 0.4843 | G Loss: 3.5935\n",
      "Epoch [923/5000] | D Loss: 1.2386 | G Loss: 0.3943\n",
      "Epoch [924/5000] | D Loss: 1.1225 | G Loss: 1.1079\n",
      "Epoch [925/5000] | D Loss: 0.5124 | G Loss: 3.9252\n",
      "Epoch [926/5000] | D Loss: 1.1003 | G Loss: 0.7885\n",
      "Epoch [927/5000] | D Loss: 0.4505 | G Loss: 3.9536\n",
      "Epoch [928/5000] | D Loss: 0.6071 | G Loss: 2.1996\n",
      "Epoch [929/5000] | D Loss: 0.4914 | G Loss: 3.7468\n",
      "Epoch [930/5000] | D Loss: 0.6007 | G Loss: 0.9881\n",
      "Epoch [931/5000] | D Loss: 0.8599 | G Loss: 0.8385\n",
      "Epoch [932/5000] | D Loss: 0.8511 | G Loss: 0.8175\n",
      "Epoch [933/5000] | D Loss: 1.9342 | G Loss: 0.4645\n",
      "Epoch [934/5000] | D Loss: 1.0147 | G Loss: 0.9434\n",
      "Epoch [935/5000] | D Loss: 0.7488 | G Loss: 1.8210\n",
      "Epoch [936/5000] | D Loss: 0.8119 | G Loss: 1.0524\n",
      "Epoch [937/5000] | D Loss: 0.6403 | G Loss: 2.1469\n",
      "Epoch [938/5000] | D Loss: 0.4828 | G Loss: 3.4733\n",
      "Epoch [939/5000] | D Loss: 0.5060 | G Loss: 2.9364\n",
      "Epoch [940/5000] | D Loss: 0.5009 | G Loss: 3.4701\n",
      "Epoch [941/5000] | D Loss: 1.4319 | G Loss: 1.6763\n",
      "Epoch [942/5000] | D Loss: 1.6498 | G Loss: 2.5345\n",
      "Epoch [943/5000] | D Loss: 1.4576 | G Loss: 0.4461\n",
      "Epoch [944/5000] | D Loss: 0.7244 | G Loss: 1.5904\n",
      "Epoch [945/5000] | D Loss: 0.6612 | G Loss: 2.3350\n",
      "Epoch [946/5000] | D Loss: 1.7194 | G Loss: 0.7347\n",
      "Epoch [947/5000] | D Loss: 1.0423 | G Loss: 0.8364\n",
      "Epoch [948/5000] | D Loss: 0.4898 | G Loss: 3.2841\n",
      "Epoch [949/5000] | D Loss: 0.5511 | G Loss: 4.7831\n",
      "Epoch [950/5000] | D Loss: 0.7137 | G Loss: 2.2815\n",
      "Epoch [951/5000] | D Loss: 0.8704 | G Loss: 2.0116\n",
      "Epoch [952/5000] | D Loss: 0.4890 | G Loss: 3.3898\n",
      "Epoch [953/5000] | D Loss: 0.5967 | G Loss: 2.1727\n",
      "Epoch [954/5000] | D Loss: 0.4817 | G Loss: 3.0331\n",
      "Epoch [955/5000] | D Loss: 0.4783 | G Loss: 2.4728\n",
      "Epoch [956/5000] | D Loss: 2.3699 | G Loss: 1.9658\n",
      "Epoch [957/5000] | D Loss: 1.1097 | G Loss: 1.0814\n",
      "Epoch [958/5000] | D Loss: 1.0589 | G Loss: 0.6692\n",
      "Epoch [959/5000] | D Loss: 0.5459 | G Loss: 1.5946\n",
      "Epoch [960/5000] | D Loss: 0.4860 | G Loss: 3.0120\n",
      "Epoch [961/5000] | D Loss: 2.9665 | G Loss: 2.3518\n",
      "Epoch [962/5000] | D Loss: 0.5647 | G Loss: 3.1048\n",
      "Epoch [963/5000] | D Loss: 0.4870 | G Loss: 3.5381\n",
      "Epoch [964/5000] | D Loss: 1.2438 | G Loss: 1.3663\n",
      "Epoch [965/5000] | D Loss: 0.8615 | G Loss: 1.4872\n",
      "Epoch [966/5000] | D Loss: 1.7518 | G Loss: 0.3775\n",
      "Epoch [967/5000] | D Loss: 0.8221 | G Loss: 0.9105\n",
      "Epoch [968/5000] | D Loss: 1.2789 | G Loss: 0.4959\n",
      "Epoch [969/5000] | D Loss: 0.5825 | G Loss: 1.7973\n",
      "Epoch [970/5000] | D Loss: 0.4895 | G Loss: 3.7102\n",
      "Epoch [971/5000] | D Loss: 0.5351 | G Loss: 3.5169\n",
      "Epoch [972/5000] | D Loss: 0.6574 | G Loss: 3.2278\n",
      "Epoch [973/5000] | D Loss: 1.3617 | G Loss: 0.4271\n",
      "Epoch [974/5000] | D Loss: 0.5017 | G Loss: 3.4996\n",
      "Epoch [975/5000] | D Loss: 0.7212 | G Loss: 2.2030\n",
      "Epoch [976/5000] | D Loss: 1.2462 | G Loss: 1.2552\n",
      "Epoch [977/5000] | D Loss: 0.5915 | G Loss: 2.6262\n",
      "Epoch [978/5000] | D Loss: 0.8831 | G Loss: 1.2644\n",
      "Epoch [979/5000] | D Loss: 0.5542 | G Loss: 2.9742\n",
      "Epoch [980/5000] | D Loss: 1.5318 | G Loss: 0.2982\n",
      "Epoch [981/5000] | D Loss: 0.6589 | G Loss: 3.2085\n",
      "Epoch [982/5000] | D Loss: 1.9323 | G Loss: 0.3855\n",
      "Epoch [983/5000] | D Loss: 2.7663 | G Loss: 0.6742\n",
      "Epoch [984/5000] | D Loss: 0.7919 | G Loss: 1.4021\n",
      "Epoch [985/5000] | D Loss: 0.8715 | G Loss: 1.6322\n",
      "Epoch [986/5000] | D Loss: 0.5026 | G Loss: 3.2383\n",
      "Epoch [987/5000] | D Loss: 0.5151 | G Loss: 2.9701\n",
      "Epoch [988/5000] | D Loss: 1.0047 | G Loss: 1.5625\n",
      "Epoch [989/5000] | D Loss: 0.5515 | G Loss: 4.9016\n",
      "Epoch [990/5000] | D Loss: 0.5430 | G Loss: 2.7728\n",
      "Epoch [991/5000] | D Loss: 0.7909 | G Loss: 0.9443\n",
      "Epoch [992/5000] | D Loss: 0.8053 | G Loss: 2.6410\n",
      "Epoch [993/5000] | D Loss: 0.6332 | G Loss: 0.9709\n",
      "Epoch [994/5000] | D Loss: 0.7099 | G Loss: 2.1856\n",
      "Epoch [995/5000] | D Loss: 0.7726 | G Loss: 1.9393\n",
      "Epoch [996/5000] | D Loss: 0.7996 | G Loss: 1.6340\n",
      "Epoch [997/5000] | D Loss: 1.1070 | G Loss: 0.7617\n",
      "Epoch [998/5000] | D Loss: 0.5265 | G Loss: 2.8385\n",
      "Epoch [999/5000] | D Loss: 0.8304 | G Loss: 1.4907\n",
      "Epoch [1000/5000] | D Loss: 0.7494 | G Loss: 3.1567\n",
      "Epoch 1000 FID Score: 158.1097\n",
      "Epoch [1001/5000] | D Loss: 1.7546 | G Loss: 2.6914\n",
      "Epoch [1002/5000] | D Loss: 0.4994 | G Loss: 2.7938\n",
      "Epoch [1003/5000] | D Loss: 2.1160 | G Loss: 0.2880\n",
      "Epoch [1004/5000] | D Loss: 0.5925 | G Loss: 2.1714\n",
      "Epoch [1005/5000] | D Loss: 0.7176 | G Loss: 1.8364\n",
      "Epoch [1006/5000] | D Loss: 2.1924 | G Loss: 0.4693\n",
      "Epoch [1007/5000] | D Loss: 1.4349 | G Loss: 0.5931\n",
      "Epoch [1008/5000] | D Loss: 0.5703 | G Loss: 3.6229\n",
      "Epoch [1009/5000] | D Loss: 0.5781 | G Loss: 1.5261\n",
      "Epoch [1010/5000] | D Loss: 1.8689 | G Loss: 1.7640\n",
      "Epoch [1011/5000] | D Loss: 0.9820 | G Loss: 1.1271\n",
      "Epoch [1012/5000] | D Loss: 0.6549 | G Loss: 2.1334\n",
      "Epoch [1013/5000] | D Loss: 0.8029 | G Loss: 2.1571\n",
      "Epoch [1014/5000] | D Loss: 0.7719 | G Loss: 2.7923\n",
      "Epoch [1015/5000] | D Loss: 1.0712 | G Loss: 0.8301\n",
      "Epoch [1016/5000] | D Loss: 0.7221 | G Loss: 1.7691\n",
      "Epoch [1017/5000] | D Loss: 0.5392 | G Loss: 2.3200\n",
      "Epoch [1018/5000] | D Loss: 0.5287 | G Loss: 2.8257\n",
      "Epoch [1019/5000] | D Loss: 0.7957 | G Loss: 1.6216\n",
      "Epoch [1020/5000] | D Loss: 1.3258 | G Loss: 0.5799\n",
      "Epoch [1021/5000] | D Loss: 1.7993 | G Loss: 0.9693\n",
      "Epoch [1022/5000] | D Loss: 0.4977 | G Loss: 3.2525\n",
      "Epoch [1023/5000] | D Loss: 1.7943 | G Loss: 0.9686\n",
      "Epoch [1024/5000] | D Loss: 0.5782 | G Loss: 2.9521\n",
      "Epoch [1025/5000] | D Loss: 0.4883 | G Loss: 3.0963\n",
      "Epoch [1026/5000] | D Loss: 0.8268 | G Loss: 1.6488\n",
      "Epoch [1027/5000] | D Loss: 1.2154 | G Loss: 1.6136\n",
      "Epoch [1028/5000] | D Loss: 1.5480 | G Loss: 1.3296\n",
      "Epoch [1029/5000] | D Loss: 2.6697 | G Loss: 0.5265\n",
      "Epoch [1030/5000] | D Loss: 1.2853 | G Loss: 2.3008\n",
      "Epoch [1031/5000] | D Loss: 0.5276 | G Loss: 2.2257\n",
      "Epoch [1032/5000] | D Loss: 0.6225 | G Loss: 2.6140\n",
      "Epoch [1033/5000] | D Loss: 0.5671 | G Loss: 3.0687\n",
      "Epoch [1034/5000] | D Loss: 0.6361 | G Loss: 2.1161\n",
      "Epoch [1035/5000] | D Loss: 1.7535 | G Loss: 1.6331\n",
      "Epoch [1036/5000] | D Loss: 0.5725 | G Loss: 2.5085\n",
      "Epoch [1037/5000] | D Loss: 0.8012 | G Loss: 1.7382\n",
      "Epoch [1038/5000] | D Loss: 0.5144 | G Loss: 2.1582\n",
      "Epoch [1039/5000] | D Loss: 1.1792 | G Loss: 0.5284\n",
      "Epoch [1040/5000] | D Loss: 0.5025 | G Loss: 2.9890\n",
      "Epoch [1041/5000] | D Loss: 1.0336 | G Loss: 1.4398\n",
      "Epoch [1042/5000] | D Loss: 1.1415 | G Loss: 0.7860\n",
      "Epoch [1043/5000] | D Loss: 0.5756 | G Loss: 4.3011\n",
      "Epoch [1044/5000] | D Loss: 0.8599 | G Loss: 1.2240\n",
      "Epoch [1045/5000] | D Loss: 0.8721 | G Loss: 1.0458\n",
      "Epoch [1046/5000] | D Loss: 0.4873 | G Loss: 2.9576\n",
      "Epoch [1047/5000] | D Loss: 0.7573 | G Loss: 2.2042\n",
      "Epoch [1048/5000] | D Loss: 1.4666 | G Loss: 0.7497\n",
      "Epoch [1049/5000] | D Loss: 1.6680 | G Loss: 0.2697\n",
      "Epoch [1050/5000] | D Loss: 0.5494 | G Loss: 3.0420\n",
      "Epoch [1051/5000] | D Loss: 0.5292 | G Loss: 2.9174\n",
      "Epoch [1052/5000] | D Loss: 0.5108 | G Loss: 2.7815\n",
      "Epoch [1053/5000] | D Loss: 0.8979 | G Loss: 2.0896\n",
      "Epoch [1054/5000] | D Loss: 1.1011 | G Loss: 1.5812\n",
      "Epoch [1055/5000] | D Loss: 0.5613 | G Loss: 3.1703\n",
      "Epoch [1056/5000] | D Loss: 0.8836 | G Loss: 2.4164\n",
      "Epoch [1057/5000] | D Loss: 0.8032 | G Loss: 1.7368\n",
      "Epoch [1058/5000] | D Loss: 0.8651 | G Loss: 2.4686\n",
      "Epoch [1059/5000] | D Loss: 1.9042 | G Loss: 0.4547\n",
      "Epoch [1060/5000] | D Loss: 0.9093 | G Loss: 1.7487\n",
      "Epoch [1061/5000] | D Loss: 1.1483 | G Loss: 0.8506\n",
      "Epoch [1062/5000] | D Loss: 0.5683 | G Loss: 1.8920\n",
      "Epoch [1063/5000] | D Loss: 1.5870 | G Loss: 0.8559\n",
      "Epoch [1064/5000] | D Loss: 0.8011 | G Loss: 1.5360\n",
      "Epoch [1065/5000] | D Loss: 1.0124 | G Loss: 1.7200\n",
      "Epoch [1066/5000] | D Loss: 0.6308 | G Loss: 2.2845\n",
      "Epoch [1067/5000] | D Loss: 0.8011 | G Loss: 3.1776\n",
      "Epoch [1068/5000] | D Loss: 0.6297 | G Loss: 2.5680\n",
      "Epoch [1069/5000] | D Loss: 1.3142 | G Loss: 0.8169\n",
      "Epoch [1070/5000] | D Loss: 0.6634 | G Loss: 2.5291\n",
      "Epoch [1071/5000] | D Loss: 1.5681 | G Loss: 0.3254\n",
      "Epoch [1072/5000] | D Loss: 3.4119 | G Loss: 1.3313\n",
      "Epoch [1073/5000] | D Loss: 2.9141 | G Loss: 1.5182\n",
      "Epoch [1074/5000] | D Loss: 1.5357 | G Loss: 0.3792\n",
      "Epoch [1075/5000] | D Loss: 0.7344 | G Loss: 2.0417\n",
      "Epoch [1076/5000] | D Loss: 0.6889 | G Loss: 2.2313\n",
      "Epoch [1077/5000] | D Loss: 1.4421 | G Loss: 0.4974\n",
      "Epoch [1078/5000] | D Loss: 1.1541 | G Loss: 0.8994\n",
      "Epoch [1079/5000] | D Loss: 0.5130 | G Loss: 3.0756\n",
      "Epoch [1080/5000] | D Loss: 0.9102 | G Loss: 1.3515\n",
      "Epoch [1081/5000] | D Loss: 0.6281 | G Loss: 2.4464\n",
      "Epoch [1082/5000] | D Loss: 1.1685 | G Loss: 1.4511\n",
      "Epoch [1083/5000] | D Loss: 0.4884 | G Loss: 4.2552\n",
      "Epoch [1084/5000] | D Loss: 0.5000 | G Loss: 2.5201\n",
      "Epoch [1085/5000] | D Loss: 0.7197 | G Loss: 1.2768\n",
      "Epoch [1086/5000] | D Loss: 0.8109 | G Loss: 2.2225\n",
      "Epoch [1087/5000] | D Loss: 0.8051 | G Loss: 1.5681\n",
      "Epoch [1088/5000] | D Loss: 0.5308 | G Loss: 3.0587\n",
      "Epoch [1089/5000] | D Loss: 0.5583 | G Loss: 4.1073\n",
      "Epoch [1090/5000] | D Loss: 0.6200 | G Loss: 1.7517\n",
      "Epoch [1091/5000] | D Loss: 0.5933 | G Loss: 2.5051\n",
      "Epoch [1092/5000] | D Loss: 0.6327 | G Loss: 4.1708\n",
      "Epoch [1093/5000] | D Loss: 3.0997 | G Loss: 1.8521\n",
      "Epoch [1094/5000] | D Loss: 0.4826 | G Loss: 2.7138\n",
      "Epoch [1095/5000] | D Loss: 0.5039 | G Loss: 3.2108\n",
      "Epoch [1096/5000] | D Loss: 0.6417 | G Loss: 2.5488\n",
      "Epoch [1097/5000] | D Loss: 0.5041 | G Loss: 1.8612\n",
      "Epoch [1098/5000] | D Loss: 0.6179 | G Loss: 2.2261\n",
      "Epoch [1099/5000] | D Loss: 0.6894 | G Loss: 2.0298\n",
      "Epoch [1100/5000] | D Loss: 0.7390 | G Loss: 1.9124\n",
      "Epoch 1100 FID Score: 142.6797\n",
      "Epoch [1101/5000] | D Loss: 1.7083 | G Loss: 0.8063\n",
      "Epoch [1102/5000] | D Loss: 1.0890 | G Loss: 0.9526\n",
      "Epoch [1103/5000] | D Loss: 1.6262 | G Loss: 0.2946\n",
      "Epoch [1104/5000] | D Loss: 0.5458 | G Loss: 1.6927\n",
      "Epoch [1105/5000] | D Loss: 0.4957 | G Loss: 2.6501\n",
      "Epoch [1106/5000] | D Loss: 0.5461 | G Loss: 5.3212\n",
      "Epoch [1107/5000] | D Loss: 2.1958 | G Loss: 0.7853\n",
      "Epoch [1108/5000] | D Loss: 2.4475 | G Loss: 2.2239\n",
      "Epoch [1109/5000] | D Loss: 0.5666 | G Loss: 2.6029\n",
      "Epoch [1110/5000] | D Loss: 0.4754 | G Loss: 2.5428\n",
      "Epoch [1111/5000] | D Loss: 0.5173 | G Loss: 3.7611\n",
      "Epoch [1112/5000] | D Loss: 1.7903 | G Loss: 0.7867\n",
      "Epoch [1113/5000] | D Loss: 1.9527 | G Loss: 0.6453\n",
      "Epoch [1114/5000] | D Loss: 2.9469 | G Loss: 1.9354\n",
      "Epoch [1115/5000] | D Loss: 0.7478 | G Loss: 1.0960\n",
      "Epoch [1116/5000] | D Loss: 0.5486 | G Loss: 2.4757\n",
      "Epoch [1117/5000] | D Loss: 1.0574 | G Loss: 2.2151\n",
      "Epoch [1118/5000] | D Loss: 0.4718 | G Loss: 2.5675\n",
      "Epoch [1119/5000] | D Loss: 2.6816 | G Loss: 2.0501\n",
      "Epoch [1120/5000] | D Loss: 2.1069 | G Loss: 0.3824\n",
      "Epoch [1121/5000] | D Loss: 0.6421 | G Loss: 1.5401\n",
      "Epoch [1122/5000] | D Loss: 2.7013 | G Loss: 1.0435\n",
      "Epoch [1123/5000] | D Loss: 0.9603 | G Loss: 1.2993\n",
      "Epoch [1124/5000] | D Loss: 2.3614 | G Loss: 0.2573\n",
      "Epoch [1125/5000] | D Loss: 0.4885 | G Loss: 2.9587\n",
      "Epoch [1126/5000] | D Loss: 3.2725 | G Loss: 1.7931\n",
      "Epoch [1127/5000] | D Loss: 0.5127 | G Loss: 3.2518\n",
      "Epoch [1128/5000] | D Loss: 0.5132 | G Loss: 4.8637\n",
      "Epoch [1129/5000] | D Loss: 3.7528 | G Loss: 1.4084\n",
      "Epoch [1130/5000] | D Loss: 2.0379 | G Loss: 0.5868\n",
      "Epoch [1131/5000] | D Loss: 0.8259 | G Loss: 2.8010\n",
      "Epoch [1132/5000] | D Loss: 2.0354 | G Loss: 0.7286\n",
      "Epoch [1133/5000] | D Loss: 0.5406 | G Loss: 2.2481\n",
      "Epoch [1134/5000] | D Loss: 0.4975 | G Loss: 3.1084\n",
      "Epoch [1135/5000] | D Loss: 0.6971 | G Loss: 1.5915\n",
      "Epoch [1136/5000] | D Loss: 1.5077 | G Loss: 0.7714\n",
      "Epoch [1137/5000] | D Loss: 1.2831 | G Loss: 1.7456\n",
      "Epoch [1138/5000] | D Loss: 0.8171 | G Loss: 1.8045\n",
      "Epoch [1139/5000] | D Loss: 1.8482 | G Loss: 0.8267\n",
      "Epoch [1140/5000] | D Loss: 1.7183 | G Loss: 1.6660\n",
      "Epoch [1141/5000] | D Loss: 0.5069 | G Loss: 2.7535\n",
      "Epoch [1142/5000] | D Loss: 0.4758 | G Loss: 3.3807\n",
      "Epoch [1143/5000] | D Loss: 2.2818 | G Loss: 0.4609\n",
      "Epoch [1144/5000] | D Loss: 1.0391 | G Loss: 1.5583\n",
      "Epoch [1145/5000] | D Loss: 0.6319 | G Loss: 1.7353\n",
      "Epoch [1146/5000] | D Loss: 0.8904 | G Loss: 2.5261\n",
      "Epoch [1147/5000] | D Loss: 0.7613 | G Loss: 1.2288\n",
      "Epoch [1148/5000] | D Loss: 0.9501 | G Loss: 1.0134\n",
      "Epoch [1149/5000] | D Loss: 0.7275 | G Loss: 1.5123\n",
      "Epoch [1150/5000] | D Loss: 0.9895 | G Loss: 1.9870\n",
      "Epoch [1151/5000] | D Loss: 0.5014 | G Loss: 3.2008\n",
      "Epoch [1152/5000] | D Loss: 0.8289 | G Loss: 2.4431\n",
      "Epoch [1153/5000] | D Loss: 0.5181 | G Loss: 4.0218\n",
      "Epoch [1154/5000] | D Loss: 0.5168 | G Loss: 2.2566\n",
      "Epoch [1155/5000] | D Loss: 0.8724 | G Loss: 1.0760\n",
      "Epoch [1156/5000] | D Loss: 0.5303 | G Loss: 2.8046\n",
      "Epoch [1157/5000] | D Loss: 0.5186 | G Loss: 3.8181\n",
      "Epoch [1158/5000] | D Loss: 2.5645 | G Loss: 0.5915\n",
      "Epoch [1159/5000] | D Loss: 0.7245 | G Loss: 3.0088\n",
      "Epoch [1160/5000] | D Loss: 0.5473 | G Loss: 3.7954\n",
      "Epoch [1161/5000] | D Loss: 0.5566 | G Loss: 3.8669\n",
      "Epoch [1162/5000] | D Loss: 3.0765 | G Loss: 1.4718\n",
      "Epoch [1163/5000] | D Loss: 2.0804 | G Loss: 0.3206\n",
      "Epoch [1164/5000] | D Loss: 0.9661 | G Loss: 0.7257\n",
      "Epoch [1165/5000] | D Loss: 0.6337 | G Loss: 2.0117\n",
      "Epoch [1166/5000] | D Loss: 1.5112 | G Loss: 1.0045\n",
      "Epoch [1167/5000] | D Loss: 2.9175 | G Loss: 0.9762\n",
      "Epoch [1168/5000] | D Loss: 1.2952 | G Loss: 0.8934\n",
      "Epoch [1169/5000] | D Loss: 0.7596 | G Loss: 2.6736\n",
      "Epoch [1170/5000] | D Loss: 0.5178 | G Loss: 1.6833\n",
      "Epoch [1171/5000] | D Loss: 0.5788 | G Loss: 2.7400\n",
      "Epoch [1172/5000] | D Loss: 0.5004 | G Loss: 2.3273\n",
      "Epoch [1173/5000] | D Loss: 0.6952 | G Loss: 2.6857\n",
      "Epoch [1174/5000] | D Loss: 0.8965 | G Loss: 1.4498\n",
      "Epoch [1175/5000] | D Loss: 0.7688 | G Loss: 2.8093\n",
      "Epoch [1176/5000] | D Loss: 0.5909 | G Loss: 2.9400\n",
      "Epoch [1177/5000] | D Loss: 2.5178 | G Loss: 0.5917\n",
      "Epoch [1178/5000] | D Loss: 0.5253 | G Loss: 3.5044\n",
      "Epoch [1179/5000] | D Loss: 1.7639 | G Loss: 0.6089\n",
      "Epoch [1180/5000] | D Loss: 1.8934 | G Loss: 0.9920\n",
      "Epoch [1181/5000] | D Loss: 0.7945 | G Loss: 1.8222\n",
      "Epoch [1182/5000] | D Loss: 2.6258 | G Loss: 0.7293\n",
      "Epoch [1183/5000] | D Loss: 0.7373 | G Loss: 2.5141\n",
      "Epoch [1184/5000] | D Loss: 0.8996 | G Loss: 1.5413\n",
      "Epoch [1185/5000] | D Loss: 0.8067 | G Loss: 1.7413\n",
      "Epoch [1186/5000] | D Loss: 0.4927 | G Loss: 3.3114\n",
      "Epoch [1187/5000] | D Loss: 1.7044 | G Loss: 0.4630\n",
      "Epoch [1188/5000] | D Loss: 0.9876 | G Loss: 0.6841\n",
      "Epoch [1189/5000] | D Loss: 0.5447 | G Loss: 3.4010\n",
      "Epoch [1190/5000] | D Loss: 0.6327 | G Loss: 2.4799\n",
      "Epoch [1191/5000] | D Loss: 1.9106 | G Loss: 1.2924\n",
      "Epoch [1192/5000] | D Loss: 0.6759 | G Loss: 2.0195\n",
      "Epoch [1193/5000] | D Loss: 0.5207 | G Loss: 2.7773\n",
      "Epoch [1194/5000] | D Loss: 1.8891 | G Loss: 1.6994\n",
      "Epoch [1195/5000] | D Loss: 2.7550 | G Loss: 1.0308\n",
      "Epoch [1196/5000] | D Loss: 1.3734 | G Loss: 0.9416\n",
      "Epoch [1197/5000] | D Loss: 0.9994 | G Loss: 0.9653\n",
      "Epoch [1198/5000] | D Loss: 0.5318 | G Loss: 2.4574\n",
      "Epoch [1199/5000] | D Loss: 0.5577 | G Loss: 3.3675\n",
      "Epoch [1200/5000] | D Loss: 2.7631 | G Loss: 0.3527\n",
      "Epoch 1200 FID Score: 141.5210\n",
      "Epoch [1201/5000] | D Loss: 1.0782 | G Loss: 1.2153\n",
      "Epoch [1202/5000] | D Loss: 0.8938 | G Loss: 1.2455\n",
      "Epoch [1203/5000] | D Loss: 0.7960 | G Loss: 1.3152\n",
      "Epoch [1204/5000] | D Loss: 2.8629 | G Loss: 1.1922\n",
      "Epoch [1205/5000] | D Loss: 1.0921 | G Loss: 1.1776\n",
      "Epoch [1206/5000] | D Loss: 1.7256 | G Loss: 1.2383\n",
      "Epoch [1207/5000] | D Loss: 0.6927 | G Loss: 1.9346\n",
      "Epoch [1208/5000] | D Loss: 1.2180 | G Loss: 1.3716\n",
      "Epoch [1209/5000] | D Loss: 0.5308 | G Loss: 2.0016\n",
      "Epoch [1210/5000] | D Loss: 1.8867 | G Loss: 0.4021\n",
      "Epoch [1211/5000] | D Loss: 2.6300 | G Loss: 0.7514\n",
      "Epoch [1212/5000] | D Loss: 1.1394 | G Loss: 2.2224\n",
      "Epoch [1213/5000] | D Loss: 1.3021 | G Loss: 0.4304\n",
      "Epoch [1214/5000] | D Loss: 0.5191 | G Loss: 2.7038\n",
      "Epoch [1215/5000] | D Loss: 2.6157 | G Loss: 1.2050\n",
      "Epoch [1216/5000] | D Loss: 2.6458 | G Loss: 0.2293\n",
      "Epoch [1217/5000] | D Loss: 0.9300 | G Loss: 1.7698\n",
      "Epoch [1218/5000] | D Loss: 0.8730 | G Loss: 2.0095\n",
      "Epoch [1219/5000] | D Loss: 0.6778 | G Loss: 2.3670\n",
      "Epoch [1220/5000] | D Loss: 0.6019 | G Loss: 2.1860\n",
      "Epoch [1221/5000] | D Loss: 1.3218 | G Loss: 1.0933\n",
      "Epoch [1222/5000] | D Loss: 1.2503 | G Loss: 1.3489\n",
      "Epoch [1223/5000] | D Loss: 0.5787 | G Loss: 1.8544\n",
      "Epoch [1224/5000] | D Loss: 1.1189 | G Loss: 1.5062\n",
      "Epoch [1225/5000] | D Loss: 1.1741 | G Loss: 0.7135\n",
      "Epoch [1226/5000] | D Loss: 2.1375 | G Loss: 0.6546\n",
      "Epoch [1227/5000] | D Loss: 0.4846 | G Loss: 2.1507\n",
      "Epoch [1228/5000] | D Loss: 3.2151 | G Loss: 0.6649\n",
      "Epoch [1229/5000] | D Loss: 0.4835 | G Loss: 3.4368\n",
      "Epoch [1230/5000] | D Loss: 2.5302 | G Loss: 1.8782\n",
      "Epoch [1231/5000] | D Loss: 2.2966 | G Loss: 1.9235\n",
      "Epoch [1232/5000] | D Loss: 0.7199 | G Loss: 2.3953\n",
      "Epoch [1233/5000] | D Loss: 0.6063 | G Loss: 1.9358\n",
      "Epoch [1234/5000] | D Loss: 0.7229 | G Loss: 2.8387\n",
      "Epoch [1235/5000] | D Loss: 0.5292 | G Loss: 2.5234\n",
      "Epoch [1236/5000] | D Loss: 0.6943 | G Loss: 2.3972\n",
      "Epoch [1237/5000] | D Loss: 0.5495 | G Loss: 1.9739\n",
      "Epoch [1238/5000] | D Loss: 0.4701 | G Loss: 2.8523\n",
      "Epoch [1239/5000] | D Loss: 2.3082 | G Loss: 0.2774\n",
      "Epoch [1240/5000] | D Loss: 0.8412 | G Loss: 1.5348\n",
      "Epoch [1241/5000] | D Loss: 2.8467 | G Loss: 0.6736\n",
      "Epoch [1242/5000] | D Loss: 2.2304 | G Loss: 0.4681\n",
      "Epoch [1243/5000] | D Loss: 0.6597 | G Loss: 1.3924\n",
      "Epoch [1244/5000] | D Loss: 0.7390 | G Loss: 2.1556\n",
      "Epoch [1245/5000] | D Loss: 1.2150 | G Loss: 0.9553\n",
      "Epoch [1246/5000] | D Loss: 1.1648 | G Loss: 0.9263\n",
      "Epoch [1247/5000] | D Loss: 0.9592 | G Loss: 0.9521\n",
      "Epoch [1248/5000] | D Loss: 0.6987 | G Loss: 3.4200\n",
      "Epoch [1249/5000] | D Loss: 0.4729 | G Loss: 3.0625\n",
      "Epoch [1250/5000] | D Loss: 0.5474 | G Loss: 3.5471\n",
      "Epoch [1251/5000] | D Loss: 0.7905 | G Loss: 1.4879\n",
      "Epoch [1252/5000] | D Loss: 0.8780 | G Loss: 2.6716\n",
      "Epoch [1253/5000] | D Loss: 1.1962 | G Loss: 0.8687\n",
      "Epoch [1254/5000] | D Loss: 0.5806 | G Loss: 3.3625\n",
      "Epoch [1255/5000] | D Loss: 0.7542 | G Loss: 1.5412\n",
      "Epoch [1256/5000] | D Loss: 0.4785 | G Loss: 3.2229\n",
      "Epoch [1257/5000] | D Loss: 3.2220 | G Loss: 2.4983\n",
      "Epoch [1258/5000] | D Loss: 0.5198 | G Loss: 2.8613\n",
      "Epoch [1259/5000] | D Loss: 0.5368 | G Loss: 4.5387\n",
      "Epoch [1260/5000] | D Loss: 0.5541 | G Loss: 3.8115\n",
      "Epoch [1261/5000] | D Loss: 1.2802 | G Loss: 1.8630\n",
      "Epoch [1262/5000] | D Loss: 0.4905 | G Loss: 2.8452\n",
      "Epoch [1263/5000] | D Loss: 1.1009 | G Loss: 1.2648\n",
      "Epoch [1264/5000] | D Loss: 1.0553 | G Loss: 1.4714\n",
      "Epoch [1265/5000] | D Loss: 0.5770 | G Loss: 2.3263\n",
      "Epoch [1266/5000] | D Loss: 0.9274 | G Loss: 1.2175\n",
      "Epoch [1267/5000] | D Loss: 0.9905 | G Loss: 2.1073\n",
      "Epoch [1268/5000] | D Loss: 1.8606 | G Loss: 0.7022\n",
      "Epoch [1269/5000] | D Loss: 0.6069 | G Loss: 3.5258\n",
      "Epoch [1270/5000] | D Loss: 1.2698 | G Loss: 0.4049\n",
      "Epoch [1271/5000] | D Loss: 0.5512 | G Loss: 2.8194\n",
      "Epoch [1272/5000] | D Loss: 0.9538 | G Loss: 1.4244\n",
      "Epoch [1273/5000] | D Loss: 2.1727 | G Loss: 0.5833\n",
      "Epoch [1274/5000] | D Loss: 0.5118 | G Loss: 3.3071\n",
      "Epoch [1275/5000] | D Loss: 0.4786 | G Loss: 2.6989\n",
      "Epoch [1276/5000] | D Loss: 0.6298 | G Loss: 2.6712\n",
      "Epoch [1277/5000] | D Loss: 1.4859 | G Loss: 0.7518\n",
      "Epoch [1278/5000] | D Loss: 1.2042 | G Loss: 0.7400\n",
      "Epoch [1279/5000] | D Loss: 1.5429 | G Loss: 0.6282\n",
      "Epoch [1280/5000] | D Loss: 0.8246 | G Loss: 1.5478\n",
      "Epoch [1281/5000] | D Loss: 0.5710 | G Loss: 2.3745\n",
      "Epoch [1282/5000] | D Loss: 2.2120 | G Loss: 0.8255\n",
      "Epoch [1283/5000] | D Loss: 1.6553 | G Loss: 0.9804\n",
      "Epoch [1284/5000] | D Loss: 0.4806 | G Loss: 2.5110\n",
      "Epoch [1285/5000] | D Loss: 1.7322 | G Loss: 0.5275\n",
      "Epoch [1286/5000] | D Loss: 0.8012 | G Loss: 1.3103\n",
      "Epoch [1287/5000] | D Loss: 0.6019 | G Loss: 1.6398\n",
      "Epoch [1288/5000] | D Loss: 1.1490 | G Loss: 0.8969\n",
      "Epoch [1289/5000] | D Loss: 3.8203 | G Loss: 0.5426\n",
      "Epoch [1290/5000] | D Loss: 0.5201 | G Loss: 2.5875\n",
      "Epoch [1291/5000] | D Loss: 1.4190 | G Loss: 0.8051\n",
      "Epoch [1292/5000] | D Loss: 1.0994 | G Loss: 1.8038\n",
      "Epoch [1293/5000] | D Loss: 0.5043 | G Loss: 3.8737\n",
      "Epoch [1294/5000] | D Loss: 0.5062 | G Loss: 2.8079\n",
      "Epoch [1295/5000] | D Loss: 1.9555 | G Loss: 0.2297\n",
      "Epoch [1296/5000] | D Loss: 0.7380 | G Loss: 1.8223\n",
      "Epoch [1297/5000] | D Loss: 0.8114 | G Loss: 2.8762\n",
      "Epoch [1298/5000] | D Loss: 1.9568 | G Loss: 0.3786\n",
      "Epoch [1299/5000] | D Loss: 2.1266 | G Loss: 0.8317\n",
      "Epoch [1300/5000] | D Loss: 0.4949 | G Loss: 4.0440\n",
      "Epoch 1300 FID Score: 133.1961\n",
      "Epoch [1301/5000] | D Loss: 0.4982 | G Loss: 4.4428\n",
      "Epoch [1302/5000] | D Loss: 1.0454 | G Loss: 1.2055\n",
      "Epoch [1303/5000] | D Loss: 0.6985 | G Loss: 1.7663\n",
      "Epoch [1304/5000] | D Loss: 1.7604 | G Loss: 0.2786\n",
      "Epoch [1305/5000] | D Loss: 0.4948 | G Loss: 3.9294\n",
      "Epoch [1306/5000] | D Loss: 0.5725 | G Loss: 2.7170\n",
      "Epoch [1307/5000] | D Loss: 0.5266 | G Loss: 2.5218\n",
      "Epoch [1308/5000] | D Loss: 2.5500 | G Loss: 0.5362\n",
      "Epoch [1309/5000] | D Loss: 2.1363 | G Loss: 0.4307\n",
      "Epoch [1310/5000] | D Loss: 0.5545 | G Loss: 3.0523\n",
      "Epoch [1311/5000] | D Loss: 0.7144 | G Loss: 1.6363\n",
      "Epoch [1312/5000] | D Loss: 0.5227 | G Loss: 3.8052\n",
      "Epoch [1313/5000] | D Loss: 0.5244 | G Loss: 2.5614\n",
      "Epoch [1314/5000] | D Loss: 0.5230 | G Loss: 3.5850\n",
      "Epoch [1315/5000] | D Loss: 0.6643 | G Loss: 2.4178\n",
      "Epoch [1316/5000] | D Loss: 0.5719 | G Loss: 2.4725\n",
      "Epoch [1317/5000] | D Loss: 0.7053 | G Loss: 2.0366\n",
      "Epoch [1318/5000] | D Loss: 0.6345 | G Loss: 2.9678\n",
      "Epoch [1319/5000] | D Loss: 0.7110 | G Loss: 1.7337\n",
      "Epoch [1320/5000] | D Loss: 0.5133 | G Loss: 2.4329\n",
      "Epoch [1321/5000] | D Loss: 0.7865 | G Loss: 2.0926\n",
      "Epoch [1322/5000] | D Loss: 0.9031 | G Loss: 1.3244\n",
      "Epoch [1323/5000] | D Loss: 1.1888 | G Loss: 0.6290\n",
      "Epoch [1324/5000] | D Loss: 0.5751 | G Loss: 5.7507\n",
      "Epoch [1325/5000] | D Loss: 1.3111 | G Loss: 0.7469\n",
      "Epoch [1326/5000] | D Loss: 0.4999 | G Loss: 4.0479\n",
      "Epoch [1327/5000] | D Loss: 2.6809 | G Loss: 0.3736\n",
      "Epoch [1328/5000] | D Loss: 1.1612 | G Loss: 1.0061\n",
      "Epoch [1329/5000] | D Loss: 2.7070 | G Loss: 0.3573\n",
      "Epoch [1330/5000] | D Loss: 1.9683 | G Loss: 0.8556\n",
      "Epoch [1331/5000] | D Loss: 0.7715 | G Loss: 2.5677\n",
      "Epoch [1332/5000] | D Loss: 0.7308 | G Loss: 1.7932\n",
      "Epoch [1333/5000] | D Loss: 2.5592 | G Loss: 0.4706\n",
      "Epoch [1334/5000] | D Loss: 0.5692 | G Loss: 3.1724\n",
      "Epoch [1335/5000] | D Loss: 1.1377 | G Loss: 0.4422\n",
      "Epoch [1336/5000] | D Loss: 0.5311 | G Loss: 2.0014\n",
      "Epoch [1337/5000] | D Loss: 0.8990 | G Loss: 1.4099\n",
      "Epoch [1338/5000] | D Loss: 0.6750 | G Loss: 1.6153\n",
      "Epoch [1339/5000] | D Loss: 0.7455 | G Loss: 1.5840\n",
      "Epoch [1340/5000] | D Loss: 0.5984 | G Loss: 3.3792\n",
      "Epoch [1341/5000] | D Loss: 2.5547 | G Loss: 0.3149\n",
      "Epoch [1342/5000] | D Loss: 2.8742 | G Loss: 0.8388\n",
      "Epoch [1343/5000] | D Loss: 0.8456 | G Loss: 1.1801\n",
      "Epoch [1344/5000] | D Loss: 1.8382 | G Loss: 1.9055\n",
      "Epoch [1345/5000] | D Loss: 0.5594 | G Loss: 2.4864\n",
      "Epoch [1346/5000] | D Loss: 2.7327 | G Loss: 0.5015\n",
      "Epoch [1347/5000] | D Loss: 0.7519 | G Loss: 2.8176\n",
      "Epoch [1348/5000] | D Loss: 0.6939 | G Loss: 1.5878\n",
      "Epoch [1349/5000] | D Loss: 0.4810 | G Loss: 2.3002\n",
      "Epoch [1350/5000] | D Loss: 0.8941 | G Loss: 2.4849\n",
      "Epoch [1351/5000] | D Loss: 3.4565 | G Loss: 1.7655\n",
      "Epoch [1352/5000] | D Loss: 2.4762 | G Loss: 0.9717\n",
      "Epoch [1353/5000] | D Loss: 1.3761 | G Loss: 0.7092\n",
      "Epoch [1354/5000] | D Loss: 1.4935 | G Loss: 0.5758\n",
      "Epoch [1355/5000] | D Loss: 0.5581 | G Loss: 3.1269\n",
      "Epoch [1356/5000] | D Loss: 0.4875 | G Loss: 3.3052\n",
      "Epoch [1357/5000] | D Loss: 2.5641 | G Loss: 0.3122\n",
      "Epoch [1358/5000] | D Loss: 0.6744 | G Loss: 2.6198\n",
      "Epoch [1359/5000] | D Loss: 0.5237 | G Loss: 3.4041\n",
      "Epoch [1360/5000] | D Loss: 1.7349 | G Loss: 0.8108\n",
      "Epoch [1361/5000] | D Loss: 0.6922 | G Loss: 1.6591\n",
      "Epoch [1362/5000] | D Loss: 0.4830 | G Loss: 2.1802\n",
      "Epoch [1363/5000] | D Loss: 0.5611 | G Loss: 4.2299\n",
      "Epoch [1364/5000] | D Loss: 0.8769 | G Loss: 1.0751\n",
      "Epoch [1365/5000] | D Loss: 0.5425 | G Loss: 4.8278\n",
      "Epoch [1366/5000] | D Loss: 0.5075 | G Loss: 2.9771\n",
      "Epoch [1367/5000] | D Loss: 0.5843 | G Loss: 1.8114\n",
      "Epoch [1368/5000] | D Loss: 0.5013 | G Loss: 3.2875\n",
      "Epoch [1369/5000] | D Loss: 0.5262 | G Loss: 2.0162\n",
      "Epoch [1370/5000] | D Loss: 0.5545 | G Loss: 3.7853\n",
      "Epoch [1371/5000] | D Loss: 0.4982 | G Loss: 2.1683\n",
      "Epoch [1372/5000] | D Loss: 1.2845 | G Loss: 1.4993\n",
      "Epoch [1373/5000] | D Loss: 2.7063 | G Loss: 1.2664\n",
      "Epoch [1374/5000] | D Loss: 0.8357 | G Loss: 1.5248\n",
      "Epoch [1375/5000] | D Loss: 0.7009 | G Loss: 2.9251\n",
      "Epoch [1376/5000] | D Loss: 3.3630 | G Loss: 2.8834\n",
      "Epoch [1377/5000] | D Loss: 0.9666 | G Loss: 2.0364\n",
      "Epoch [1378/5000] | D Loss: 0.9159 | G Loss: 2.9255\n",
      "Epoch [1379/5000] | D Loss: 1.3070 | G Loss: 1.4334\n",
      "Epoch [1380/5000] | D Loss: 0.4983 | G Loss: 2.9618\n",
      "Epoch [1381/5000] | D Loss: 0.5755 | G Loss: 2.2831\n",
      "Epoch [1382/5000] | D Loss: 0.5922 | G Loss: 4.1851\n",
      "Epoch [1383/5000] | D Loss: 0.6768 | G Loss: 2.0728\n",
      "Epoch [1384/5000] | D Loss: 1.1058 | G Loss: 1.3623\n",
      "Epoch [1385/5000] | D Loss: 0.8713 | G Loss: 0.6857\n",
      "Epoch [1386/5000] | D Loss: 0.4643 | G Loss: 3.3662\n",
      "Epoch [1387/5000] | D Loss: 0.5093 | G Loss: 2.3346\n",
      "Epoch [1388/5000] | D Loss: 1.4660 | G Loss: 1.2429\n",
      "Epoch [1389/5000] | D Loss: 0.5551 | G Loss: 2.9627\n",
      "Epoch [1390/5000] | D Loss: 2.0179 | G Loss: 1.1296\n",
      "Epoch [1391/5000] | D Loss: 1.0549 | G Loss: 3.0771\n",
      "Epoch [1392/5000] | D Loss: 0.5208 | G Loss: 2.3999\n",
      "Epoch [1393/5000] | D Loss: 2.2435 | G Loss: 1.9225\n",
      "Epoch [1394/5000] | D Loss: 0.9035 | G Loss: 1.4279\n",
      "Epoch [1395/5000] | D Loss: 1.2923 | G Loss: 1.4448\n",
      "Epoch [1396/5000] | D Loss: 0.6183 | G Loss: 3.3398\n",
      "Epoch [1397/5000] | D Loss: 2.6233 | G Loss: 1.4052\n",
      "Epoch [1398/5000] | D Loss: 0.5074 | G Loss: 3.2829\n",
      "Epoch [1399/5000] | D Loss: 0.9549 | G Loss: 1.3951\n",
      "Epoch [1400/5000] | D Loss: 1.2653 | G Loss: 1.3015\n",
      "Epoch 1400 FID Score: 131.2010\n",
      "Epoch [1401/5000] | D Loss: 0.5411 | G Loss: 3.9563\n",
      "Epoch [1402/5000] | D Loss: 0.6251 | G Loss: 2.0436\n",
      "Epoch [1403/5000] | D Loss: 1.1048 | G Loss: 0.8582\n",
      "Epoch [1404/5000] | D Loss: 0.5690 | G Loss: 2.6212\n",
      "Epoch [1405/5000] | D Loss: 0.4647 | G Loss: 4.0297\n",
      "Epoch [1406/5000] | D Loss: 0.8244 | G Loss: 2.8787\n",
      "Epoch [1407/5000] | D Loss: 0.5730 | G Loss: 2.1707\n",
      "Epoch [1408/5000] | D Loss: 1.4875 | G Loss: 1.2472\n",
      "Epoch [1409/5000] | D Loss: 2.8087 | G Loss: 1.0381\n",
      "Epoch [1410/5000] | D Loss: 1.3973 | G Loss: 0.7450\n",
      "Epoch [1411/5000] | D Loss: 0.6020 | G Loss: 1.6020\n",
      "Epoch [1412/5000] | D Loss: 1.0251 | G Loss: 2.0868\n",
      "Epoch [1413/5000] | D Loss: 0.6701 | G Loss: 2.0017\n",
      "Epoch [1414/5000] | D Loss: 0.5839 | G Loss: 2.9096\n",
      "Epoch [1415/5000] | D Loss: 0.5490 | G Loss: 3.0420\n",
      "Epoch [1416/5000] | D Loss: 0.7164 | G Loss: 1.9522\n",
      "Epoch [1417/5000] | D Loss: 0.7534 | G Loss: 1.7506\n",
      "Epoch [1418/5000] | D Loss: 1.8245 | G Loss: 0.8733\n",
      "Epoch [1419/5000] | D Loss: 0.5045 | G Loss: 1.5494\n",
      "Epoch [1420/5000] | D Loss: 0.5724 | G Loss: 2.0540\n",
      "Epoch [1421/5000] | D Loss: 2.3366 | G Loss: 1.3243\n",
      "Epoch [1422/5000] | D Loss: 2.6118 | G Loss: 1.1200\n",
      "Epoch [1423/5000] | D Loss: 3.0771 | G Loss: 0.4068\n",
      "Epoch [1424/5000] | D Loss: 0.8574 | G Loss: 2.1526\n",
      "Epoch [1425/5000] | D Loss: 0.4952 | G Loss: 2.2043\n",
      "Epoch [1426/5000] | D Loss: 2.8982 | G Loss: 1.2919\n",
      "Epoch [1427/5000] | D Loss: 0.9886 | G Loss: 2.3188\n",
      "Epoch [1428/5000] | D Loss: 0.5138 | G Loss: 4.4092\n",
      "Epoch [1429/5000] | D Loss: 0.4635 | G Loss: 3.1647\n",
      "Epoch [1430/5000] | D Loss: 2.9959 | G Loss: 0.3779\n",
      "Epoch [1431/5000] | D Loss: 0.4885 | G Loss: 3.6134\n",
      "Epoch [1432/5000] | D Loss: 1.1972 | G Loss: 1.6007\n",
      "Epoch [1433/5000] | D Loss: 1.6718 | G Loss: 0.3268\n",
      "Epoch [1434/5000] | D Loss: 0.6384 | G Loss: 1.7655\n",
      "Epoch [1435/5000] | D Loss: 0.5459 | G Loss: 3.2838\n",
      "Epoch [1436/5000] | D Loss: 0.6036 | G Loss: 2.8151\n",
      "Epoch [1437/5000] | D Loss: 1.2787 | G Loss: 1.2267\n",
      "Epoch [1438/5000] | D Loss: 0.9075 | G Loss: 1.7353\n",
      "Epoch [1439/5000] | D Loss: 0.8318 | G Loss: 2.8899\n",
      "Epoch [1440/5000] | D Loss: 0.6237 | G Loss: 2.8205\n",
      "Epoch [1441/5000] | D Loss: 1.0931 | G Loss: 2.1874\n",
      "Epoch [1442/5000] | D Loss: 0.6115 | G Loss: 1.4207\n",
      "Epoch [1443/5000] | D Loss: 0.9560 | G Loss: 3.1045\n",
      "Epoch [1444/5000] | D Loss: 3.3060 | G Loss: 1.2789\n",
      "Epoch [1445/5000] | D Loss: 2.0057 | G Loss: 0.4283\n",
      "Epoch [1446/5000] | D Loss: 0.4733 | G Loss: 2.9141\n",
      "Epoch [1447/5000] | D Loss: 0.6499 | G Loss: 1.9478\n",
      "Epoch [1448/5000] | D Loss: 1.6817 | G Loss: 0.3835\n",
      "Epoch [1449/5000] | D Loss: 0.5624 | G Loss: 3.7436\n",
      "Epoch [1450/5000] | D Loss: 0.9520 | G Loss: 1.1708\n",
      "Epoch [1451/5000] | D Loss: 1.6400 | G Loss: 0.3737\n",
      "Epoch [1452/5000] | D Loss: 2.1327 | G Loss: 0.2411\n",
      "Epoch [1453/5000] | D Loss: 0.7519 | G Loss: 1.8788\n",
      "Epoch [1454/5000] | D Loss: 1.3591 | G Loss: 0.4564\n",
      "Epoch [1455/5000] | D Loss: 0.7437 | G Loss: 2.3084\n",
      "Epoch [1456/5000] | D Loss: 0.5609 | G Loss: 3.4025\n",
      "Epoch [1457/5000] | D Loss: 0.5351 | G Loss: 2.0427\n",
      "Epoch [1458/5000] | D Loss: 0.6924 | G Loss: 1.2889\n",
      "Epoch [1459/5000] | D Loss: 0.7932 | G Loss: 2.4882\n",
      "Epoch [1460/5000] | D Loss: 0.5638 | G Loss: 3.9021\n",
      "Epoch [1461/5000] | D Loss: 0.5406 | G Loss: 2.1504\n",
      "Epoch [1462/5000] | D Loss: 0.6888 | G Loss: 1.5492\n",
      "Epoch [1463/5000] | D Loss: 0.7132 | G Loss: 1.5557\n",
      "Epoch [1464/5000] | D Loss: 0.4960 | G Loss: 2.7183\n",
      "Epoch [1465/5000] | D Loss: 1.9075 | G Loss: 0.7499\n",
      "Epoch [1466/5000] | D Loss: 0.5916 | G Loss: 1.8497\n",
      "Epoch [1467/5000] | D Loss: 0.8237 | G Loss: 2.0860\n",
      "Epoch [1468/5000] | D Loss: 0.4753 | G Loss: 3.6958\n",
      "Epoch [1469/5000] | D Loss: 1.2013 | G Loss: 1.8047\n",
      "Epoch [1470/5000] | D Loss: 1.3275 | G Loss: 1.5515\n",
      "Epoch [1471/5000] | D Loss: 0.5103 | G Loss: 2.9783\n",
      "Epoch [1472/5000] | D Loss: 2.5322 | G Loss: 0.6060\n",
      "Epoch [1473/5000] | D Loss: 0.7771 | G Loss: 1.8176\n",
      "Epoch [1474/5000] | D Loss: 3.3721 | G Loss: 1.8341\n",
      "Epoch [1475/5000] | D Loss: 1.0865 | G Loss: 1.3034\n",
      "Epoch [1476/5000] | D Loss: 0.5271 | G Loss: 2.0267\n",
      "Epoch [1477/5000] | D Loss: 1.0048 | G Loss: 0.8744\n",
      "Epoch [1478/5000] | D Loss: 2.2199 | G Loss: 0.3006\n",
      "Epoch [1479/5000] | D Loss: 3.1278 | G Loss: 2.8018\n",
      "Epoch [1480/5000] | D Loss: 0.5919 | G Loss: 1.3649\n",
      "Epoch [1481/5000] | D Loss: 3.1046 | G Loss: 0.8118\n",
      "Epoch [1482/5000] | D Loss: 2.7200 | G Loss: 0.2851\n",
      "Epoch [1483/5000] | D Loss: 0.9408 | G Loss: 2.4755\n",
      "Epoch [1484/5000] | D Loss: 0.5971 | G Loss: 3.8292\n",
      "Epoch [1485/5000] | D Loss: 0.5510 | G Loss: 2.6551\n",
      "Epoch [1486/5000] | D Loss: 0.5503 | G Loss: 3.5106\n",
      "Epoch [1487/5000] | D Loss: 0.5544 | G Loss: 3.2947\n",
      "Epoch [1488/5000] | D Loss: 1.0686 | G Loss: 1.7407\n",
      "Epoch [1489/5000] | D Loss: 1.4624 | G Loss: 1.9110\n",
      "Epoch [1490/5000] | D Loss: 0.9135 | G Loss: 2.3432\n",
      "Epoch [1491/5000] | D Loss: 0.7091 | G Loss: 2.0503\n",
      "Epoch [1492/5000] | D Loss: 1.9898 | G Loss: 0.6051\n",
      "Epoch [1493/5000] | D Loss: 0.9267 | G Loss: 3.5012\n",
      "Epoch [1494/5000] | D Loss: 0.6177 | G Loss: 2.5398\n",
      "Epoch [1495/5000] | D Loss: 1.8286 | G Loss: 0.4442\n",
      "Epoch [1496/5000] | D Loss: 0.7927 | G Loss: 1.9512\n",
      "Epoch [1497/5000] | D Loss: 1.7658 | G Loss: 0.7773\n",
      "Epoch [1498/5000] | D Loss: 0.7966 | G Loss: 2.5689\n",
      "Epoch [1499/5000] | D Loss: 0.6278 | G Loss: 3.0325\n",
      "Epoch [1500/5000] | D Loss: 0.5802 | G Loss: 2.7421\n",
      "Epoch 1500 FID Score: 133.2803\n",
      "Epoch [1501/5000] | D Loss: 2.2894 | G Loss: 0.6110\n",
      "Epoch [1502/5000] | D Loss: 1.1221 | G Loss: 1.5107\n",
      "Epoch [1503/5000] | D Loss: 1.1445 | G Loss: 1.6237\n",
      "Epoch [1504/5000] | D Loss: 0.5972 | G Loss: 4.5087\n",
      "Epoch [1505/5000] | D Loss: 1.8617 | G Loss: 0.7008\n",
      "Epoch [1506/5000] | D Loss: 0.8026 | G Loss: 1.5036\n",
      "Epoch [1507/5000] | D Loss: 1.9955 | G Loss: 0.4527\n",
      "Epoch [1508/5000] | D Loss: 0.9397 | G Loss: 2.0513\n",
      "Epoch [1509/5000] | D Loss: 1.3216 | G Loss: 1.2643\n",
      "Epoch [1510/5000] | D Loss: 1.2569 | G Loss: 0.9426\n",
      "Epoch [1511/5000] | D Loss: 1.1750 | G Loss: 1.0826\n",
      "Epoch [1512/5000] | D Loss: 2.0948 | G Loss: 0.4685\n",
      "Epoch [1513/5000] | D Loss: 1.7353 | G Loss: 0.4772\n",
      "Epoch [1514/5000] | D Loss: 1.1451 | G Loss: 1.0143\n",
      "Epoch [1515/5000] | D Loss: 0.5445 | G Loss: 2.8913\n",
      "Epoch [1516/5000] | D Loss: 1.0966 | G Loss: 0.8007\n",
      "Epoch [1517/5000] | D Loss: 1.0308 | G Loss: 0.8633\n",
      "Epoch [1518/5000] | D Loss: 0.7592 | G Loss: 1.8845\n",
      "Epoch [1519/5000] | D Loss: 1.0800 | G Loss: 1.6700\n",
      "Epoch [1520/5000] | D Loss: 2.2100 | G Loss: 0.7585\n",
      "Epoch [1521/5000] | D Loss: 0.5166 | G Loss: 3.2535\n",
      "Epoch [1522/5000] | D Loss: 1.0955 | G Loss: 1.5118\n",
      "Epoch [1523/5000] | D Loss: 0.7132 | G Loss: 2.5882\n",
      "Epoch [1524/5000] | D Loss: 1.3264 | G Loss: 0.9940\n",
      "Epoch [1525/5000] | D Loss: 1.1697 | G Loss: 1.0563\n",
      "Epoch [1526/5000] | D Loss: 1.4663 | G Loss: 2.5761\n",
      "Epoch [1527/5000] | D Loss: 2.9517 | G Loss: 0.8313\n",
      "Epoch [1528/5000] | D Loss: 0.9789 | G Loss: 2.0839\n",
      "Epoch [1529/5000] | D Loss: 0.7732 | G Loss: 3.1939\n",
      "Epoch [1530/5000] | D Loss: 0.9405 | G Loss: 1.6467\n",
      "Epoch [1531/5000] | D Loss: 1.3833 | G Loss: 2.3224\n",
      "Epoch [1532/5000] | D Loss: 0.6251 | G Loss: 1.9269\n",
      "Epoch [1533/5000] | D Loss: 0.5333 | G Loss: 2.3171\n",
      "Epoch [1534/5000] | D Loss: 2.8373 | G Loss: 0.2676\n",
      "Epoch [1535/5000] | D Loss: 0.4743 | G Loss: 3.5581\n",
      "Epoch [1536/5000] | D Loss: 2.6181 | G Loss: 0.8484\n",
      "Epoch [1537/5000] | D Loss: 0.5552 | G Loss: 2.1649\n",
      "Epoch [1538/5000] | D Loss: 0.9206 | G Loss: 1.6603\n",
      "Epoch [1539/5000] | D Loss: 2.3920 | G Loss: 1.4770\n",
      "Epoch [1540/5000] | D Loss: 1.7059 | G Loss: 0.4974\n",
      "Epoch [1541/5000] | D Loss: 0.5650 | G Loss: 2.7626\n",
      "Epoch [1542/5000] | D Loss: 0.7210 | G Loss: 1.7990\n",
      "Epoch [1543/5000] | D Loss: 2.3152 | G Loss: 0.2642\n",
      "Epoch [1544/5000] | D Loss: 0.5489 | G Loss: 2.9919\n",
      "Epoch [1545/5000] | D Loss: 0.6250 | G Loss: 3.1889\n",
      "Epoch [1546/5000] | D Loss: 0.5756 | G Loss: 4.0757\n",
      "Epoch [1547/5000] | D Loss: 0.8355 | G Loss: 2.5187\n",
      "Epoch [1548/5000] | D Loss: 1.7213 | G Loss: 0.3693\n",
      "Epoch [1549/5000] | D Loss: 1.8744 | G Loss: 0.7332\n",
      "Epoch [1550/5000] | D Loss: 0.4892 | G Loss: 3.4305\n",
      "Epoch [1551/5000] | D Loss: 1.5171 | G Loss: 0.7587\n",
      "Epoch [1552/5000] | D Loss: 0.5890 | G Loss: 2.2799\n",
      "Epoch [1553/5000] | D Loss: 0.8505 | G Loss: 1.2896\n",
      "Epoch [1554/5000] | D Loss: 0.5534 | G Loss: 1.3641\n",
      "Epoch [1555/5000] | D Loss: 1.2452 | G Loss: 1.3698\n",
      "Epoch [1556/5000] | D Loss: 1.0592 | G Loss: 1.1842\n",
      "Epoch [1557/5000] | D Loss: 1.0177 | G Loss: 2.2583\n",
      "Epoch [1558/5000] | D Loss: 1.1896 | G Loss: 0.6603\n",
      "Epoch [1559/5000] | D Loss: 0.9121 | G Loss: 0.8876\n",
      "Epoch [1560/5000] | D Loss: 1.6852 | G Loss: 1.0610\n",
      "Epoch [1561/5000] | D Loss: 1.8850 | G Loss: 1.8572\n",
      "Epoch [1562/5000] | D Loss: 1.1414 | G Loss: 2.6437\n",
      "Epoch [1563/5000] | D Loss: 2.0094 | G Loss: 0.2713\n",
      "Epoch [1564/5000] | D Loss: 1.3673 | G Loss: 0.5901\n",
      "Epoch [1565/5000] | D Loss: 1.7024 | G Loss: 0.5593\n",
      "Epoch [1566/5000] | D Loss: 0.4881 | G Loss: 2.5910\n",
      "Epoch [1567/5000] | D Loss: 2.0125 | G Loss: 2.1260\n",
      "Epoch [1568/5000] | D Loss: 1.3920 | G Loss: 0.8909\n",
      "Epoch [1569/5000] | D Loss: 0.8909 | G Loss: 2.0456\n",
      "Epoch [1570/5000] | D Loss: 0.6642 | G Loss: 1.9703\n",
      "Epoch [1571/5000] | D Loss: 2.7107 | G Loss: 1.5156\n",
      "Epoch [1572/5000] | D Loss: 0.6118 | G Loss: 5.0761\n",
      "Epoch [1573/5000] | D Loss: 1.1537 | G Loss: 1.4915\n",
      "Epoch [1574/5000] | D Loss: 1.4403 | G Loss: 0.7186\n",
      "Epoch [1575/5000] | D Loss: 0.6733 | G Loss: 2.7111\n",
      "Epoch [1576/5000] | D Loss: 0.4742 | G Loss: 3.2428\n",
      "Epoch [1577/5000] | D Loss: 2.9885 | G Loss: 0.3058\n",
      "Epoch [1578/5000] | D Loss: 0.6082 | G Loss: 2.7242\n",
      "Epoch [1579/5000] | D Loss: 3.0756 | G Loss: 1.0190\n",
      "Epoch [1580/5000] | D Loss: 0.6918 | G Loss: 1.8278\n",
      "Epoch [1581/5000] | D Loss: 0.5438 | G Loss: 2.3000\n",
      "Epoch [1582/5000] | D Loss: 0.5658 | G Loss: 2.3947\n",
      "Epoch [1583/5000] | D Loss: 3.1836 | G Loss: 0.3731\n",
      "Epoch [1584/5000] | D Loss: 1.1135 | G Loss: 0.6934\n",
      "Epoch [1585/5000] | D Loss: 1.7828 | G Loss: 1.7468\n",
      "Epoch [1586/5000] | D Loss: 2.6650 | G Loss: 0.4626\n",
      "Epoch [1587/5000] | D Loss: 2.4037 | G Loss: 0.2716\n",
      "Epoch [1588/5000] | D Loss: 0.5064 | G Loss: 2.5779\n",
      "Epoch [1589/5000] | D Loss: 0.8273 | G Loss: 1.6824\n",
      "Epoch [1590/5000] | D Loss: 0.8615 | G Loss: 0.8755\n",
      "Epoch [1591/5000] | D Loss: 2.6108 | G Loss: 1.0528\n",
      "Epoch [1592/5000] | D Loss: 0.5664 | G Loss: 3.2294\n",
      "Epoch [1593/5000] | D Loss: 0.4898 | G Loss: 3.2420\n",
      "Epoch [1594/5000] | D Loss: 3.0143 | G Loss: 0.5242\n",
      "Epoch [1595/5000] | D Loss: 0.6933 | G Loss: 2.3893\n",
      "Epoch [1596/5000] | D Loss: 0.5701 | G Loss: 2.4956\n",
      "Epoch [1597/5000] | D Loss: 0.5343 | G Loss: 3.6112\n",
      "Epoch [1598/5000] | D Loss: 2.1704 | G Loss: 0.5815\n",
      "Epoch [1599/5000] | D Loss: 1.6688 | G Loss: 0.9775\n",
      "Epoch [1600/5000] | D Loss: 3.1134 | G Loss: 1.0702\n",
      "Epoch 1600 FID Score: 132.5914\n",
      "Epoch [1601/5000] | D Loss: 0.7776 | G Loss: 1.4074\n",
      "Epoch [1602/5000] | D Loss: 1.5651 | G Loss: 0.5139\n",
      "Epoch [1603/5000] | D Loss: 2.5751 | G Loss: 0.6035\n",
      "Epoch [1604/5000] | D Loss: 0.7188 | G Loss: 2.5189\n",
      "Epoch [1605/5000] | D Loss: 0.9599 | G Loss: 0.6288\n",
      "Epoch [1606/5000] | D Loss: 0.4873 | G Loss: 2.5441\n",
      "Epoch [1607/5000] | D Loss: 0.5246 | G Loss: 2.3160\n",
      "Epoch [1608/5000] | D Loss: 1.0606 | G Loss: 1.3737\n",
      "Epoch [1609/5000] | D Loss: 0.8518 | G Loss: 2.0617\n",
      "Epoch [1610/5000] | D Loss: 0.5441 | G Loss: 3.2530\n",
      "Epoch [1611/5000] | D Loss: 0.6929 | G Loss: 1.8757\n",
      "Epoch [1612/5000] | D Loss: 1.6937 | G Loss: 0.4914\n",
      "Epoch [1613/5000] | D Loss: 1.0786 | G Loss: 1.0199\n",
      "Epoch [1614/5000] | D Loss: 0.4928 | G Loss: 3.7883\n",
      "Epoch [1615/5000] | D Loss: 1.1741 | G Loss: 1.1708\n",
      "Epoch [1616/5000] | D Loss: 0.7407 | G Loss: 2.2543\n",
      "Epoch [1617/5000] | D Loss: 3.1561 | G Loss: 3.7126\n",
      "Epoch [1618/5000] | D Loss: 0.6551 | G Loss: 2.3671\n",
      "Epoch [1619/5000] | D Loss: 0.7962 | G Loss: 1.4194\n",
      "Epoch [1620/5000] | D Loss: 0.5363 | G Loss: 3.6388\n",
      "Epoch [1621/5000] | D Loss: 1.6258 | G Loss: 1.1591\n",
      "Epoch [1622/5000] | D Loss: 0.7136 | G Loss: 1.8462\n",
      "Epoch [1623/5000] | D Loss: 0.5293 | G Loss: 1.8616\n",
      "Epoch [1624/5000] | D Loss: 2.2441 | G Loss: 0.9599\n",
      "Epoch [1625/5000] | D Loss: 1.4272 | G Loss: 2.7134\n",
      "Epoch [1626/5000] | D Loss: 2.6464 | G Loss: 0.5526\n",
      "Epoch [1627/5000] | D Loss: 1.9705 | G Loss: 0.7820\n",
      "Epoch [1628/5000] | D Loss: 0.4889 | G Loss: 2.8015\n",
      "Epoch [1629/5000] | D Loss: 1.7188 | G Loss: 0.5372\n",
      "Epoch [1630/5000] | D Loss: 3.0666 | G Loss: 0.8412\n",
      "Epoch [1631/5000] | D Loss: 1.7041 | G Loss: 0.5888\n",
      "Epoch [1632/5000] | D Loss: 1.1114 | G Loss: 0.7917\n",
      "Epoch [1633/5000] | D Loss: 0.5419 | G Loss: 2.5561\n",
      "Epoch [1634/5000] | D Loss: 2.1593 | G Loss: 0.2225\n",
      "Epoch [1635/5000] | D Loss: 0.5133 | G Loss: 3.1977\n",
      "Epoch [1636/5000] | D Loss: 1.3327 | G Loss: 0.5948\n",
      "Epoch [1637/5000] | D Loss: 1.3115 | G Loss: 1.4653\n",
      "Epoch [1638/5000] | D Loss: 2.6559 | G Loss: 0.3320\n",
      "Epoch [1639/5000] | D Loss: 2.4338 | G Loss: 0.5711\n",
      "Epoch [1640/5000] | D Loss: 1.7355 | G Loss: 0.4543\n",
      "Epoch [1641/5000] | D Loss: 2.0509 | G Loss: 0.8240\n",
      "Epoch [1642/5000] | D Loss: 0.9687 | G Loss: 1.7898\n",
      "Epoch [1643/5000] | D Loss: 2.3750 | G Loss: 0.3260\n",
      "Epoch [1644/5000] | D Loss: 1.3322 | G Loss: 0.9181\n",
      "Epoch [1645/5000] | D Loss: 1.1130 | G Loss: 0.9781\n",
      "Epoch [1646/5000] | D Loss: 2.5549 | G Loss: 0.3876\n",
      "Epoch [1647/5000] | D Loss: 0.4881 | G Loss: 3.7131\n",
      "Epoch [1648/5000] | D Loss: 2.4845 | G Loss: 0.4554\n",
      "Epoch [1649/5000] | D Loss: 0.5563 | G Loss: 3.4289\n",
      "Epoch [1650/5000] | D Loss: 1.0981 | G Loss: 1.2316\n",
      "Epoch [1651/5000] | D Loss: 0.9426 | G Loss: 0.9540\n",
      "Epoch [1652/5000] | D Loss: 0.6969 | G Loss: 1.6664\n",
      "Epoch [1653/5000] | D Loss: 1.5422 | G Loss: 1.1440\n",
      "Epoch [1654/5000] | D Loss: 2.5414 | G Loss: 0.3334\n",
      "Epoch [1655/5000] | D Loss: 2.1963 | G Loss: 0.2647\n",
      "Epoch [1656/5000] | D Loss: 1.2121 | G Loss: 2.5539\n",
      "Epoch [1657/5000] | D Loss: 1.6876 | G Loss: 0.8488\n",
      "Epoch [1658/5000] | D Loss: 0.6028 | G Loss: 2.2820\n",
      "Epoch [1659/5000] | D Loss: 1.0557 | G Loss: 1.6741\n",
      "Epoch [1660/5000] | D Loss: 0.7195 | G Loss: 2.1394\n",
      "Epoch [1661/5000] | D Loss: 0.6155 | G Loss: 2.9525\n",
      "Epoch [1662/5000] | D Loss: 3.1498 | G Loss: 0.4286\n",
      "Epoch [1663/5000] | D Loss: 2.0926 | G Loss: 1.2395\n",
      "Epoch [1664/5000] | D Loss: 0.5117 | G Loss: 3.2443\n",
      "Epoch [1665/5000] | D Loss: 1.5189 | G Loss: 2.1797\n",
      "Epoch [1666/5000] | D Loss: 0.5326 | G Loss: 3.2636\n",
      "Epoch [1667/5000] | D Loss: 0.8760 | G Loss: 1.2246\n",
      "Epoch [1668/5000] | D Loss: 1.8400 | G Loss: 2.3364\n",
      "Epoch [1669/5000] | D Loss: 0.5531 | G Loss: 2.6932\n",
      "Epoch [1670/5000] | D Loss: 1.7429 | G Loss: 0.5628\n",
      "Epoch [1671/5000] | D Loss: 1.4751 | G Loss: 0.5707\n",
      "Epoch [1672/5000] | D Loss: 1.8267 | G Loss: 0.4541\n",
      "Epoch [1673/5000] | D Loss: 1.5469 | G Loss: 1.1492\n",
      "Epoch [1674/5000] | D Loss: 3.7373 | G Loss: 0.8531\n",
      "Epoch [1675/5000] | D Loss: 1.0737 | G Loss: 1.3953\n",
      "Epoch [1676/5000] | D Loss: 0.7145 | G Loss: 1.2935\n",
      "Epoch [1677/5000] | D Loss: 1.7008 | G Loss: 0.7222\n",
      "Epoch [1678/5000] | D Loss: 1.6220 | G Loss: 0.7292\n",
      "Epoch [1679/5000] | D Loss: 3.2095 | G Loss: 0.3958\n",
      "Epoch [1680/5000] | D Loss: 1.0807 | G Loss: 1.2476\n",
      "Epoch [1681/5000] | D Loss: 0.8247 | G Loss: 1.8332\n",
      "Epoch [1682/5000] | D Loss: 2.5278 | G Loss: 0.6450\n",
      "Epoch [1683/5000] | D Loss: 0.5321 | G Loss: 2.9996\n",
      "Epoch [1684/5000] | D Loss: 0.8262 | G Loss: 2.1191\n",
      "Epoch [1685/5000] | D Loss: 0.5251 | G Loss: 3.0319\n",
      "Epoch [1686/5000] | D Loss: 2.6779 | G Loss: 1.2162\n",
      "Epoch [1687/5000] | D Loss: 1.8819 | G Loss: 0.9489\n",
      "Epoch [1688/5000] | D Loss: 1.3153 | G Loss: 0.9251\n",
      "Epoch [1689/5000] | D Loss: 0.7613 | G Loss: 2.0836\n",
      "Epoch [1690/5000] | D Loss: 0.6886 | G Loss: 1.4274\n",
      "Epoch [1691/5000] | D Loss: 0.5974 | G Loss: 2.3254\n",
      "Epoch [1692/5000] | D Loss: 2.4077 | G Loss: 0.2157\n",
      "Epoch [1693/5000] | D Loss: 3.1832 | G Loss: 1.7617\n",
      "Epoch [1694/5000] | D Loss: 2.5111 | G Loss: 0.5718\n",
      "Epoch [1695/5000] | D Loss: 0.4959 | G Loss: 3.5021\n",
      "Epoch [1696/5000] | D Loss: 3.6529 | G Loss: 0.5474\n",
      "Epoch [1697/5000] | D Loss: 0.5950 | G Loss: 3.0335\n",
      "Epoch [1698/5000] | D Loss: 1.4573 | G Loss: 0.8050\n",
      "Epoch [1699/5000] | D Loss: 0.8174 | G Loss: 1.0822\n",
      "Epoch [1700/5000] | D Loss: 2.3180 | G Loss: 0.8826\n",
      "Epoch 1700 FID Score: 126.6677\n",
      "Epoch [1701/5000] | D Loss: 1.4335 | G Loss: 1.0247\n",
      "Epoch [1702/5000] | D Loss: 2.4411 | G Loss: 1.4469\n",
      "Epoch [1703/5000] | D Loss: 2.4288 | G Loss: 0.7214\n",
      "Epoch [1704/5000] | D Loss: 1.3833 | G Loss: 1.4595\n",
      "Epoch [1705/5000] | D Loss: 1.0758 | G Loss: 0.9808\n",
      "Epoch [1706/5000] | D Loss: 2.5819 | G Loss: 0.4120\n",
      "Epoch [1707/5000] | D Loss: 1.0954 | G Loss: 1.1891\n",
      "Epoch [1708/5000] | D Loss: 0.5345 | G Loss: 2.7415\n",
      "Epoch [1709/5000] | D Loss: 1.8155 | G Loss: 0.6815\n",
      "Epoch [1710/5000] | D Loss: 0.6242 | G Loss: 3.8405\n",
      "Epoch [1711/5000] | D Loss: 2.4544 | G Loss: 0.5102\n",
      "Epoch [1712/5000] | D Loss: 1.2941 | G Loss: 2.1111\n",
      "Epoch [1713/5000] | D Loss: 1.8483 | G Loss: 1.3189\n",
      "Epoch [1714/5000] | D Loss: 0.6926 | G Loss: 2.9925\n",
      "Epoch [1715/5000] | D Loss: 0.5158 | G Loss: 3.1914\n",
      "Epoch [1716/5000] | D Loss: 1.5740 | G Loss: 1.0841\n",
      "Epoch [1717/5000] | D Loss: 1.7815 | G Loss: 0.3873\n",
      "Epoch [1718/5000] | D Loss: 1.2124 | G Loss: 1.3108\n",
      "Epoch [1719/5000] | D Loss: 2.7643 | G Loss: 0.5861\n",
      "Epoch [1720/5000] | D Loss: 1.2307 | G Loss: 0.8953\n",
      "Epoch [1721/5000] | D Loss: 0.5537 | G Loss: 2.7213\n",
      "Epoch [1722/5000] | D Loss: 1.0476 | G Loss: 1.9383\n",
      "Epoch [1723/5000] | D Loss: 0.8675 | G Loss: 2.4038\n",
      "Epoch [1724/5000] | D Loss: 2.7841 | G Loss: 0.4816\n",
      "Epoch [1725/5000] | D Loss: 2.2419 | G Loss: 0.6861\n",
      "Epoch [1726/5000] | D Loss: 1.9621 | G Loss: 0.3710\n",
      "Epoch [1727/5000] | D Loss: 2.2299 | G Loss: 0.3771\n",
      "Epoch [1728/5000] | D Loss: 1.6124 | G Loss: 0.8173\n",
      "Epoch [1729/5000] | D Loss: 1.0126 | G Loss: 0.9972\n",
      "Epoch [1730/5000] | D Loss: 1.8340 | G Loss: 0.2454\n",
      "Epoch [1731/5000] | D Loss: 1.5670 | G Loss: 0.5225\n",
      "Epoch [1732/5000] | D Loss: 1.2525 | G Loss: 0.9946\n",
      "Epoch [1733/5000] | D Loss: 0.4957 | G Loss: 2.0498\n",
      "Epoch [1734/5000] | D Loss: 1.3097 | G Loss: 1.0175\n",
      "Epoch [1735/5000] | D Loss: 0.6444 | G Loss: 2.6646\n",
      "Epoch [1736/5000] | D Loss: 0.6839 | G Loss: 3.4473\n",
      "Epoch [1737/5000] | D Loss: 2.0377 | G Loss: 0.7602\n",
      "Epoch [1738/5000] | D Loss: 0.5829 | G Loss: 2.7636\n",
      "Epoch [1739/5000] | D Loss: 1.4603 | G Loss: 0.8629\n",
      "Epoch [1740/5000] | D Loss: 0.8651 | G Loss: 2.3686\n",
      "Epoch [1741/5000] | D Loss: 0.8872 | G Loss: 1.7916\n",
      "Epoch [1742/5000] | D Loss: 2.0521 | G Loss: 0.8371\n",
      "Epoch [1743/5000] | D Loss: 2.9843 | G Loss: 1.6561\n",
      "Epoch [1744/5000] | D Loss: 1.7186 | G Loss: 0.6760\n",
      "Epoch [1745/5000] | D Loss: 0.5522 | G Loss: 2.8004\n",
      "Epoch [1746/5000] | D Loss: 2.7972 | G Loss: 0.7302\n",
      "Epoch [1747/5000] | D Loss: 1.4349 | G Loss: 0.6817\n",
      "Epoch [1748/5000] | D Loss: 0.9134 | G Loss: 1.5809\n",
      "Epoch [1749/5000] | D Loss: 1.2653 | G Loss: 1.7115\n",
      "Epoch [1750/5000] | D Loss: 1.3614 | G Loss: 0.7614\n",
      "Epoch [1751/5000] | D Loss: 0.6331 | G Loss: 2.3204\n",
      "Epoch [1752/5000] | D Loss: 2.3121 | G Loss: 0.7462\n",
      "Epoch [1753/5000] | D Loss: 1.8595 | G Loss: 0.4121\n",
      "Epoch [1754/5000] | D Loss: 0.4995 | G Loss: 1.9290\n",
      "Epoch [1755/5000] | D Loss: 0.7947 | G Loss: 2.5397\n",
      "Epoch [1756/5000] | D Loss: 0.7194 | G Loss: 2.9932\n",
      "Epoch [1757/5000] | D Loss: 1.7851 | G Loss: 0.7368\n",
      "Epoch [1758/5000] | D Loss: 0.9156 | G Loss: 1.5238\n",
      "Epoch [1759/5000] | D Loss: 1.0904 | G Loss: 1.8930\n",
      "Epoch [1760/5000] | D Loss: 1.1343 | G Loss: 0.5607\n",
      "Epoch [1761/5000] | D Loss: 1.0172 | G Loss: 1.6790\n",
      "Epoch [1762/5000] | D Loss: 0.7059 | G Loss: 2.0331\n",
      "Epoch [1763/5000] | D Loss: 0.5184 | G Loss: 2.2380\n",
      "Epoch [1764/5000] | D Loss: 0.9452 | G Loss: 0.7853\n",
      "Epoch [1765/5000] | D Loss: 2.1747 | G Loss: 0.2817\n",
      "Epoch [1766/5000] | D Loss: 2.4612 | G Loss: 0.2571\n",
      "Epoch [1767/5000] | D Loss: 1.0798 | G Loss: 2.1353\n",
      "Epoch [1768/5000] | D Loss: 1.5562 | G Loss: 0.5921\n",
      "Epoch [1769/5000] | D Loss: 1.5934 | G Loss: 2.7730\n",
      "Epoch [1770/5000] | D Loss: 1.5682 | G Loss: 0.7861\n",
      "Epoch [1771/5000] | D Loss: 1.7259 | G Loss: 0.6376\n",
      "Epoch [1772/5000] | D Loss: 1.0124 | G Loss: 1.4756\n",
      "Epoch [1773/5000] | D Loss: 0.6675 | G Loss: 2.4573\n",
      "Epoch [1774/5000] | D Loss: 0.9136 | G Loss: 0.7690\n",
      "Epoch [1775/5000] | D Loss: 1.1859 | G Loss: 0.7065\n",
      "Epoch [1776/5000] | D Loss: 1.5238 | G Loss: 0.5643\n",
      "Epoch [1777/5000] | D Loss: 0.5411 | G Loss: 2.4959\n",
      "Epoch [1778/5000] | D Loss: 1.2017 | G Loss: 0.6407\n",
      "Epoch [1779/5000] | D Loss: 1.1810 | G Loss: 1.4959\n",
      "Epoch [1780/5000] | D Loss: 1.2071 | G Loss: 2.1364\n",
      "Epoch [1781/5000] | D Loss: 0.5670 | G Loss: 1.9997\n",
      "Epoch [1782/5000] | D Loss: 1.7284 | G Loss: 0.7639\n",
      "Epoch [1783/5000] | D Loss: 0.9393 | G Loss: 1.3706\n",
      "Epoch [1784/5000] | D Loss: 2.5132 | G Loss: 0.4272\n",
      "Epoch [1785/5000] | D Loss: 2.1168 | G Loss: 0.5058\n",
      "Epoch [1786/5000] | D Loss: 0.8931 | G Loss: 1.2342\n",
      "Epoch [1787/5000] | D Loss: 0.7438 | G Loss: 2.3331\n",
      "Epoch [1788/5000] | D Loss: 2.5564 | G Loss: 0.5699\n",
      "Epoch [1789/5000] | D Loss: 0.5193 | G Loss: 4.6969\n",
      "Epoch [1790/5000] | D Loss: 0.5759 | G Loss: 2.2675\n",
      "Epoch [1791/5000] | D Loss: 2.6447 | G Loss: 0.7824\n",
      "Epoch [1792/5000] | D Loss: 2.4015 | G Loss: 0.6547\n",
      "Epoch [1793/5000] | D Loss: 1.5057 | G Loss: 0.5090\n",
      "Epoch [1794/5000] | D Loss: 2.1347 | G Loss: 0.9544\n",
      "Epoch [1795/5000] | D Loss: 0.5863 | G Loss: 1.7589\n",
      "Epoch [1796/5000] | D Loss: 0.4883 | G Loss: 2.7817\n",
      "Epoch [1797/5000] | D Loss: 1.7215 | G Loss: 0.2456\n",
      "Epoch [1798/5000] | D Loss: 1.3235 | G Loss: 0.8281\n",
      "Epoch [1799/5000] | D Loss: 2.6360 | G Loss: 0.6051\n",
      "Epoch [1800/5000] | D Loss: 1.6468 | G Loss: 1.1387\n",
      "Epoch 1800 FID Score: 122.2749\n",
      "Epoch [1801/5000] | D Loss: 0.9289 | G Loss: 1.5944\n",
      "Epoch [1802/5000] | D Loss: 1.2974 | G Loss: 0.4261\n",
      "Epoch [1803/5000] | D Loss: 0.5631 | G Loss: 2.3857\n",
      "Epoch [1804/5000] | D Loss: 1.0916 | G Loss: 1.1688\n",
      "Epoch [1805/5000] | D Loss: 2.4893 | G Loss: 1.0450\n",
      "Epoch [1806/5000] | D Loss: 0.5815 | G Loss: 1.9158\n",
      "Epoch [1807/5000] | D Loss: 0.5284 | G Loss: 4.0615\n",
      "Epoch [1808/5000] | D Loss: 0.8486 | G Loss: 1.4153\n",
      "Epoch [1809/5000] | D Loss: 1.7175 | G Loss: 0.6215\n",
      "Epoch [1810/5000] | D Loss: 1.2398 | G Loss: 0.6585\n",
      "Epoch [1811/5000] | D Loss: 1.4244 | G Loss: 1.0835\n",
      "Epoch [1812/5000] | D Loss: 0.9763 | G Loss: 0.5230\n",
      "Epoch [1813/5000] | D Loss: 1.2427 | G Loss: 1.4194\n",
      "Epoch [1814/5000] | D Loss: 1.4465 | G Loss: 0.4343\n",
      "Epoch [1815/5000] | D Loss: 2.5114 | G Loss: 0.5630\n",
      "Epoch [1816/5000] | D Loss: 1.4094 | G Loss: 0.9649\n",
      "Epoch [1817/5000] | D Loss: 1.4757 | G Loss: 0.8315\n",
      "Epoch [1818/5000] | D Loss: 0.9148 | G Loss: 1.9166\n",
      "Epoch [1819/5000] | D Loss: 0.8575 | G Loss: 1.7092\n",
      "Epoch [1820/5000] | D Loss: 1.0337 | G Loss: 1.0587\n",
      "Epoch [1821/5000] | D Loss: 2.7442 | G Loss: 0.8393\n",
      "Epoch [1822/5000] | D Loss: 1.4335 | G Loss: 0.4017\n",
      "Epoch [1823/5000] | D Loss: 0.8642 | G Loss: 1.3862\n",
      "Epoch [1824/5000] | D Loss: 1.6207 | G Loss: 0.5202\n",
      "Epoch [1825/5000] | D Loss: 0.8049 | G Loss: 1.9697\n",
      "Epoch [1826/5000] | D Loss: 0.8636 | G Loss: 1.0830\n",
      "Epoch [1827/5000] | D Loss: 2.7462 | G Loss: 0.6673\n",
      "Epoch [1828/5000] | D Loss: 0.8480 | G Loss: 0.7381\n",
      "Epoch [1829/5000] | D Loss: 1.5430 | G Loss: 0.5615\n",
      "Epoch [1830/5000] | D Loss: 3.5442 | G Loss: 0.4672\n",
      "Epoch [1831/5000] | D Loss: 0.5662 | G Loss: 2.9416\n",
      "Epoch [1832/5000] | D Loss: 0.9984 | G Loss: 2.1458\n",
      "Epoch [1833/5000] | D Loss: 1.3885 | G Loss: 1.2048\n",
      "Epoch [1834/5000] | D Loss: 1.0632 | G Loss: 1.6131\n",
      "Epoch [1835/5000] | D Loss: 1.0043 | G Loss: 1.2623\n",
      "Epoch [1836/5000] | D Loss: 0.6250 | G Loss: 2.0640\n",
      "Epoch [1837/5000] | D Loss: 2.9068 | G Loss: 0.6769\n",
      "Epoch [1838/5000] | D Loss: 0.8023 | G Loss: 2.5283\n",
      "Epoch [1839/5000] | D Loss: 1.0418 | G Loss: 1.8731\n",
      "Epoch [1840/5000] | D Loss: 0.7329 | G Loss: 1.5662\n",
      "Epoch [1841/5000] | D Loss: 2.1806 | G Loss: 0.4425\n",
      "Epoch [1842/5000] | D Loss: 2.1739 | G Loss: 0.4706\n",
      "Epoch [1843/5000] | D Loss: 1.3066 | G Loss: 0.5666\n",
      "Epoch [1844/5000] | D Loss: 1.5988 | G Loss: 0.6263\n",
      "Epoch [1845/5000] | D Loss: 2.0402 | G Loss: 0.4109\n",
      "Epoch [1846/5000] | D Loss: 2.1082 | G Loss: 0.2476\n",
      "Epoch [1847/5000] | D Loss: 2.4558 | G Loss: 0.2204\n",
      "Epoch [1848/5000] | D Loss: 1.3637 | G Loss: 1.6431\n",
      "Epoch [1849/5000] | D Loss: 2.6980 | G Loss: 1.0888\n",
      "Epoch [1850/5000] | D Loss: 1.9677 | G Loss: 0.8996\n",
      "Epoch [1851/5000] | D Loss: 0.4938 | G Loss: 3.0988\n",
      "Epoch [1852/5000] | D Loss: 2.0193 | G Loss: 1.2287\n",
      "Epoch [1853/5000] | D Loss: 1.2813 | G Loss: 0.5372\n",
      "Epoch [1854/5000] | D Loss: 2.2923 | G Loss: 0.5275\n",
      "Epoch [1855/5000] | D Loss: 0.4620 | G Loss: 2.5230\n",
      "Epoch [1856/5000] | D Loss: 0.9363 | G Loss: 1.7102\n",
      "Epoch [1857/5000] | D Loss: 2.8349 | G Loss: 0.9440\n",
      "Epoch [1858/5000] | D Loss: 2.3201 | G Loss: 0.7837\n",
      "Epoch [1859/5000] | D Loss: 1.2524 | G Loss: 0.4525\n",
      "Epoch [1860/5000] | D Loss: 3.1221 | G Loss: 2.6504\n",
      "Epoch [1861/5000] | D Loss: 0.6257 | G Loss: 1.9981\n",
      "Epoch [1862/5000] | D Loss: 1.0899 | G Loss: 1.6184\n",
      "Epoch [1863/5000] | D Loss: 0.5011 | G Loss: 2.4084\n",
      "Epoch [1864/5000] | D Loss: 2.5392 | G Loss: 0.6873\n",
      "Epoch [1865/5000] | D Loss: 1.6425 | G Loss: 0.3340\n",
      "Epoch [1866/5000] | D Loss: 1.2216 | G Loss: 1.6897\n",
      "Epoch [1867/5000] | D Loss: 2.7672 | G Loss: 2.6009\n",
      "Epoch [1868/5000] | D Loss: 1.1011 | G Loss: 0.7398\n",
      "Epoch [1869/5000] | D Loss: 1.7190 | G Loss: 0.9649\n",
      "Epoch [1870/5000] | D Loss: 2.0909 | G Loss: 0.3968\n",
      "Epoch [1871/5000] | D Loss: 0.8572 | G Loss: 2.5015\n",
      "Epoch [1872/5000] | D Loss: 1.4291 | G Loss: 0.5308\n",
      "Epoch [1873/5000] | D Loss: 1.2827 | G Loss: 1.0389\n",
      "Epoch [1874/5000] | D Loss: 2.2583 | G Loss: 0.4359\n",
      "Epoch [1875/5000] | D Loss: 2.0724 | G Loss: 0.3441\n",
      "Epoch [1876/5000] | D Loss: 1.6620 | G Loss: 1.6771\n",
      "Epoch [1877/5000] | D Loss: 0.8297 | G Loss: 1.3404\n",
      "Epoch [1878/5000] | D Loss: 2.3625 | G Loss: 0.9254\n",
      "Epoch [1879/5000] | D Loss: 1.5164 | G Loss: 1.1218\n",
      "Epoch [1880/5000] | D Loss: 1.6374 | G Loss: 0.9664\n",
      "Epoch [1881/5000] | D Loss: 2.0052 | G Loss: 1.0164\n",
      "Epoch [1882/5000] | D Loss: 2.0054 | G Loss: 0.8577\n",
      "Epoch [1883/5000] | D Loss: 2.2692 | G Loss: 0.7159\n",
      "Epoch [1884/5000] | D Loss: 1.5322 | G Loss: 1.0293\n",
      "Epoch [1885/5000] | D Loss: 1.1264 | G Loss: 1.1431\n",
      "Epoch [1886/5000] | D Loss: 1.0986 | G Loss: 0.6423\n",
      "Epoch [1887/5000] | D Loss: 0.6521 | G Loss: 1.9077\n",
      "Epoch [1888/5000] | D Loss: 2.1582 | G Loss: 0.5336\n",
      "Epoch [1889/5000] | D Loss: 2.4003 | G Loss: 1.2028\n",
      "Epoch [1890/5000] | D Loss: 1.6601 | G Loss: 0.9603\n",
      "Epoch [1891/5000] | D Loss: 1.2531 | G Loss: 0.4031\n",
      "Epoch [1892/5000] | D Loss: 1.2357 | G Loss: 1.8665\n",
      "Epoch [1893/5000] | D Loss: 1.0646 | G Loss: 1.0473\n",
      "Epoch [1894/5000] | D Loss: 1.9459 | G Loss: 0.2711\n",
      "Epoch [1895/5000] | D Loss: 2.3742 | G Loss: 0.8419\n",
      "Epoch [1896/5000] | D Loss: 2.1777 | G Loss: 0.2729\n",
      "Epoch [1897/5000] | D Loss: 1.9391 | G Loss: 1.0456\n",
      "Epoch [1898/5000] | D Loss: 0.5252 | G Loss: 2.4975\n",
      "Epoch [1899/5000] | D Loss: 2.1651 | G Loss: 0.3891\n",
      "Epoch [1900/5000] | D Loss: 1.3385 | G Loss: 1.1518\n",
      "Epoch 1900 FID Score: 120.4351\n",
      "Epoch [1901/5000] | D Loss: 2.3888 | G Loss: 0.4212\n",
      "Epoch [1902/5000] | D Loss: 1.0900 | G Loss: 0.7654\n",
      "Epoch [1903/5000] | D Loss: 2.7987 | G Loss: 0.3486\n",
      "Epoch [1904/5000] | D Loss: 1.3404 | G Loss: 0.4066\n",
      "Epoch [1905/5000] | D Loss: 1.6646 | G Loss: 0.6241\n",
      "Epoch [1906/5000] | D Loss: 0.7577 | G Loss: 1.5992\n",
      "Epoch [1907/5000] | D Loss: 1.4960 | G Loss: 0.6072\n",
      "Epoch [1908/5000] | D Loss: 1.7098 | G Loss: 0.6423\n",
      "Epoch [1909/5000] | D Loss: 2.1490 | G Loss: 0.6815\n",
      "Epoch [1910/5000] | D Loss: 2.6190 | G Loss: 0.6294\n",
      "Epoch [1911/5000] | D Loss: 1.9011 | G Loss: 0.5361\n",
      "Epoch [1912/5000] | D Loss: 2.3841 | G Loss: 0.2492\n",
      "Epoch [1913/5000] | D Loss: 1.4288 | G Loss: 0.3804\n",
      "Epoch [1914/5000] | D Loss: 1.5787 | G Loss: 0.5013\n",
      "Epoch [1915/5000] | D Loss: 1.1582 | G Loss: 1.0171\n",
      "Epoch [1916/5000] | D Loss: 1.2572 | G Loss: 1.7596\n",
      "Epoch [1917/5000] | D Loss: 2.0929 | G Loss: 0.4057\n",
      "Epoch [1918/5000] | D Loss: 2.4990 | G Loss: 0.5326\n",
      "Epoch [1919/5000] | D Loss: 1.6811 | G Loss: 0.5438\n",
      "Epoch [1920/5000] | D Loss: 1.8816 | G Loss: 0.4581\n",
      "Epoch [1921/5000] | D Loss: 1.4765 | G Loss: 0.6511\n",
      "Epoch [1922/5000] | D Loss: 1.9420 | G Loss: 0.5897\n",
      "Epoch [1923/5000] | D Loss: 1.6442 | G Loss: 0.9625\n",
      "Epoch [1924/5000] | D Loss: 0.8953 | G Loss: 0.9640\n",
      "Epoch [1925/5000] | D Loss: 2.5274 | G Loss: 0.7297\n",
      "Epoch [1926/5000] | D Loss: 2.1184 | G Loss: 0.4072\n",
      "Epoch [1927/5000] | D Loss: 0.8879 | G Loss: 2.5661\n",
      "Epoch [1928/5000] | D Loss: 0.5243 | G Loss: 2.2371\n",
      "Epoch [1929/5000] | D Loss: 0.5587 | G Loss: 3.3122\n",
      "Epoch [1930/5000] | D Loss: 1.2781 | G Loss: 0.4924\n",
      "Epoch [1931/5000] | D Loss: 1.1709 | G Loss: 1.0913\n",
      "Epoch [1932/5000] | D Loss: 1.8090 | G Loss: 1.0112\n",
      "Epoch [1933/5000] | D Loss: 2.0091 | G Loss: 0.5553\n",
      "Epoch [1934/5000] | D Loss: 2.1429 | G Loss: 0.5750\n",
      "Epoch [1935/5000] | D Loss: 2.1825 | G Loss: 0.5245\n",
      "Epoch [1936/5000] | D Loss: 0.4939 | G Loss: 2.9227\n",
      "Epoch [1937/5000] | D Loss: 1.4944 | G Loss: 1.2101\n",
      "Epoch [1938/5000] | D Loss: 0.9123 | G Loss: 1.3400\n",
      "Epoch [1939/5000] | D Loss: 1.7270 | G Loss: 0.4828\n",
      "Epoch [1940/5000] | D Loss: 1.5113 | G Loss: 1.0015\n",
      "Epoch [1941/5000] | D Loss: 0.8482 | G Loss: 1.2223\n",
      "Epoch [1942/5000] | D Loss: 1.7483 | G Loss: 1.8718\n",
      "Epoch [1943/5000] | D Loss: 2.2918 | G Loss: 1.1903\n",
      "Epoch [1944/5000] | D Loss: 1.6100 | G Loss: 0.6234\n",
      "Epoch [1945/5000] | D Loss: 0.6268 | G Loss: 1.5382\n",
      "Epoch [1946/5000] | D Loss: 2.2075 | G Loss: 0.4898\n",
      "Epoch [1947/5000] | D Loss: 1.9376 | G Loss: 0.5199\n",
      "Epoch [1948/5000] | D Loss: 2.1216 | G Loss: 1.0943\n",
      "Epoch [1949/5000] | D Loss: 1.5192 | G Loss: 0.5948\n",
      "Epoch [1950/5000] | D Loss: 0.7505 | G Loss: 2.2256\n",
      "Epoch [1951/5000] | D Loss: 1.6533 | G Loss: 0.5606\n",
      "Epoch [1952/5000] | D Loss: 1.5399 | G Loss: 0.4228\n",
      "Epoch [1953/5000] | D Loss: 1.8112 | G Loss: 0.4982\n",
      "Epoch [1954/5000] | D Loss: 1.7398 | G Loss: 0.3203\n",
      "Epoch [1955/5000] | D Loss: 2.8379 | G Loss: 1.7987\n",
      "Epoch [1956/5000] | D Loss: 1.3588 | G Loss: 1.4660\n",
      "Epoch [1957/5000] | D Loss: 0.5602 | G Loss: 2.3005\n",
      "Epoch [1958/5000] | D Loss: 2.3649 | G Loss: 0.7544\n",
      "Epoch [1959/5000] | D Loss: 0.8240 | G Loss: 1.6073\n",
      "Epoch [1960/5000] | D Loss: 0.6090 | G Loss: 1.9953\n",
      "Epoch [1961/5000] | D Loss: 1.9423 | G Loss: 0.2382\n",
      "Epoch [1962/5000] | D Loss: 0.8305 | G Loss: 1.9932\n",
      "Epoch [1963/5000] | D Loss: 1.6644 | G Loss: 1.0092\n",
      "Epoch [1964/5000] | D Loss: 2.1081 | G Loss: 0.4869\n",
      "Epoch [1965/5000] | D Loss: 0.7670 | G Loss: 1.4287\n",
      "Epoch [1966/5000] | D Loss: 2.0594 | G Loss: 0.4794\n",
      "Epoch [1967/5000] | D Loss: 0.5600 | G Loss: 2.1770\n",
      "Epoch [1968/5000] | D Loss: 0.9187 | G Loss: 1.6456\n",
      "Epoch [1969/5000] | D Loss: 2.4834 | G Loss: 0.9721\n",
      "Epoch [1970/5000] | D Loss: 2.3325 | G Loss: 0.9068\n",
      "Epoch [1971/5000] | D Loss: 2.6054 | G Loss: 0.8027\n",
      "Epoch [1972/5000] | D Loss: 1.1415 | G Loss: 0.9001\n",
      "Epoch [1973/5000] | D Loss: 2.2752 | G Loss: 0.7400\n",
      "Epoch [1974/5000] | D Loss: 1.8495 | G Loss: 1.2048\n",
      "Epoch [1975/5000] | D Loss: 2.1768 | G Loss: 0.9746\n",
      "Epoch [1976/5000] | D Loss: 0.6827 | G Loss: 2.1532\n",
      "Epoch [1977/5000] | D Loss: 1.4852 | G Loss: 0.5418\n",
      "Epoch [1978/5000] | D Loss: 2.3894 | G Loss: 1.6990\n",
      "Epoch [1979/5000] | D Loss: 2.2298 | G Loss: 0.6731\n",
      "Epoch [1980/5000] | D Loss: 2.2931 | G Loss: 0.3768\n",
      "Epoch [1981/5000] | D Loss: 1.6926 | G Loss: 0.9433\n",
      "Epoch [1982/5000] | D Loss: 1.5820 | G Loss: 0.4462\n",
      "Epoch [1983/5000] | D Loss: 1.5086 | G Loss: 0.6356\n",
      "Epoch [1984/5000] | D Loss: 0.8042 | G Loss: 1.5434\n",
      "Epoch [1985/5000] | D Loss: 2.3665 | G Loss: 1.2737\n",
      "Epoch [1986/5000] | D Loss: 1.2018 | G Loss: 0.6851\n",
      "Epoch [1987/5000] | D Loss: 0.5917 | G Loss: 2.4431\n",
      "Epoch [1988/5000] | D Loss: 0.4901 | G Loss: 3.8488\n",
      "Epoch [1989/5000] | D Loss: 1.2982 | G Loss: 0.8214\n",
      "Epoch [1990/5000] | D Loss: 0.7634 | G Loss: 2.0173\n",
      "Epoch [1991/5000] | D Loss: 1.8641 | G Loss: 0.8441\n",
      "Epoch [1992/5000] | D Loss: 1.9901 | G Loss: 0.4897\n",
      "Epoch [1993/5000] | D Loss: 0.7795 | G Loss: 1.3997\n",
      "Epoch [1994/5000] | D Loss: 1.7335 | G Loss: 1.1994\n",
      "Epoch [1995/5000] | D Loss: 1.1255 | G Loss: 0.9430\n",
      "Epoch [1996/5000] | D Loss: 1.0528 | G Loss: 1.1708\n",
      "Epoch [1997/5000] | D Loss: 0.6506 | G Loss: 1.7591\n",
      "Epoch [1998/5000] | D Loss: 1.7833 | G Loss: 0.9324\n",
      "Epoch [1999/5000] | D Loss: 1.9748 | G Loss: 0.6325\n",
      "Epoch [2000/5000] | D Loss: 2.0987 | G Loss: 0.5475\n",
      "Epoch 2000 FID Score: 113.6968\n",
      "Epoch [2001/5000] | D Loss: 1.2420 | G Loss: 1.3455\n",
      "Epoch [2002/5000] | D Loss: 1.6621 | G Loss: 0.9074\n",
      "Epoch [2003/5000] | D Loss: 2.0336 | G Loss: 0.7472\n",
      "Epoch [2004/5000] | D Loss: 0.5952 | G Loss: 1.4204\n",
      "Epoch [2005/5000] | D Loss: 1.6733 | G Loss: 0.6500\n",
      "Epoch [2006/5000] | D Loss: 2.5510 | G Loss: 0.6742\n",
      "Epoch [2007/5000] | D Loss: 1.9826 | G Loss: 0.3439\n",
      "Epoch [2008/5000] | D Loss: 1.4651 | G Loss: 1.9736\n",
      "Epoch [2009/5000] | D Loss: 1.7337 | G Loss: 0.3501\n",
      "Epoch [2010/5000] | D Loss: 1.4242 | G Loss: 0.5005\n",
      "Epoch [2011/5000] | D Loss: 1.7424 | G Loss: 0.6363\n",
      "Epoch [2012/5000] | D Loss: 2.0084 | G Loss: 0.5802\n",
      "Epoch [2013/5000] | D Loss: 1.0156 | G Loss: 1.4216\n",
      "Epoch [2014/5000] | D Loss: 1.8073 | G Loss: 0.6321\n",
      "Epoch [2015/5000] | D Loss: 0.4577 | G Loss: 2.8432\n",
      "Epoch [2016/5000] | D Loss: 1.9507 | G Loss: 1.0658\n",
      "Epoch [2017/5000] | D Loss: 2.3026 | G Loss: 1.0120\n",
      "Epoch [2018/5000] | D Loss: 0.9205 | G Loss: 1.3771\n",
      "Epoch [2019/5000] | D Loss: 1.3953 | G Loss: 0.9557\n",
      "Epoch [2020/5000] | D Loss: 2.0993 | G Loss: 0.4479\n",
      "Epoch [2021/5000] | D Loss: 2.3717 | G Loss: 0.3636\n",
      "Epoch [2022/5000] | D Loss: 0.6124 | G Loss: 1.6870\n",
      "Epoch [2023/5000] | D Loss: 1.9794 | G Loss: 0.9647\n",
      "Epoch [2024/5000] | D Loss: 0.8895 | G Loss: 1.2704\n",
      "Epoch [2025/5000] | D Loss: 1.8461 | G Loss: 0.7362\n",
      "Epoch [2026/5000] | D Loss: 0.9466 | G Loss: 0.8169\n",
      "Epoch [2027/5000] | D Loss: 1.4098 | G Loss: 0.9345\n",
      "Epoch [2028/5000] | D Loss: 0.4565 | G Loss: 3.3362\n",
      "Epoch [2029/5000] | D Loss: 0.8444 | G Loss: 1.6139\n",
      "Epoch [2030/5000] | D Loss: 1.6192 | G Loss: 0.4544\n",
      "Epoch [2031/5000] | D Loss: 1.8178 | G Loss: 0.6639\n",
      "Epoch [2032/5000] | D Loss: 1.7998 | G Loss: 0.7869\n",
      "Epoch [2033/5000] | D Loss: 0.4979 | G Loss: 2.2051\n",
      "Epoch [2034/5000] | D Loss: 2.1991 | G Loss: 0.4333\n",
      "Epoch [2035/5000] | D Loss: 1.2085 | G Loss: 0.8768\n",
      "Epoch [2036/5000] | D Loss: 2.0705 | G Loss: 0.7764\n",
      "Epoch [2037/5000] | D Loss: 2.2586 | G Loss: 0.4572\n",
      "Epoch [2038/5000] | D Loss: 0.9065 | G Loss: 1.5261\n",
      "Epoch [2039/5000] | D Loss: 1.0390 | G Loss: 0.6744\n",
      "Epoch [2040/5000] | D Loss: 1.2392 | G Loss: 0.7639\n",
      "Epoch [2041/5000] | D Loss: 1.5783 | G Loss: 0.8479\n",
      "Epoch [2042/5000] | D Loss: 1.6893 | G Loss: 0.6666\n",
      "Epoch [2043/5000] | D Loss: 0.9692 | G Loss: 1.3690\n",
      "Epoch [2044/5000] | D Loss: 1.3133 | G Loss: 1.2042\n",
      "Epoch [2045/5000] | D Loss: 1.9777 | G Loss: 1.1438\n",
      "Epoch [2046/5000] | D Loss: 1.6528 | G Loss: 1.2083\n",
      "Epoch [2047/5000] | D Loss: 2.0796 | G Loss: 0.6299\n",
      "Epoch [2048/5000] | D Loss: 0.4439 | G Loss: 3.2932\n",
      "Epoch [2049/5000] | D Loss: 1.5578 | G Loss: 0.6813\n",
      "Epoch [2050/5000] | D Loss: 1.6323 | G Loss: 0.4351\n",
      "Epoch [2051/5000] | D Loss: 2.3176 | G Loss: 1.0192\n",
      "Epoch [2052/5000] | D Loss: 1.0888 | G Loss: 1.0092\n",
      "Epoch [2053/5000] | D Loss: 1.1407 | G Loss: 1.6131\n",
      "Epoch [2054/5000] | D Loss: 1.3669 | G Loss: 0.9466\n",
      "Epoch [2055/5000] | D Loss: 1.7361 | G Loss: 0.5467\n",
      "Epoch [2056/5000] | D Loss: 1.3403 | G Loss: 1.1329\n",
      "Epoch [2057/5000] | D Loss: 1.4192 | G Loss: 0.9242\n",
      "Epoch [2058/5000] | D Loss: 2.0878 | G Loss: 1.4312\n",
      "Epoch [2059/5000] | D Loss: 0.7712 | G Loss: 2.1451\n",
      "Epoch [2060/5000] | D Loss: 1.4738 | G Loss: 0.9829\n",
      "Epoch [2061/5000] | D Loss: 2.0206 | G Loss: 0.8011\n",
      "Epoch [2062/5000] | D Loss: 0.9085 | G Loss: 1.9931\n",
      "Epoch [2063/5000] | D Loss: 1.4380 | G Loss: 1.5056\n",
      "Epoch [2064/5000] | D Loss: 0.6740 | G Loss: 1.1490\n",
      "Epoch [2065/5000] | D Loss: 2.1442 | G Loss: 0.6763\n",
      "Epoch [2066/5000] | D Loss: 1.7739 | G Loss: 1.0159\n",
      "Epoch [2067/5000] | D Loss: 1.6765 | G Loss: 0.6873\n",
      "Epoch [2068/5000] | D Loss: 1.7563 | G Loss: 0.3951\n",
      "Epoch [2069/5000] | D Loss: 1.0124 | G Loss: 2.3061\n",
      "Epoch [2070/5000] | D Loss: 2.0384 | G Loss: 0.7709\n",
      "Epoch [2071/5000] | D Loss: 1.8914 | G Loss: 0.9032\n",
      "Epoch [2072/5000] | D Loss: 1.6702 | G Loss: 0.6860\n",
      "Epoch [2073/5000] | D Loss: 2.2262 | G Loss: 0.5537\n",
      "Epoch [2074/5000] | D Loss: 1.0173 | G Loss: 1.1783\n",
      "Epoch [2075/5000] | D Loss: 0.6829 | G Loss: 2.0596\n",
      "Epoch [2076/5000] | D Loss: 1.1223 | G Loss: 1.1485\n",
      "Epoch [2077/5000] | D Loss: 1.7187 | G Loss: 1.2725\n",
      "Epoch [2078/5000] | D Loss: 1.6374 | G Loss: 0.6733\n",
      "Epoch [2079/5000] | D Loss: 2.0791 | G Loss: 0.6451\n",
      "Epoch [2080/5000] | D Loss: 1.2120 | G Loss: 0.7351\n",
      "Epoch [2081/5000] | D Loss: 1.8081 | G Loss: 0.4200\n",
      "Epoch [2082/5000] | D Loss: 1.3013 | G Loss: 1.1541\n",
      "Epoch [2083/5000] | D Loss: 1.6366 | G Loss: 0.6365\n",
      "Epoch [2084/5000] | D Loss: 1.9250 | G Loss: 0.9232\n",
      "Epoch [2085/5000] | D Loss: 1.7225 | G Loss: 0.6974\n",
      "Epoch [2086/5000] | D Loss: 0.6081 | G Loss: 2.2085\n",
      "Epoch [2087/5000] | D Loss: 1.1282 | G Loss: 1.0304\n",
      "Epoch [2088/5000] | D Loss: 2.0398 | G Loss: 0.4494\n",
      "Epoch [2089/5000] | D Loss: 0.8875 | G Loss: 1.0627\n",
      "Epoch [2090/5000] | D Loss: 0.7478 | G Loss: 1.5321\n",
      "Epoch [2091/5000] | D Loss: 1.9470 | G Loss: 0.7490\n",
      "Epoch [2092/5000] | D Loss: 1.4069 | G Loss: 0.9306\n",
      "Epoch [2093/5000] | D Loss: 1.8764 | G Loss: 0.7254\n",
      "Epoch [2094/5000] | D Loss: 0.4781 | G Loss: 1.9712\n",
      "Epoch [2095/5000] | D Loss: 1.4235 | G Loss: 1.0183\n",
      "Epoch [2096/5000] | D Loss: 1.5932 | G Loss: 0.8281\n",
      "Epoch [2097/5000] | D Loss: 1.2926 | G Loss: 0.8990\n",
      "Epoch [2098/5000] | D Loss: 1.8099 | G Loss: 0.7379\n",
      "Epoch [2099/5000] | D Loss: 1.4826 | G Loss: 0.8901\n",
      "Epoch [2100/5000] | D Loss: 1.0057 | G Loss: 0.9025\n",
      "Epoch 2100 FID Score: 107.8826\n",
      "Saved improved model at Epoch 2100 with FID 107.8826\n",
      "Epoch [2101/5000] | D Loss: 1.1496 | G Loss: 0.7127\n",
      "Epoch [2102/5000] | D Loss: 1.4748 | G Loss: 0.7540\n",
      "Epoch [2103/5000] | D Loss: 1.9576 | G Loss: 1.4730\n",
      "Epoch [2104/5000] | D Loss: 0.8519 | G Loss: 0.9491\n",
      "Epoch [2105/5000] | D Loss: 2.0294 | G Loss: 0.7895\n",
      "Epoch [2106/5000] | D Loss: 1.3388 | G Loss: 1.1018\n",
      "Epoch [2107/5000] | D Loss: 1.3463 | G Loss: 0.8864\n",
      "Epoch [2108/5000] | D Loss: 1.7130 | G Loss: 0.8733\n",
      "Epoch [2109/5000] | D Loss: 1.2678 | G Loss: 0.5693\n",
      "Epoch [2110/5000] | D Loss: 1.5259 | G Loss: 0.5689\n",
      "Epoch [2111/5000] | D Loss: 1.7232 | G Loss: 0.6837\n",
      "Epoch [2112/5000] | D Loss: 1.9038 | G Loss: 0.6188\n",
      "Epoch [2113/5000] | D Loss: 1.6741 | G Loss: 0.8046\n",
      "Epoch [2114/5000] | D Loss: 1.2436 | G Loss: 0.9612\n",
      "Epoch [2115/5000] | D Loss: 1.6884 | G Loss: 0.7538\n",
      "Epoch [2116/5000] | D Loss: 1.5173 | G Loss: 1.1035\n",
      "Epoch [2117/5000] | D Loss: 1.7201 | G Loss: 0.6011\n",
      "Epoch [2118/5000] | D Loss: 1.6020 | G Loss: 0.8335\n",
      "Epoch [2119/5000] | D Loss: 1.4440 | G Loss: 0.9592\n",
      "Epoch [2120/5000] | D Loss: 1.8674 | G Loss: 0.8230\n",
      "Epoch [2121/5000] | D Loss: 0.5422 | G Loss: 2.4933\n",
      "Epoch [2122/5000] | D Loss: 1.3764 | G Loss: 0.8710\n",
      "Epoch [2123/5000] | D Loss: 1.8268 | G Loss: 1.1685\n",
      "Epoch [2124/5000] | D Loss: 1.5873 | G Loss: 0.6949\n",
      "Epoch [2125/5000] | D Loss: 1.3955 | G Loss: 0.8449\n",
      "Epoch [2126/5000] | D Loss: 1.6648 | G Loss: 0.8788\n",
      "Epoch [2127/5000] | D Loss: 1.4384 | G Loss: 0.8731\n",
      "Epoch [2128/5000] | D Loss: 1.2905 | G Loss: 0.9602\n",
      "Epoch [2129/5000] | D Loss: 1.7406 | G Loss: 0.6174\n",
      "Epoch [2130/5000] | D Loss: 1.3205 | G Loss: 0.7437\n",
      "Epoch [2131/5000] | D Loss: 1.3654 | G Loss: 0.7098\n",
      "Epoch [2132/5000] | D Loss: 2.0172 | G Loss: 0.7952\n",
      "Epoch [2133/5000] | D Loss: 1.5188 | G Loss: 0.7609\n",
      "Epoch [2134/5000] | D Loss: 1.4718 | G Loss: 0.9866\n",
      "Epoch [2135/5000] | D Loss: 1.5536 | G Loss: 0.6488\n",
      "Epoch [2136/5000] | D Loss: 1.7413 | G Loss: 1.1614\n",
      "Epoch [2137/5000] | D Loss: 1.1469 | G Loss: 0.8614\n",
      "Epoch [2138/5000] | D Loss: 1.7761 | G Loss: 0.9756\n",
      "Epoch [2139/5000] | D Loss: 1.8189 | G Loss: 0.8344\n",
      "Epoch [2140/5000] | D Loss: 1.2478 | G Loss: 1.5758\n",
      "Epoch [2141/5000] | D Loss: 1.5170 | G Loss: 0.8517\n",
      "Epoch [2142/5000] | D Loss: 1.6072 | G Loss: 0.7852\n",
      "Epoch [2143/5000] | D Loss: 1.4613 | G Loss: 0.5788\n",
      "Epoch [2144/5000] | D Loss: 1.7042 | G Loss: 0.8876\n",
      "Epoch [2145/5000] | D Loss: 1.0879 | G Loss: 1.1599\n",
      "Epoch [2146/5000] | D Loss: 1.2897 | G Loss: 0.8383\n",
      "Epoch [2147/5000] | D Loss: 1.4374 | G Loss: 0.6050\n",
      "Epoch [2148/5000] | D Loss: 1.5898 | G Loss: 0.9081\n",
      "Epoch [2149/5000] | D Loss: 1.4602 | G Loss: 0.8379\n",
      "Epoch [2150/5000] | D Loss: 1.3167 | G Loss: 1.2880\n",
      "Epoch [2151/5000] | D Loss: 1.0388 | G Loss: 1.1803\n",
      "Epoch [2152/5000] | D Loss: 1.2111 | G Loss: 1.0530\n",
      "Epoch [2153/5000] | D Loss: 1.6200 | G Loss: 0.9441\n",
      "Epoch [2154/5000] | D Loss: 1.3584 | G Loss: 0.8359\n",
      "Epoch [2155/5000] | D Loss: 1.8705 | G Loss: 0.9339\n",
      "Epoch [2156/5000] | D Loss: 1.2901 | G Loss: 1.0360\n",
      "Epoch [2157/5000] | D Loss: 1.6579 | G Loss: 0.9011\n",
      "Epoch [2158/5000] | D Loss: 1.5577 | G Loss: 1.1035\n",
      "Epoch [2159/5000] | D Loss: 1.2921 | G Loss: 0.8508\n",
      "Epoch [2160/5000] | D Loss: 1.3822 | G Loss: 0.7157\n",
      "Epoch [2161/5000] | D Loss: 1.3425 | G Loss: 0.8455\n",
      "Epoch [2162/5000] | D Loss: 1.6961 | G Loss: 0.7870\n",
      "Epoch [2163/5000] | D Loss: 1.5474 | G Loss: 0.9970\n",
      "Epoch [2164/5000] | D Loss: 1.7127 | G Loss: 1.2274\n",
      "Epoch [2165/5000] | D Loss: 1.5088 | G Loss: 0.8325\n",
      "Epoch [2166/5000] | D Loss: 1.3763 | G Loss: 0.7659\n",
      "Epoch [2167/5000] | D Loss: 1.4274 | G Loss: 0.9739\n",
      "Epoch [2168/5000] | D Loss: 1.4489 | G Loss: 0.9675\n",
      "Epoch [2169/5000] | D Loss: 1.5947 | G Loss: 0.9631\n",
      "Epoch [2170/5000] | D Loss: 1.4769 | G Loss: 0.8234\n",
      "Epoch [2171/5000] | D Loss: 1.2034 | G Loss: 0.7479\n",
      "Epoch [2172/5000] | D Loss: 1.5511 | G Loss: 1.0321\n",
      "Epoch [2173/5000] | D Loss: 1.3001 | G Loss: 1.0009\n",
      "Epoch [2174/5000] | D Loss: 1.5975 | G Loss: 0.7357\n",
      "Epoch [2175/5000] | D Loss: 0.7334 | G Loss: 1.4510\n",
      "Epoch [2176/5000] | D Loss: 1.6837 | G Loss: 0.9053\n",
      "Epoch [2177/5000] | D Loss: 1.3506 | G Loss: 0.5967\n",
      "Epoch [2178/5000] | D Loss: 1.3781 | G Loss: 0.8082\n",
      "Epoch [2179/5000] | D Loss: 1.4929 | G Loss: 0.7784\n",
      "Epoch [2180/5000] | D Loss: 1.4316 | G Loss: 0.7363\n",
      "Epoch [2181/5000] | D Loss: 1.6946 | G Loss: 0.7017\n",
      "Epoch [2182/5000] | D Loss: 1.3205 | G Loss: 0.8489\n",
      "Epoch [2183/5000] | D Loss: 0.7547 | G Loss: 1.2381\n",
      "Epoch [2184/5000] | D Loss: 1.4076 | G Loss: 0.7372\n",
      "Epoch [2185/5000] | D Loss: 1.3252 | G Loss: 1.2635\n",
      "Epoch [2186/5000] | D Loss: 1.5708 | G Loss: 0.8029\n",
      "Epoch [2187/5000] | D Loss: 1.3525 | G Loss: 0.8080\n",
      "Epoch [2188/5000] | D Loss: 1.3922 | G Loss: 1.1403\n",
      "Epoch [2189/5000] | D Loss: 1.5930 | G Loss: 0.6659\n",
      "Epoch [2190/5000] | D Loss: 1.4815 | G Loss: 0.8155\n",
      "Epoch [2191/5000] | D Loss: 1.3204 | G Loss: 0.9081\n",
      "Epoch [2192/5000] | D Loss: 1.5192 | G Loss: 0.8508\n",
      "Epoch [2193/5000] | D Loss: 1.5635 | G Loss: 0.8143\n",
      "Epoch [2194/5000] | D Loss: 1.5698 | G Loss: 0.8660\n",
      "Epoch [2195/5000] | D Loss: 1.4207 | G Loss: 1.0719\n",
      "Epoch [2196/5000] | D Loss: 1.2515 | G Loss: 0.9625\n",
      "Epoch [2197/5000] | D Loss: 1.5428 | G Loss: 0.6745\n",
      "Epoch [2198/5000] | D Loss: 1.3594 | G Loss: 0.8739\n",
      "Epoch [2199/5000] | D Loss: 0.6107 | G Loss: 1.8675\n",
      "Epoch [2200/5000] | D Loss: 1.5948 | G Loss: 0.9656\n",
      "Epoch 2200 FID Score: 108.5378\n",
      "Epoch [2201/5000] | D Loss: 1.5008 | G Loss: 1.0933\n",
      "Epoch [2202/5000] | D Loss: 1.2239 | G Loss: 0.8320\n",
      "Epoch [2203/5000] | D Loss: 1.0098 | G Loss: 1.0076\n",
      "Epoch [2204/5000] | D Loss: 1.4735 | G Loss: 0.8948\n",
      "Epoch [2205/5000] | D Loss: 1.4794 | G Loss: 0.6039\n",
      "Epoch [2206/5000] | D Loss: 1.2383 | G Loss: 0.7443\n",
      "Epoch [2207/5000] | D Loss: 1.5056 | G Loss: 0.9542\n",
      "Epoch [2208/5000] | D Loss: 1.3103 | G Loss: 0.5752\n",
      "Epoch [2209/5000] | D Loss: 1.2609 | G Loss: 0.9170\n",
      "Epoch [2210/5000] | D Loss: 1.4197 | G Loss: 0.9512\n",
      "Epoch [2211/5000] | D Loss: 1.5943 | G Loss: 0.7547\n",
      "Epoch [2212/5000] | D Loss: 1.1957 | G Loss: 0.8752\n",
      "Epoch [2213/5000] | D Loss: 0.7966 | G Loss: 1.7570\n",
      "Epoch [2214/5000] | D Loss: 1.2655 | G Loss: 0.9004\n",
      "Epoch [2215/5000] | D Loss: 1.5004 | G Loss: 0.9883\n",
      "Epoch [2216/5000] | D Loss: 1.2970 | G Loss: 0.7511\n",
      "Epoch [2217/5000] | D Loss: 1.3504 | G Loss: 0.7077\n",
      "Epoch [2218/5000] | D Loss: 1.5414 | G Loss: 0.7803\n",
      "Epoch [2219/5000] | D Loss: 1.4016 | G Loss: 0.6978\n",
      "Epoch [2220/5000] | D Loss: 1.3338 | G Loss: 0.7512\n",
      "Epoch [2221/5000] | D Loss: 1.5919 | G Loss: 0.6756\n",
      "Epoch [2222/5000] | D Loss: 1.5611 | G Loss: 0.6996\n",
      "Epoch [2223/5000] | D Loss: 1.4526 | G Loss: 0.7026\n",
      "Epoch [2224/5000] | D Loss: 0.9363 | G Loss: 0.9056\n",
      "Epoch [2225/5000] | D Loss: 1.4252 | G Loss: 0.7369\n",
      "Epoch [2226/5000] | D Loss: 1.5332 | G Loss: 0.6574\n",
      "Epoch [2227/5000] | D Loss: 1.2247 | G Loss: 0.7493\n",
      "Epoch [2228/5000] | D Loss: 1.2708 | G Loss: 0.7313\n",
      "Epoch [2229/5000] | D Loss: 1.4546 | G Loss: 0.8466\n",
      "Epoch [2230/5000] | D Loss: 1.3435 | G Loss: 0.8246\n",
      "Epoch [2231/5000] | D Loss: 1.2671 | G Loss: 0.8415\n",
      "Epoch [2232/5000] | D Loss: 1.1564 | G Loss: 0.8008\n",
      "Epoch [2233/5000] | D Loss: 1.2969 | G Loss: 0.9083\n",
      "Epoch [2234/5000] | D Loss: 1.4203 | G Loss: 0.5931\n",
      "Epoch [2235/5000] | D Loss: 1.6252 | G Loss: 0.7296\n",
      "Epoch [2236/5000] | D Loss: 0.7963 | G Loss: 1.8868\n",
      "Epoch [2237/5000] | D Loss: 1.1947 | G Loss: 0.8939\n",
      "Epoch [2238/5000] | D Loss: 1.5274 | G Loss: 0.7759\n",
      "Epoch [2239/5000] | D Loss: 1.0982 | G Loss: 0.8760\n",
      "Epoch [2240/5000] | D Loss: 0.9722 | G Loss: 0.7781\n",
      "Epoch [2241/5000] | D Loss: 1.5154 | G Loss: 0.7728\n",
      "Epoch [2242/5000] | D Loss: 1.3128 | G Loss: 0.7696\n",
      "Epoch [2243/5000] | D Loss: 1.3330 | G Loss: 0.8535\n",
      "Epoch [2244/5000] | D Loss: 1.4231 | G Loss: 0.8265\n",
      "Epoch [2245/5000] | D Loss: 1.3991 | G Loss: 0.8277\n",
      "Epoch [2246/5000] | D Loss: 1.1727 | G Loss: 0.8306\n",
      "Epoch [2247/5000] | D Loss: 1.1480 | G Loss: 0.8384\n",
      "Epoch [2248/5000] | D Loss: 1.5336 | G Loss: 0.6922\n",
      "Epoch [2249/5000] | D Loss: 1.6101 | G Loss: 0.8176\n",
      "Epoch [2250/5000] | D Loss: 1.3035 | G Loss: 0.8770\n",
      "Epoch [2251/5000] | D Loss: 1.1168 | G Loss: 0.9186\n",
      "Epoch [2252/5000] | D Loss: 1.4278 | G Loss: 1.0568\n",
      "Epoch [2253/5000] | D Loss: 1.3255 | G Loss: 0.6567\n",
      "Epoch [2254/5000] | D Loss: 0.8803 | G Loss: 2.0489\n",
      "Epoch [2255/5000] | D Loss: 1.2779 | G Loss: 0.7070\n",
      "Epoch [2256/5000] | D Loss: 1.0222 | G Loss: 1.0171\n",
      "Epoch [2257/5000] | D Loss: 1.1779 | G Loss: 0.8398\n",
      "Epoch [2258/5000] | D Loss: 1.3445 | G Loss: 0.7919\n",
      "Epoch [2259/5000] | D Loss: 0.6578 | G Loss: 1.6022\n",
      "Epoch [2260/5000] | D Loss: 1.2767 | G Loss: 0.8126\n",
      "Epoch [2261/5000] | D Loss: 0.9686 | G Loss: 1.1301\n",
      "Epoch [2262/5000] | D Loss: 1.4291 | G Loss: 0.7099\n",
      "Epoch [2263/5000] | D Loss: 1.3725 | G Loss: 0.7952\n",
      "Epoch [2264/5000] | D Loss: 1.5283 | G Loss: 0.7968\n",
      "Epoch [2265/5000] | D Loss: 1.3806 | G Loss: 0.7367\n",
      "Epoch [2266/5000] | D Loss: 1.2467 | G Loss: 0.8476\n",
      "Epoch [2267/5000] | D Loss: 1.3754 | G Loss: 0.6444\n",
      "Epoch [2268/5000] | D Loss: 1.6009 | G Loss: 0.9700\n",
      "Epoch [2269/5000] | D Loss: 1.2443 | G Loss: 0.6041\n",
      "Epoch [2270/5000] | D Loss: 1.2477 | G Loss: 0.8640\n",
      "Epoch [2271/5000] | D Loss: 1.6294 | G Loss: 0.6710\n",
      "Epoch [2272/5000] | D Loss: 1.5489 | G Loss: 0.6698\n",
      "Epoch [2273/5000] | D Loss: 1.3017 | G Loss: 1.1903\n",
      "Epoch [2274/5000] | D Loss: 1.5409 | G Loss: 0.9417\n",
      "Epoch [2275/5000] | D Loss: 1.4099 | G Loss: 0.7363\n",
      "Epoch [2276/5000] | D Loss: 1.4945 | G Loss: 0.9572\n",
      "Epoch [2277/5000] | D Loss: 1.2764 | G Loss: 1.0499\n",
      "Epoch [2278/5000] | D Loss: 1.4475 | G Loss: 0.7472\n",
      "Epoch [2279/5000] | D Loss: 1.2616 | G Loss: 0.9395\n",
      "Epoch [2280/5000] | D Loss: 1.3426 | G Loss: 0.6452\n",
      "Epoch [2281/5000] | D Loss: 0.7801 | G Loss: 1.4871\n",
      "Epoch [2282/5000] | D Loss: 1.4108 | G Loss: 0.8366\n",
      "Epoch [2283/5000] | D Loss: 1.4542 | G Loss: 0.6985\n",
      "Epoch [2284/5000] | D Loss: 1.3967 | G Loss: 0.6878\n",
      "Epoch [2285/5000] | D Loss: 0.9850 | G Loss: 1.4001\n",
      "Epoch [2286/5000] | D Loss: 1.1358 | G Loss: 0.8188\n",
      "Epoch [2287/5000] | D Loss: 0.6405 | G Loss: 1.4800\n",
      "Epoch [2288/5000] | D Loss: 1.3979 | G Loss: 0.6771\n",
      "Epoch [2289/5000] | D Loss: 1.3068 | G Loss: 0.7799\n",
      "Epoch [2290/5000] | D Loss: 1.3524 | G Loss: 0.9371\n",
      "Epoch [2291/5000] | D Loss: 1.2308 | G Loss: 0.7262\n",
      "Epoch [2292/5000] | D Loss: 1.5630 | G Loss: 0.6680\n",
      "Epoch [2293/5000] | D Loss: 1.4244 | G Loss: 0.7451\n",
      "Epoch [2294/5000] | D Loss: 1.0962 | G Loss: 1.0251\n",
      "Epoch [2295/5000] | D Loss: 1.3496 | G Loss: 0.7744\n",
      "Epoch [2296/5000] | D Loss: 1.4446 | G Loss: 0.8521\n",
      "Epoch [2297/5000] | D Loss: 1.2573 | G Loss: 0.8038\n",
      "Epoch [2298/5000] | D Loss: 1.4276 | G Loss: 0.8331\n",
      "Epoch [2299/5000] | D Loss: 1.3200 | G Loss: 0.6639\n",
      "Epoch [2300/5000] | D Loss: 1.2911 | G Loss: 0.9154\n",
      "Epoch 2300 FID Score: 105.4595\n",
      "Saved improved model at Epoch 2300 with FID 105.4595\n",
      "Epoch [2301/5000] | D Loss: 1.3035 | G Loss: 0.7290\n",
      "Epoch [2302/5000] | D Loss: 1.3312 | G Loss: 0.7623\n",
      "Epoch [2303/5000] | D Loss: 1.4187 | G Loss: 0.7311\n",
      "Epoch [2304/5000] | D Loss: 1.3403 | G Loss: 1.0528\n",
      "Epoch [2305/5000] | D Loss: 1.4105 | G Loss: 0.7125\n",
      "Epoch [2306/5000] | D Loss: 1.5098 | G Loss: 0.7402\n",
      "Epoch [2307/5000] | D Loss: 1.2242 | G Loss: 1.0425\n",
      "Epoch [2308/5000] | D Loss: 1.3092 | G Loss: 0.8214\n",
      "Epoch [2309/5000] | D Loss: 1.5441 | G Loss: 0.9029\n",
      "Epoch [2310/5000] | D Loss: 1.2358 | G Loss: 0.7493\n",
      "Epoch [2311/5000] | D Loss: 1.4511 | G Loss: 0.9981\n",
      "Epoch [2312/5000] | D Loss: 1.2884 | G Loss: 0.6795\n",
      "Epoch [2313/5000] | D Loss: 1.3631 | G Loss: 0.6507\n",
      "Epoch [2314/5000] | D Loss: 1.2642 | G Loss: 0.8497\n",
      "Epoch [2315/5000] | D Loss: 1.3070 | G Loss: 0.6710\n",
      "Epoch [2316/5000] | D Loss: 1.1536 | G Loss: 0.9288\n",
      "Epoch [2317/5000] | D Loss: 1.2875 | G Loss: 0.7746\n",
      "Epoch [2318/5000] | D Loss: 1.4873 | G Loss: 0.8287\n",
      "Epoch [2319/5000] | D Loss: 1.3191 | G Loss: 0.9221\n",
      "Epoch [2320/5000] | D Loss: 1.4609 | G Loss: 0.7161\n",
      "Epoch [2321/5000] | D Loss: 1.3276 | G Loss: 0.8674\n",
      "Epoch [2322/5000] | D Loss: 1.2715 | G Loss: 0.7807\n",
      "Epoch [2323/5000] | D Loss: 0.8269 | G Loss: 1.4117\n",
      "Epoch [2324/5000] | D Loss: 1.3881 | G Loss: 0.8635\n",
      "Epoch [2325/5000] | D Loss: 1.2136 | G Loss: 0.7717\n",
      "Epoch [2326/5000] | D Loss: 1.5045 | G Loss: 0.9523\n",
      "Epoch [2327/5000] | D Loss: 1.3318 | G Loss: 0.8035\n",
      "Epoch [2328/5000] | D Loss: 1.3305 | G Loss: 0.6357\n",
      "Epoch [2329/5000] | D Loss: 1.4703 | G Loss: 0.9191\n",
      "Epoch [2330/5000] | D Loss: 1.3861 | G Loss: 0.7215\n",
      "Epoch [2331/5000] | D Loss: 1.1995 | G Loss: 0.9612\n",
      "Epoch [2332/5000] | D Loss: 1.3373 | G Loss: 0.8692\n",
      "Epoch [2333/5000] | D Loss: 1.2323 | G Loss: 0.7244\n",
      "Epoch [2334/5000] | D Loss: 1.2107 | G Loss: 0.9583\n",
      "Epoch [2335/5000] | D Loss: 1.4322 | G Loss: 0.8772\n",
      "Epoch [2336/5000] | D Loss: 1.4174 | G Loss: 0.6797\n",
      "Epoch [2337/5000] | D Loss: 1.3925 | G Loss: 0.8809\n",
      "Epoch [2338/5000] | D Loss: 1.2159 | G Loss: 0.8741\n",
      "Epoch [2339/5000] | D Loss: 1.3880 | G Loss: 0.8058\n",
      "Epoch [2340/5000] | D Loss: 1.2676 | G Loss: 0.6896\n",
      "Epoch [2341/5000] | D Loss: 1.3105 | G Loss: 0.7063\n",
      "Epoch [2342/5000] | D Loss: 1.2662 | G Loss: 0.6903\n",
      "Epoch [2343/5000] | D Loss: 1.3954 | G Loss: 0.7755\n",
      "Epoch [2344/5000] | D Loss: 1.3306 | G Loss: 0.6614\n",
      "Epoch [2345/5000] | D Loss: 1.3763 | G Loss: 0.6377\n",
      "Epoch [2346/5000] | D Loss: 1.0968 | G Loss: 0.7768\n",
      "Epoch [2347/5000] | D Loss: 1.2114 | G Loss: 1.0552\n",
      "Epoch [2348/5000] | D Loss: 1.3321 | G Loss: 0.8063\n",
      "Epoch [2349/5000] | D Loss: 1.3393 | G Loss: 0.6572\n",
      "Epoch [2350/5000] | D Loss: 1.2877 | G Loss: 0.8101\n",
      "Epoch [2351/5000] | D Loss: 1.4508 | G Loss: 0.6916\n",
      "Epoch [2352/5000] | D Loss: 1.1655 | G Loss: 0.9239\n",
      "Epoch [2353/5000] | D Loss: 1.1842 | G Loss: 0.8078\n",
      "Epoch [2354/5000] | D Loss: 1.3238 | G Loss: 0.7256\n",
      "Epoch [2355/5000] | D Loss: 1.2736 | G Loss: 0.7678\n",
      "Epoch [2356/5000] | D Loss: 1.3057 | G Loss: 0.7211\n",
      "Epoch [2357/5000] | D Loss: 1.0301 | G Loss: 0.9254\n",
      "Epoch [2358/5000] | D Loss: 1.1946 | G Loss: 0.9911\n",
      "Epoch [2359/5000] | D Loss: 1.2739 | G Loss: 0.9854\n",
      "Epoch [2360/5000] | D Loss: 1.3412 | G Loss: 0.7786\n",
      "Epoch [2361/5000] | D Loss: 1.3885 | G Loss: 0.7130\n",
      "Epoch [2362/5000] | D Loss: 1.3958 | G Loss: 0.7864\n",
      "Epoch [2363/5000] | D Loss: 1.2133 | G Loss: 0.9256\n",
      "Epoch [2364/5000] | D Loss: 1.3399 | G Loss: 0.7551\n",
      "Epoch [2365/5000] | D Loss: 1.1837 | G Loss: 0.6539\n",
      "Epoch [2366/5000] | D Loss: 1.2966 | G Loss: 0.8367\n",
      "Epoch [2367/5000] | D Loss: 1.2578 | G Loss: 0.7100\n",
      "Epoch [2368/5000] | D Loss: 1.3189 | G Loss: 0.7831\n",
      "Epoch [2369/5000] | D Loss: 1.5106 | G Loss: 0.6973\n",
      "Epoch [2370/5000] | D Loss: 1.1586 | G Loss: 1.0955\n",
      "Epoch [2371/5000] | D Loss: 1.1088 | G Loss: 0.9010\n",
      "Epoch [2372/5000] | D Loss: 1.3249 | G Loss: 0.6689\n",
      "Epoch [2373/5000] | D Loss: 1.1540 | G Loss: 1.0026\n",
      "Epoch [2374/5000] | D Loss: 1.3397 | G Loss: 0.7464\n",
      "Epoch [2375/5000] | D Loss: 1.4462 | G Loss: 0.7487\n",
      "Epoch [2376/5000] | D Loss: 1.2667 | G Loss: 0.9301\n",
      "Epoch [2377/5000] | D Loss: 1.3615 | G Loss: 0.7579\n",
      "Epoch [2378/5000] | D Loss: 1.4001 | G Loss: 0.6874\n",
      "Epoch [2379/5000] | D Loss: 1.3703 | G Loss: 0.6979\n",
      "Epoch [2380/5000] | D Loss: 1.4438 | G Loss: 0.7900\n",
      "Epoch [2381/5000] | D Loss: 1.4154 | G Loss: 0.7538\n",
      "Epoch [2382/5000] | D Loss: 1.4617 | G Loss: 0.9030\n",
      "Epoch [2383/5000] | D Loss: 1.2226 | G Loss: 0.8072\n",
      "Epoch [2384/5000] | D Loss: 1.2752 | G Loss: 0.8743\n",
      "Epoch [2385/5000] | D Loss: 1.4203 | G Loss: 0.8503\n",
      "Epoch [2386/5000] | D Loss: 1.3571 | G Loss: 0.8210\n",
      "Epoch [2387/5000] | D Loss: 1.2761 | G Loss: 1.0408\n",
      "Epoch [2388/5000] | D Loss: 1.3975 | G Loss: 0.6556\n",
      "Epoch [2389/5000] | D Loss: 1.3707 | G Loss: 0.7783\n",
      "Epoch [2390/5000] | D Loss: 1.4540 | G Loss: 0.7951\n",
      "Epoch [2391/5000] | D Loss: 1.2740 | G Loss: 0.7909\n",
      "Epoch [2392/5000] | D Loss: 1.3459 | G Loss: 0.6848\n",
      "Epoch [2393/5000] | D Loss: 1.3815 | G Loss: 0.7991\n",
      "Epoch [2394/5000] | D Loss: 1.3790 | G Loss: 0.7581\n",
      "Epoch [2395/5000] | D Loss: 1.3711 | G Loss: 0.7174\n",
      "Epoch [2396/5000] | D Loss: 1.3204 | G Loss: 0.6363\n",
      "Epoch [2397/5000] | D Loss: 1.0190 | G Loss: 1.0097\n",
      "Epoch [2398/5000] | D Loss: 1.3287 | G Loss: 0.6326\n",
      "Epoch [2399/5000] | D Loss: 1.1587 | G Loss: 0.8945\n",
      "Epoch [2400/5000] | D Loss: 1.3544 | G Loss: 0.8461\n",
      "Epoch 2400 FID Score: 97.8198\n",
      "Saved improved model at Epoch 2400 with FID 97.8198\n",
      "Epoch [2401/5000] | D Loss: 1.3195 | G Loss: 0.7639\n",
      "Epoch [2402/5000] | D Loss: 1.4345 | G Loss: 0.7389\n",
      "Epoch [2403/5000] | D Loss: 1.3072 | G Loss: 0.7238\n",
      "Epoch [2404/5000] | D Loss: 1.3060 | G Loss: 0.7089\n",
      "Epoch [2405/5000] | D Loss: 1.4196 | G Loss: 0.6784\n",
      "Epoch [2406/5000] | D Loss: 1.4585 | G Loss: 0.6884\n",
      "Epoch [2407/5000] | D Loss: 1.3018 | G Loss: 0.7394\n",
      "Epoch [2408/5000] | D Loss: 1.3158 | G Loss: 0.6994\n",
      "Epoch [2409/5000] | D Loss: 1.0702 | G Loss: 0.8614\n",
      "Epoch [2410/5000] | D Loss: 1.1541 | G Loss: 0.9594\n",
      "Epoch [2411/5000] | D Loss: 1.4385 | G Loss: 0.7354\n",
      "Epoch [2412/5000] | D Loss: 1.3397 | G Loss: 0.7948\n",
      "Epoch [2413/5000] | D Loss: 1.2773 | G Loss: 0.8088\n",
      "Epoch [2414/5000] | D Loss: 1.4063 | G Loss: 0.7774\n",
      "Epoch [2415/5000] | D Loss: 1.3745 | G Loss: 0.7368\n",
      "Epoch [2416/5000] | D Loss: 1.2744 | G Loss: 0.8094\n",
      "Epoch [2417/5000] | D Loss: 1.3259 | G Loss: 0.7923\n",
      "Epoch [2418/5000] | D Loss: 1.2255 | G Loss: 0.7832\n",
      "Epoch [2419/5000] | D Loss: 1.2879 | G Loss: 0.8261\n",
      "Epoch [2420/5000] | D Loss: 1.2784 | G Loss: 0.7784\n",
      "Epoch [2421/5000] | D Loss: 1.4349 | G Loss: 0.8206\n",
      "Epoch [2422/5000] | D Loss: 1.4279 | G Loss: 0.7300\n",
      "Epoch [2423/5000] | D Loss: 1.1979 | G Loss: 0.7379\n",
      "Epoch [2424/5000] | D Loss: 1.2900 | G Loss: 0.7490\n",
      "Epoch [2425/5000] | D Loss: 1.4528 | G Loss: 0.7438\n",
      "Epoch [2426/5000] | D Loss: 1.1681 | G Loss: 0.8214\n",
      "Epoch [2427/5000] | D Loss: 1.2997 | G Loss: 0.6978\n",
      "Epoch [2428/5000] | D Loss: 1.3999 | G Loss: 0.7380\n",
      "Epoch [2429/5000] | D Loss: 1.3013 | G Loss: 0.7456\n",
      "Epoch [2430/5000] | D Loss: 1.2801 | G Loss: 0.7549\n",
      "Epoch [2431/5000] | D Loss: 1.3076 | G Loss: 0.7374\n",
      "Epoch [2432/5000] | D Loss: 1.1186 | G Loss: 0.8334\n",
      "Epoch [2433/5000] | D Loss: 1.3251 | G Loss: 0.7123\n",
      "Epoch [2434/5000] | D Loss: 1.2923 | G Loss: 0.7580\n",
      "Epoch [2435/5000] | D Loss: 1.3724 | G Loss: 0.9109\n",
      "Epoch [2436/5000] | D Loss: 1.3801 | G Loss: 0.7767\n",
      "Epoch [2437/5000] | D Loss: 1.3456 | G Loss: 0.8387\n",
      "Epoch [2438/5000] | D Loss: 1.1972 | G Loss: 0.7844\n",
      "Epoch [2439/5000] | D Loss: 1.2244 | G Loss: 0.7387\n",
      "Epoch [2440/5000] | D Loss: 1.4047 | G Loss: 0.6533\n",
      "Epoch [2441/5000] | D Loss: 1.2692 | G Loss: 0.7522\n",
      "Epoch [2442/5000] | D Loss: 1.3276 | G Loss: 0.7086\n",
      "Epoch [2443/5000] | D Loss: 1.3079 | G Loss: 0.7679\n",
      "Epoch [2444/5000] | D Loss: 1.0937 | G Loss: 0.9557\n",
      "Epoch [2445/5000] | D Loss: 1.3788 | G Loss: 0.7320\n",
      "Epoch [2446/5000] | D Loss: 1.3667 | G Loss: 0.6727\n",
      "Epoch [2447/5000] | D Loss: 1.4113 | G Loss: 0.7515\n",
      "Epoch [2448/5000] | D Loss: 1.2811 | G Loss: 0.7150\n",
      "Epoch [2449/5000] | D Loss: 1.3618 | G Loss: 0.7515\n",
      "Epoch [2450/5000] | D Loss: 1.2556 | G Loss: 0.7431\n",
      "Epoch [2451/5000] | D Loss: 1.4078 | G Loss: 0.6518\n",
      "Epoch [2452/5000] | D Loss: 1.3863 | G Loss: 0.7957\n",
      "Epoch [2453/5000] | D Loss: 1.3799 | G Loss: 0.6646\n",
      "Epoch [2454/5000] | D Loss: 1.3726 | G Loss: 0.6393\n",
      "Epoch [2455/5000] | D Loss: 1.3036 | G Loss: 0.7293\n",
      "Epoch [2456/5000] | D Loss: 1.2847 | G Loss: 0.6888\n",
      "Epoch [2457/5000] | D Loss: 1.2419 | G Loss: 1.0042\n",
      "Epoch [2458/5000] | D Loss: 1.2379 | G Loss: 0.7921\n",
      "Epoch [2459/5000] | D Loss: 1.3971 | G Loss: 0.8334\n",
      "Epoch [2460/5000] | D Loss: 1.3351 | G Loss: 0.6880\n",
      "Epoch [2461/5000] | D Loss: 1.2241 | G Loss: 0.9511\n",
      "Epoch [2462/5000] | D Loss: 1.1770 | G Loss: 0.8489\n",
      "Epoch [2463/5000] | D Loss: 1.3556 | G Loss: 0.8527\n",
      "Epoch [2464/5000] | D Loss: 1.3134 | G Loss: 0.6916\n",
      "Epoch [2465/5000] | D Loss: 1.2923 | G Loss: 0.7172\n",
      "Epoch [2466/5000] | D Loss: 1.3970 | G Loss: 0.7772\n",
      "Epoch [2467/5000] | D Loss: 1.2973 | G Loss: 0.7280\n",
      "Epoch [2468/5000] | D Loss: 1.3382 | G Loss: 0.7876\n",
      "Epoch [2469/5000] | D Loss: 1.2437 | G Loss: 0.8705\n",
      "Epoch [2470/5000] | D Loss: 1.2867 | G Loss: 0.8641\n",
      "Epoch [2471/5000] | D Loss: 1.3137 | G Loss: 0.7523\n",
      "Epoch [2472/5000] | D Loss: 1.2932 | G Loss: 0.7172\n",
      "Epoch [2473/5000] | D Loss: 1.2817 | G Loss: 0.8370\n",
      "Epoch [2474/5000] | D Loss: 1.4025 | G Loss: 0.7441\n",
      "Epoch [2475/5000] | D Loss: 1.1080 | G Loss: 0.9318\n",
      "Epoch [2476/5000] | D Loss: 1.3342 | G Loss: 0.8334\n",
      "Epoch [2477/5000] | D Loss: 1.4015 | G Loss: 0.7402\n",
      "Epoch [2478/5000] | D Loss: 1.4175 | G Loss: 0.7430\n",
      "Epoch [2479/5000] | D Loss: 1.2872 | G Loss: 0.8844\n",
      "Epoch [2480/5000] | D Loss: 1.3280 | G Loss: 0.6792\n",
      "Epoch [2481/5000] | D Loss: 1.4371 | G Loss: 0.7465\n",
      "Epoch [2482/5000] | D Loss: 1.3085 | G Loss: 0.7997\n",
      "Epoch [2483/5000] | D Loss: 1.2209 | G Loss: 0.7568\n",
      "Epoch [2484/5000] | D Loss: 1.3719 | G Loss: 0.6437\n",
      "Epoch [2485/5000] | D Loss: 1.3490 | G Loss: 1.0499\n",
      "Epoch [2486/5000] | D Loss: 1.2535 | G Loss: 0.8341\n",
      "Epoch [2487/5000] | D Loss: 1.2776 | G Loss: 0.7174\n",
      "Epoch [2488/5000] | D Loss: 1.1788 | G Loss: 0.9307\n",
      "Epoch [2489/5000] | D Loss: 1.3396 | G Loss: 1.0030\n",
      "Epoch [2490/5000] | D Loss: 1.2764 | G Loss: 0.7419\n",
      "Epoch [2491/5000] | D Loss: 1.3441 | G Loss: 0.8441\n",
      "Epoch [2492/5000] | D Loss: 1.1890 | G Loss: 0.9069\n",
      "Epoch [2493/5000] | D Loss: 1.4711 | G Loss: 0.7403\n",
      "Epoch [2494/5000] | D Loss: 1.3405 | G Loss: 0.8788\n",
      "Epoch [2495/5000] | D Loss: 1.3867 | G Loss: 0.6403\n",
      "Epoch [2496/5000] | D Loss: 1.3697 | G Loss: 0.8237\n",
      "Epoch [2497/5000] | D Loss: 1.2495 | G Loss: 0.8521\n",
      "Epoch [2498/5000] | D Loss: 1.1258 | G Loss: 0.9707\n",
      "Epoch [2499/5000] | D Loss: 1.2316 | G Loss: 0.9086\n",
      "Epoch [2500/5000] | D Loss: 1.4171 | G Loss: 0.6949\n",
      "Epoch 2500 FID Score: 96.5074\n",
      "Saved improved model at Epoch 2500 with FID 96.5074\n",
      "Epoch [2501/5000] | D Loss: 1.2881 | G Loss: 0.8099\n",
      "Epoch [2502/5000] | D Loss: 1.0290 | G Loss: 0.9751\n",
      "Epoch [2503/5000] | D Loss: 1.3668 | G Loss: 0.7372\n",
      "Epoch [2504/5000] | D Loss: 1.4212 | G Loss: 0.7040\n",
      "Epoch [2505/5000] | D Loss: 1.0743 | G Loss: 1.2557\n",
      "Epoch [2506/5000] | D Loss: 1.3338 | G Loss: 0.7488\n",
      "Epoch [2507/5000] | D Loss: 1.3496 | G Loss: 0.7143\n",
      "Epoch [2508/5000] | D Loss: 1.3180 | G Loss: 0.8078\n",
      "Epoch [2509/5000] | D Loss: 1.2716 | G Loss: 0.8267\n",
      "Epoch [2510/5000] | D Loss: 1.3562 | G Loss: 0.8084\n",
      "Epoch [2511/5000] | D Loss: 1.3614 | G Loss: 0.6909\n",
      "Epoch [2512/5000] | D Loss: 1.4016 | G Loss: 0.7416\n",
      "Epoch [2513/5000] | D Loss: 1.0703 | G Loss: 1.0168\n",
      "Epoch [2514/5000] | D Loss: 1.3939 | G Loss: 0.7234\n",
      "Epoch [2515/5000] | D Loss: 1.4049 | G Loss: 0.7198\n",
      "Epoch [2516/5000] | D Loss: 1.1036 | G Loss: 0.9214\n",
      "Epoch [2517/5000] | D Loss: 1.3150 | G Loss: 0.7297\n",
      "Epoch [2518/5000] | D Loss: 1.2978 | G Loss: 0.8086\n",
      "Epoch [2519/5000] | D Loss: 1.3071 | G Loss: 0.9767\n",
      "Epoch [2520/5000] | D Loss: 1.3591 | G Loss: 0.6900\n",
      "Epoch [2521/5000] | D Loss: 1.2733 | G Loss: 0.8522\n",
      "Epoch [2522/5000] | D Loss: 1.4360 | G Loss: 0.6377\n",
      "Epoch [2523/5000] | D Loss: 1.3337 | G Loss: 0.7728\n",
      "Epoch [2524/5000] | D Loss: 1.2484 | G Loss: 0.8103\n",
      "Epoch [2525/5000] | D Loss: 1.3057 | G Loss: 0.7126\n",
      "Epoch [2526/5000] | D Loss: 1.2666 | G Loss: 0.7516\n",
      "Epoch [2527/5000] | D Loss: 1.2732 | G Loss: 0.8969\n",
      "Epoch [2528/5000] | D Loss: 1.2581 | G Loss: 0.7077\n",
      "Epoch [2529/5000] | D Loss: 1.3164 | G Loss: 0.8229\n",
      "Epoch [2530/5000] | D Loss: 1.2107 | G Loss: 0.8264\n",
      "Epoch [2531/5000] | D Loss: 1.3336 | G Loss: 0.7598\n",
      "Epoch [2532/5000] | D Loss: 1.3623 | G Loss: 0.7224\n",
      "Epoch [2533/5000] | D Loss: 1.3782 | G Loss: 0.7180\n",
      "Epoch [2534/5000] | D Loss: 1.2894 | G Loss: 0.9553\n",
      "Epoch [2535/5000] | D Loss: 1.3178 | G Loss: 0.7275\n",
      "Epoch [2536/5000] | D Loss: 1.3092 | G Loss: 0.8528\n",
      "Epoch [2537/5000] | D Loss: 1.3458 | G Loss: 0.7401\n",
      "Epoch [2538/5000] | D Loss: 1.3807 | G Loss: 0.6641\n",
      "Epoch [2539/5000] | D Loss: 1.3694 | G Loss: 0.7805\n",
      "Epoch [2540/5000] | D Loss: 1.4685 | G Loss: 0.9074\n",
      "Epoch [2541/5000] | D Loss: 1.3554 | G Loss: 0.7703\n",
      "Epoch [2542/5000] | D Loss: 1.2017 | G Loss: 1.0602\n",
      "Epoch [2543/5000] | D Loss: 1.3453 | G Loss: 0.7474\n",
      "Epoch [2544/5000] | D Loss: 1.1634 | G Loss: 0.8948\n",
      "Epoch [2545/5000] | D Loss: 1.3436 | G Loss: 0.8350\n",
      "Epoch [2546/5000] | D Loss: 1.2901 | G Loss: 0.7866\n",
      "Epoch [2547/5000] | D Loss: 1.3590 | G Loss: 0.7980\n",
      "Epoch [2548/5000] | D Loss: 1.2152 | G Loss: 0.7273\n",
      "Epoch [2549/5000] | D Loss: 1.2929 | G Loss: 0.7165\n",
      "Epoch [2550/5000] | D Loss: 1.3681 | G Loss: 0.6749\n",
      "Epoch [2551/5000] | D Loss: 1.3488 | G Loss: 0.7085\n",
      "Epoch [2552/5000] | D Loss: 1.1267 | G Loss: 0.9277\n",
      "Epoch [2553/5000] | D Loss: 1.3669 | G Loss: 0.8046\n",
      "Epoch [2554/5000] | D Loss: 1.3309 | G Loss: 0.7179\n",
      "Epoch [2555/5000] | D Loss: 1.4217 | G Loss: 0.7696\n",
      "Epoch [2556/5000] | D Loss: 1.2666 | G Loss: 0.7143\n",
      "Epoch [2557/5000] | D Loss: 1.3393 | G Loss: 0.8235\n",
      "Epoch [2558/5000] | D Loss: 1.2769 | G Loss: 0.7159\n",
      "Epoch [2559/5000] | D Loss: 1.4081 | G Loss: 0.7994\n",
      "Epoch [2560/5000] | D Loss: 1.4011 | G Loss: 0.6515\n",
      "Epoch [2561/5000] | D Loss: 1.3444 | G Loss: 0.8605\n",
      "Epoch [2562/5000] | D Loss: 1.3259 | G Loss: 0.7596\n",
      "Epoch [2563/5000] | D Loss: 1.3856 | G Loss: 0.7736\n",
      "Epoch [2564/5000] | D Loss: 1.3744 | G Loss: 0.7596\n",
      "Epoch [2565/5000] | D Loss: 1.3677 | G Loss: 0.7841\n",
      "Epoch [2566/5000] | D Loss: 1.2834 | G Loss: 0.7475\n",
      "Epoch [2567/5000] | D Loss: 1.2882 | G Loss: 0.8864\n",
      "Epoch [2568/5000] | D Loss: 1.3850 | G Loss: 0.7707\n",
      "Epoch [2569/5000] | D Loss: 1.2803 | G Loss: 0.8031\n",
      "Epoch [2570/5000] | D Loss: 1.4575 | G Loss: 0.7016\n",
      "Epoch [2571/5000] | D Loss: 1.3708 | G Loss: 0.6738\n",
      "Epoch [2572/5000] | D Loss: 1.3776 | G Loss: 0.8748\n",
      "Epoch [2573/5000] | D Loss: 1.3406 | G Loss: 0.6519\n",
      "Epoch [2574/5000] | D Loss: 1.2158 | G Loss: 0.8129\n",
      "Epoch [2575/5000] | D Loss: 1.3048 | G Loss: 0.7397\n",
      "Epoch [2576/5000] | D Loss: 1.1213 | G Loss: 0.9580\n",
      "Epoch [2577/5000] | D Loss: 1.2409 | G Loss: 0.6928\n",
      "Epoch [2578/5000] | D Loss: 1.2608 | G Loss: 0.7536\n",
      "Epoch [2579/5000] | D Loss: 1.3210 | G Loss: 0.7235\n",
      "Epoch [2580/5000] | D Loss: 1.3666 | G Loss: 0.7331\n",
      "Epoch [2581/5000] | D Loss: 1.2733 | G Loss: 0.7374\n",
      "Epoch [2582/5000] | D Loss: 1.2482 | G Loss: 0.7943\n",
      "Epoch [2583/5000] | D Loss: 1.3738 | G Loss: 0.7283\n",
      "Epoch [2584/5000] | D Loss: 1.2381 | G Loss: 0.7116\n",
      "Epoch [2585/5000] | D Loss: 1.3076 | G Loss: 0.7869\n",
      "Epoch [2586/5000] | D Loss: 1.3665 | G Loss: 0.6612\n",
      "Epoch [2587/5000] | D Loss: 1.3865 | G Loss: 0.7301\n",
      "Epoch [2588/5000] | D Loss: 1.3565 | G Loss: 0.7152\n",
      "Epoch [2589/5000] | D Loss: 1.4925 | G Loss: 0.6588\n",
      "Epoch [2590/5000] | D Loss: 1.2220 | G Loss: 0.8055\n",
      "Epoch [2591/5000] | D Loss: 1.4205 | G Loss: 0.7949\n",
      "Epoch [2592/5000] | D Loss: 1.2873 | G Loss: 0.8499\n",
      "Epoch [2593/5000] | D Loss: 1.2966 | G Loss: 0.8494\n",
      "Epoch [2594/5000] | D Loss: 1.3442 | G Loss: 0.7353\n",
      "Epoch [2595/5000] | D Loss: 1.2689 | G Loss: 0.6265\n",
      "Epoch [2596/5000] | D Loss: 1.3985 | G Loss: 0.7658\n",
      "Epoch [2597/5000] | D Loss: 1.3141 | G Loss: 0.7519\n",
      "Epoch [2598/5000] | D Loss: 1.2934 | G Loss: 0.7602\n",
      "Epoch [2599/5000] | D Loss: 1.3920 | G Loss: 0.8293\n",
      "Epoch [2600/5000] | D Loss: 1.2978 | G Loss: 0.7901\n",
      "Epoch 2600 FID Score: 92.0563\n",
      "Saved improved model at Epoch 2600 with FID 92.0563\n",
      "Epoch [2601/5000] | D Loss: 1.3275 | G Loss: 0.7184\n",
      "Epoch [2602/5000] | D Loss: 1.3246 | G Loss: 0.7781\n",
      "Epoch [2603/5000] | D Loss: 1.3743 | G Loss: 0.7654\n",
      "Epoch [2604/5000] | D Loss: 1.3298 | G Loss: 0.7178\n",
      "Epoch [2605/5000] | D Loss: 1.3163 | G Loss: 0.7630\n",
      "Epoch [2606/5000] | D Loss: 1.4153 | G Loss: 0.8306\n",
      "Epoch [2607/5000] | D Loss: 1.3796 | G Loss: 0.7938\n",
      "Epoch [2608/5000] | D Loss: 1.3974 | G Loss: 0.7777\n",
      "Epoch [2609/5000] | D Loss: 1.3928 | G Loss: 0.7004\n",
      "Epoch [2610/5000] | D Loss: 1.2625 | G Loss: 0.7263\n",
      "Epoch [2611/5000] | D Loss: 1.3273 | G Loss: 0.6656\n",
      "Epoch [2612/5000] | D Loss: 1.3869 | G Loss: 0.7885\n",
      "Epoch [2613/5000] | D Loss: 1.3739 | G Loss: 0.7746\n",
      "Epoch [2614/5000] | D Loss: 1.3143 | G Loss: 0.7362\n",
      "Epoch [2615/5000] | D Loss: 1.2748 | G Loss: 0.7214\n",
      "Epoch [2616/5000] | D Loss: 1.3775 | G Loss: 0.7644\n",
      "Epoch [2617/5000] | D Loss: 1.2853 | G Loss: 0.7290\n",
      "Epoch [2618/5000] | D Loss: 1.3059 | G Loss: 0.7345\n",
      "Epoch [2619/5000] | D Loss: 1.3059 | G Loss: 0.6694\n",
      "Epoch [2620/5000] | D Loss: 1.2386 | G Loss: 0.8581\n",
      "Epoch [2621/5000] | D Loss: 1.2732 | G Loss: 0.7888\n",
      "Epoch [2622/5000] | D Loss: 1.3262 | G Loss: 0.7998\n",
      "Epoch [2623/5000] | D Loss: 1.3903 | G Loss: 0.7115\n",
      "Epoch [2624/5000] | D Loss: 1.3146 | G Loss: 0.6911\n",
      "Epoch [2625/5000] | D Loss: 1.2662 | G Loss: 0.8686\n",
      "Epoch [2626/5000] | D Loss: 1.2328 | G Loss: 0.7580\n",
      "Epoch [2627/5000] | D Loss: 1.3989 | G Loss: 0.6879\n",
      "Epoch [2628/5000] | D Loss: 1.3755 | G Loss: 0.8121\n",
      "Epoch [2629/5000] | D Loss: 1.2498 | G Loss: 0.7756\n",
      "Epoch [2630/5000] | D Loss: 1.3441 | G Loss: 0.6468\n",
      "Epoch [2631/5000] | D Loss: 1.3698 | G Loss: 0.7493\n",
      "Epoch [2632/5000] | D Loss: 1.3416 | G Loss: 0.7640\n",
      "Epoch [2633/5000] | D Loss: 1.4123 | G Loss: 0.7559\n",
      "Epoch [2634/5000] | D Loss: 1.3429 | G Loss: 0.7731\n",
      "Epoch [2635/5000] | D Loss: 1.4093 | G Loss: 0.7251\n",
      "Epoch [2636/5000] | D Loss: 1.3318 | G Loss: 0.7477\n",
      "Epoch [2637/5000] | D Loss: 1.2651 | G Loss: 0.7871\n",
      "Epoch [2638/5000] | D Loss: 1.3546 | G Loss: 0.7598\n",
      "Epoch [2639/5000] | D Loss: 1.0826 | G Loss: 0.8938\n",
      "Epoch [2640/5000] | D Loss: 1.3267 | G Loss: 0.7186\n",
      "Epoch [2641/5000] | D Loss: 1.3505 | G Loss: 0.7993\n",
      "Epoch [2642/5000] | D Loss: 1.4033 | G Loss: 0.8186\n",
      "Epoch [2643/5000] | D Loss: 1.3085 | G Loss: 0.7439\n",
      "Epoch [2644/5000] | D Loss: 1.3497 | G Loss: 0.7339\n",
      "Epoch [2645/5000] | D Loss: 1.3502 | G Loss: 0.7198\n",
      "Epoch [2646/5000] | D Loss: 1.3320 | G Loss: 0.7778\n",
      "Epoch [2647/5000] | D Loss: 1.0078 | G Loss: 1.0183\n",
      "Epoch [2648/5000] | D Loss: 1.3431 | G Loss: 0.7375\n",
      "Epoch [2649/5000] | D Loss: 1.3004 | G Loss: 0.8337\n",
      "Epoch [2650/5000] | D Loss: 1.3784 | G Loss: 0.6897\n",
      "Epoch [2651/5000] | D Loss: 1.3221 | G Loss: 0.7929\n",
      "Epoch [2652/5000] | D Loss: 1.3097 | G Loss: 0.8574\n",
      "Epoch [2653/5000] | D Loss: 1.3362 | G Loss: 0.6995\n",
      "Epoch [2654/5000] | D Loss: 1.3621 | G Loss: 0.7349\n",
      "Epoch [2655/5000] | D Loss: 1.1780 | G Loss: 0.8987\n",
      "Epoch [2656/5000] | D Loss: 1.3671 | G Loss: 0.7414\n",
      "Epoch [2657/5000] | D Loss: 1.3775 | G Loss: 0.8255\n",
      "Epoch [2658/5000] | D Loss: 1.3447 | G Loss: 0.7550\n",
      "Epoch [2659/5000] | D Loss: 1.3149 | G Loss: 0.7111\n",
      "Epoch [2660/5000] | D Loss: 1.2863 | G Loss: 0.7647\n",
      "Epoch [2661/5000] | D Loss: 1.2736 | G Loss: 0.7564\n",
      "Epoch [2662/5000] | D Loss: 1.2817 | G Loss: 0.8394\n",
      "Epoch [2663/5000] | D Loss: 1.4010 | G Loss: 0.7992\n",
      "Epoch [2664/5000] | D Loss: 1.3453 | G Loss: 0.8094\n",
      "Epoch [2665/5000] | D Loss: 1.3670 | G Loss: 0.8983\n",
      "Epoch [2666/5000] | D Loss: 1.2065 | G Loss: 0.8137\n",
      "Epoch [2667/5000] | D Loss: 1.3085 | G Loss: 0.7572\n",
      "Epoch [2668/5000] | D Loss: 1.2555 | G Loss: 0.7035\n",
      "Epoch [2669/5000] | D Loss: 1.4281 | G Loss: 0.6825\n",
      "Epoch [2670/5000] | D Loss: 1.3191 | G Loss: 0.7342\n",
      "Epoch [2671/5000] | D Loss: 1.4069 | G Loss: 0.8242\n",
      "Epoch [2672/5000] | D Loss: 1.3413 | G Loss: 0.7142\n",
      "Epoch [2673/5000] | D Loss: 1.3433 | G Loss: 0.7840\n",
      "Epoch [2674/5000] | D Loss: 1.3090 | G Loss: 0.7592\n",
      "Epoch [2675/5000] | D Loss: 1.3077 | G Loss: 0.8261\n",
      "Epoch [2676/5000] | D Loss: 1.2676 | G Loss: 0.7322\n",
      "Epoch [2677/5000] | D Loss: 1.2988 | G Loss: 0.7047\n",
      "Epoch [2678/5000] | D Loss: 1.3416 | G Loss: 0.8764\n",
      "Epoch [2679/5000] | D Loss: 1.2527 | G Loss: 0.7821\n",
      "Epoch [2680/5000] | D Loss: 1.3485 | G Loss: 0.7640\n",
      "Epoch [2681/5000] | D Loss: 1.2870 | G Loss: 0.7713\n",
      "Epoch [2682/5000] | D Loss: 1.3577 | G Loss: 0.7809\n",
      "Epoch [2683/5000] | D Loss: 1.3353 | G Loss: 0.6794\n",
      "Epoch [2684/5000] | D Loss: 1.3658 | G Loss: 0.7325\n",
      "Epoch [2685/5000] | D Loss: 1.2975 | G Loss: 0.7550\n",
      "Epoch [2686/5000] | D Loss: 1.3400 | G Loss: 0.7711\n",
      "Epoch [2687/5000] | D Loss: 1.3724 | G Loss: 0.6764\n",
      "Epoch [2688/5000] | D Loss: 1.2850 | G Loss: 0.8044\n",
      "Epoch [2689/5000] | D Loss: 1.3375 | G Loss: 0.7776\n",
      "Epoch [2690/5000] | D Loss: 1.3530 | G Loss: 0.7588\n",
      "Epoch [2691/5000] | D Loss: 1.4144 | G Loss: 0.7865\n",
      "Epoch [2692/5000] | D Loss: 1.3317 | G Loss: 0.7246\n",
      "Epoch [2693/5000] | D Loss: 1.3492 | G Loss: 0.7426\n",
      "Epoch [2694/5000] | D Loss: 1.3063 | G Loss: 0.7545\n",
      "Epoch [2695/5000] | D Loss: 1.2984 | G Loss: 0.7985\n",
      "Epoch [2696/5000] | D Loss: 1.2481 | G Loss: 0.7190\n",
      "Epoch [2697/5000] | D Loss: 1.3049 | G Loss: 0.6840\n",
      "Epoch [2698/5000] | D Loss: 1.3316 | G Loss: 0.7751\n",
      "Epoch [2699/5000] | D Loss: 1.4078 | G Loss: 0.6990\n",
      "Epoch [2700/5000] | D Loss: 1.3445 | G Loss: 0.7328\n",
      "Epoch 2700 FID Score: 92.1612\n",
      "Epoch [2701/5000] | D Loss: 1.3799 | G Loss: 0.6296\n",
      "Epoch [2702/5000] | D Loss: 1.3880 | G Loss: 0.6895\n",
      "Epoch [2703/5000] | D Loss: 1.4413 | G Loss: 0.7585\n",
      "Epoch [2704/5000] | D Loss: 1.3344 | G Loss: 0.7328\n",
      "Epoch [2705/5000] | D Loss: 1.4218 | G Loss: 0.7429\n",
      "Epoch [2706/5000] | D Loss: 1.2868 | G Loss: 0.7400\n",
      "Epoch [2707/5000] | D Loss: 1.4159 | G Loss: 0.6314\n",
      "Epoch [2708/5000] | D Loss: 1.3461 | G Loss: 0.7513\n",
      "Epoch [2709/5000] | D Loss: 1.3094 | G Loss: 0.7819\n",
      "Epoch [2710/5000] | D Loss: 1.2562 | G Loss: 0.7600\n",
      "Epoch [2711/5000] | D Loss: 1.3210 | G Loss: 0.7528\n",
      "Epoch [2712/5000] | D Loss: 1.3609 | G Loss: 0.7102\n",
      "Epoch [2713/5000] | D Loss: 1.3064 | G Loss: 0.7239\n",
      "Epoch [2714/5000] | D Loss: 1.4012 | G Loss: 0.7299\n",
      "Epoch [2715/5000] | D Loss: 1.2947 | G Loss: 0.7390\n",
      "Epoch [2716/5000] | D Loss: 1.2916 | G Loss: 0.6726\n",
      "Epoch [2717/5000] | D Loss: 1.2928 | G Loss: 0.6796\n",
      "Epoch [2718/5000] | D Loss: 1.4151 | G Loss: 0.7313\n",
      "Epoch [2719/5000] | D Loss: 1.2704 | G Loss: 0.6447\n",
      "Epoch [2720/5000] | D Loss: 1.4063 | G Loss: 0.7868\n",
      "Epoch [2721/5000] | D Loss: 1.3308 | G Loss: 0.7333\n",
      "Epoch [2722/5000] | D Loss: 1.3587 | G Loss: 0.7708\n",
      "Epoch [2723/5000] | D Loss: 1.4013 | G Loss: 0.7241\n",
      "Epoch [2724/5000] | D Loss: 1.3491 | G Loss: 0.7313\n",
      "Epoch [2725/5000] | D Loss: 1.3314 | G Loss: 0.7538\n",
      "Epoch [2726/5000] | D Loss: 1.3188 | G Loss: 0.6876\n",
      "Epoch [2727/5000] | D Loss: 1.3534 | G Loss: 0.7202\n",
      "Epoch [2728/5000] | D Loss: 1.3260 | G Loss: 0.7174\n",
      "Epoch [2729/5000] | D Loss: 1.4438 | G Loss: 0.8634\n",
      "Epoch [2730/5000] | D Loss: 1.2984 | G Loss: 0.7211\n",
      "Epoch [2731/5000] | D Loss: 1.3691 | G Loss: 0.7197\n",
      "Epoch [2732/5000] | D Loss: 1.1481 | G Loss: 0.8785\n",
      "Epoch [2733/5000] | D Loss: 1.3489 | G Loss: 0.7253\n",
      "Epoch [2734/5000] | D Loss: 1.1791 | G Loss: 0.8664\n",
      "Epoch [2735/5000] | D Loss: 1.3919 | G Loss: 0.7679\n",
      "Epoch [2736/5000] | D Loss: 1.3091 | G Loss: 0.8126\n",
      "Epoch [2737/5000] | D Loss: 1.2934 | G Loss: 0.8401\n",
      "Epoch [2738/5000] | D Loss: 1.2861 | G Loss: 0.6648\n",
      "Epoch [2739/5000] | D Loss: 1.3289 | G Loss: 0.8211\n",
      "Epoch [2740/5000] | D Loss: 1.3829 | G Loss: 0.6847\n",
      "Epoch [2741/5000] | D Loss: 1.3103 | G Loss: 0.7110\n",
      "Epoch [2742/5000] | D Loss: 1.2673 | G Loss: 0.6551\n",
      "Epoch [2743/5000] | D Loss: 1.3635 | G Loss: 0.7803\n",
      "Epoch [2744/5000] | D Loss: 1.4164 | G Loss: 0.7702\n",
      "Epoch [2745/5000] | D Loss: 1.3780 | G Loss: 0.7295\n",
      "Epoch [2746/5000] | D Loss: 1.2915 | G Loss: 0.8107\n",
      "Epoch [2747/5000] | D Loss: 1.2989 | G Loss: 0.7187\n",
      "Epoch [2748/5000] | D Loss: 1.3016 | G Loss: 0.6795\n",
      "Epoch [2749/5000] | D Loss: 1.3043 | G Loss: 0.6934\n",
      "Epoch [2750/5000] | D Loss: 1.3359 | G Loss: 0.7270\n",
      "Epoch [2751/5000] | D Loss: 1.3527 | G Loss: 0.7575\n",
      "Epoch [2752/5000] | D Loss: 1.2662 | G Loss: 0.7111\n",
      "Epoch [2753/5000] | D Loss: 1.3143 | G Loss: 0.7508\n",
      "Epoch [2754/5000] | D Loss: 1.3797 | G Loss: 0.7189\n",
      "Epoch [2755/5000] | D Loss: 1.3188 | G Loss: 0.7185\n",
      "Epoch [2756/5000] | D Loss: 1.3486 | G Loss: 0.6680\n",
      "Epoch [2757/5000] | D Loss: 1.3023 | G Loss: 0.7550\n",
      "Epoch [2758/5000] | D Loss: 1.3862 | G Loss: 0.7350\n",
      "Epoch [2759/5000] | D Loss: 1.3843 | G Loss: 0.7051\n",
      "Epoch [2760/5000] | D Loss: 1.3389 | G Loss: 0.7601\n",
      "Epoch [2761/5000] | D Loss: 1.3935 | G Loss: 0.6724\n",
      "Epoch [2762/5000] | D Loss: 1.3501 | G Loss: 0.7390\n",
      "Epoch [2763/5000] | D Loss: 1.3453 | G Loss: 0.7115\n",
      "Epoch [2764/5000] | D Loss: 1.3966 | G Loss: 0.7709\n",
      "Epoch [2765/5000] | D Loss: 1.3962 | G Loss: 0.7763\n",
      "Epoch [2766/5000] | D Loss: 1.3928 | G Loss: 0.6914\n",
      "Epoch [2767/5000] | D Loss: 1.2928 | G Loss: 0.7947\n",
      "Epoch [2768/5000] | D Loss: 1.3507 | G Loss: 0.7186\n",
      "Epoch [2769/5000] | D Loss: 1.2316 | G Loss: 0.8675\n",
      "Epoch [2770/5000] | D Loss: 1.3809 | G Loss: 0.7707\n",
      "Epoch [2771/5000] | D Loss: 1.3566 | G Loss: 0.6974\n",
      "Epoch [2772/5000] | D Loss: 1.3934 | G Loss: 0.7192\n",
      "Epoch [2773/5000] | D Loss: 1.3183 | G Loss: 0.7303\n",
      "Epoch [2774/5000] | D Loss: 1.2329 | G Loss: 0.7674\n",
      "Epoch [2775/5000] | D Loss: 1.2638 | G Loss: 0.8120\n",
      "Epoch [2776/5000] | D Loss: 1.2595 | G Loss: 0.8286\n",
      "Epoch [2777/5000] | D Loss: 1.2514 | G Loss: 0.7246\n",
      "Epoch [2778/5000] | D Loss: 1.3697 | G Loss: 0.6783\n",
      "Epoch [2779/5000] | D Loss: 1.3004 | G Loss: 0.7373\n",
      "Epoch [2780/5000] | D Loss: 1.2716 | G Loss: 0.7482\n",
      "Epoch [2781/5000] | D Loss: 1.3267 | G Loss: 0.7314\n",
      "Epoch [2782/5000] | D Loss: 1.3430 | G Loss: 0.7347\n",
      "Epoch [2783/5000] | D Loss: 1.2481 | G Loss: 0.7512\n",
      "Epoch [2784/5000] | D Loss: 1.3631 | G Loss: 0.7487\n",
      "Epoch [2785/5000] | D Loss: 1.3171 | G Loss: 0.7319\n",
      "Epoch [2786/5000] | D Loss: 1.3457 | G Loss: 0.6944\n",
      "Epoch [2787/5000] | D Loss: 1.2583 | G Loss: 0.7282\n",
      "Epoch [2788/5000] | D Loss: 1.3830 | G Loss: 0.7141\n",
      "Epoch [2789/5000] | D Loss: 1.3212 | G Loss: 0.7171\n",
      "Epoch [2790/5000] | D Loss: 1.3667 | G Loss: 0.7432\n",
      "Epoch [2791/5000] | D Loss: 1.4032 | G Loss: 0.6955\n",
      "Epoch [2792/5000] | D Loss: 1.3920 | G Loss: 0.6666\n",
      "Epoch [2793/5000] | D Loss: 1.2900 | G Loss: 0.7382\n",
      "Epoch [2794/5000] | D Loss: 1.3515 | G Loss: 0.7608\n",
      "Epoch [2795/5000] | D Loss: 1.4261 | G Loss: 0.7540\n",
      "Epoch [2796/5000] | D Loss: 1.3018 | G Loss: 0.7598\n",
      "Epoch [2797/5000] | D Loss: 1.3580 | G Loss: 0.7789\n",
      "Epoch [2798/5000] | D Loss: 1.3397 | G Loss: 0.7366\n",
      "Epoch [2799/5000] | D Loss: 1.2478 | G Loss: 0.8189\n",
      "Epoch [2800/5000] | D Loss: 1.2553 | G Loss: 0.8075\n",
      "Epoch 2800 FID Score: 86.2715\n",
      "Saved improved model at Epoch 2800 with FID 86.2715\n",
      "Epoch [2801/5000] | D Loss: 1.2847 | G Loss: 0.7130\n",
      "Epoch [2802/5000] | D Loss: 1.3278 | G Loss: 0.7638\n",
      "Epoch [2803/5000] | D Loss: 1.2657 | G Loss: 0.7905\n",
      "Epoch [2804/5000] | D Loss: 1.3086 | G Loss: 0.7685\n",
      "Epoch [2805/5000] | D Loss: 1.3196 | G Loss: 0.7912\n",
      "Epoch [2806/5000] | D Loss: 1.2511 | G Loss: 0.7319\n",
      "Epoch [2807/5000] | D Loss: 1.3039 | G Loss: 0.8061\n",
      "Epoch [2808/5000] | D Loss: 1.3188 | G Loss: 0.6903\n",
      "Epoch [2809/5000] | D Loss: 1.3604 | G Loss: 0.7006\n",
      "Epoch [2810/5000] | D Loss: 1.4039 | G Loss: 0.7499\n",
      "Epoch [2811/5000] | D Loss: 1.3528 | G Loss: 0.8065\n",
      "Epoch [2812/5000] | D Loss: 1.3990 | G Loss: 0.7254\n",
      "Epoch [2813/5000] | D Loss: 1.3898 | G Loss: 0.7370\n",
      "Epoch [2814/5000] | D Loss: 1.3394 | G Loss: 0.7394\n",
      "Epoch [2815/5000] | D Loss: 1.3702 | G Loss: 0.7312\n",
      "Epoch [2816/5000] | D Loss: 1.3189 | G Loss: 0.7393\n",
      "Epoch [2817/5000] | D Loss: 1.3775 | G Loss: 0.7507\n",
      "Epoch [2818/5000] | D Loss: 1.3562 | G Loss: 0.7826\n",
      "Epoch [2819/5000] | D Loss: 1.3138 | G Loss: 0.7891\n",
      "Epoch [2820/5000] | D Loss: 1.3790 | G Loss: 0.7663\n",
      "Epoch [2821/5000] | D Loss: 1.3621 | G Loss: 0.6698\n",
      "Epoch [2822/5000] | D Loss: 1.3882 | G Loss: 0.6948\n",
      "Epoch [2823/5000] | D Loss: 1.3591 | G Loss: 0.8083\n",
      "Epoch [2824/5000] | D Loss: 1.3699 | G Loss: 0.7785\n",
      "Epoch [2825/5000] | D Loss: 1.3300 | G Loss: 0.7387\n",
      "Epoch [2826/5000] | D Loss: 1.3535 | G Loss: 0.7111\n",
      "Epoch [2827/5000] | D Loss: 1.3291 | G Loss: 0.7342\n",
      "Epoch [2828/5000] | D Loss: 1.2692 | G Loss: 0.7592\n",
      "Epoch [2829/5000] | D Loss: 1.3287 | G Loss: 0.8079\n",
      "Epoch [2830/5000] | D Loss: 1.4092 | G Loss: 0.6881\n",
      "Epoch [2831/5000] | D Loss: 1.3293 | G Loss: 0.7324\n",
      "Epoch [2832/5000] | D Loss: 1.2963 | G Loss: 0.7770\n",
      "Epoch [2833/5000] | D Loss: 1.3398 | G Loss: 0.7227\n",
      "Epoch [2834/5000] | D Loss: 1.2897 | G Loss: 0.7086\n",
      "Epoch [2835/5000] | D Loss: 1.4114 | G Loss: 0.7254\n",
      "Epoch [2836/5000] | D Loss: 1.3469 | G Loss: 0.7566\n",
      "Epoch [2837/5000] | D Loss: 1.3296 | G Loss: 0.7117\n",
      "Epoch [2838/5000] | D Loss: 1.4341 | G Loss: 0.6716\n",
      "Epoch [2839/5000] | D Loss: 1.2861 | G Loss: 0.6850\n",
      "Epoch [2840/5000] | D Loss: 1.2893 | G Loss: 0.7594\n",
      "Epoch [2841/5000] | D Loss: 1.3372 | G Loss: 0.8112\n",
      "Epoch [2842/5000] | D Loss: 1.4098 | G Loss: 0.6499\n",
      "Epoch [2843/5000] | D Loss: 1.2825 | G Loss: 0.7653\n",
      "Epoch [2844/5000] | D Loss: 1.2515 | G Loss: 0.8059\n",
      "Epoch [2845/5000] | D Loss: 1.3558 | G Loss: 0.7671\n",
      "Epoch [2846/5000] | D Loss: 1.3303 | G Loss: 0.7274\n",
      "Epoch [2847/5000] | D Loss: 1.3515 | G Loss: 0.7355\n",
      "Epoch [2848/5000] | D Loss: 1.3385 | G Loss: 0.7400\n",
      "Epoch [2849/5000] | D Loss: 1.2899 | G Loss: 0.8194\n",
      "Epoch [2850/5000] | D Loss: 1.3992 | G Loss: 0.7505\n",
      "Epoch [2851/5000] | D Loss: 1.3685 | G Loss: 0.7696\n",
      "Epoch [2852/5000] | D Loss: 1.2700 | G Loss: 0.7099\n",
      "Epoch [2853/5000] | D Loss: 1.3836 | G Loss: 0.6440\n",
      "Epoch [2854/5000] | D Loss: 1.2885 | G Loss: 0.7986\n",
      "Epoch [2855/5000] | D Loss: 1.3980 | G Loss: 0.7296\n",
      "Epoch [2856/5000] | D Loss: 1.3267 | G Loss: 0.7437\n",
      "Epoch [2857/5000] | D Loss: 1.4003 | G Loss: 0.7369\n",
      "Epoch [2858/5000] | D Loss: 1.3186 | G Loss: 0.7504\n",
      "Epoch [2859/5000] | D Loss: 1.3184 | G Loss: 0.6880\n",
      "Epoch [2860/5000] | D Loss: 1.3790 | G Loss: 0.7457\n",
      "Epoch [2861/5000] | D Loss: 1.3557 | G Loss: 0.7370\n",
      "Epoch [2862/5000] | D Loss: 1.3178 | G Loss: 0.7923\n",
      "Epoch [2863/5000] | D Loss: 1.3840 | G Loss: 0.7460\n",
      "Epoch [2864/5000] | D Loss: 1.3391 | G Loss: 0.6639\n",
      "Epoch [2865/5000] | D Loss: 1.3499 | G Loss: 0.7182\n",
      "Epoch [2866/5000] | D Loss: 1.3661 | G Loss: 0.7042\n",
      "Epoch [2867/5000] | D Loss: 1.3846 | G Loss: 0.7033\n",
      "Epoch [2868/5000] | D Loss: 1.3723 | G Loss: 0.7229\n",
      "Epoch [2869/5000] | D Loss: 1.3238 | G Loss: 0.7405\n",
      "Epoch [2870/5000] | D Loss: 1.1864 | G Loss: 0.8539\n",
      "Epoch [2871/5000] | D Loss: 1.4102 | G Loss: 0.7004\n",
      "Epoch [2872/5000] | D Loss: 1.3049 | G Loss: 0.7449\n",
      "Epoch [2873/5000] | D Loss: 1.3670 | G Loss: 0.6873\n",
      "Epoch [2874/5000] | D Loss: 1.3028 | G Loss: 0.8029\n",
      "Epoch [2875/5000] | D Loss: 1.4146 | G Loss: 0.7016\n",
      "Epoch [2876/5000] | D Loss: 1.2623 | G Loss: 0.7105\n",
      "Epoch [2877/5000] | D Loss: 1.2855 | G Loss: 0.6362\n",
      "Epoch [2878/5000] | D Loss: 1.2515 | G Loss: 0.7629\n",
      "Epoch [2879/5000] | D Loss: 1.3589 | G Loss: 0.7082\n",
      "Epoch [2880/5000] | D Loss: 1.3743 | G Loss: 0.7115\n",
      "Epoch [2881/5000] | D Loss: 1.3705 | G Loss: 0.7494\n",
      "Epoch [2882/5000] | D Loss: 1.3155 | G Loss: 0.7283\n",
      "Epoch [2883/5000] | D Loss: 1.3446 | G Loss: 0.7657\n",
      "Epoch [2884/5000] | D Loss: 1.3666 | G Loss: 0.8135\n",
      "Epoch [2885/5000] | D Loss: 1.3409 | G Loss: 0.7219\n",
      "Epoch [2886/5000] | D Loss: 1.3003 | G Loss: 0.7208\n",
      "Epoch [2887/5000] | D Loss: 1.3258 | G Loss: 0.7774\n",
      "Epoch [2888/5000] | D Loss: 1.3181 | G Loss: 0.7737\n",
      "Epoch [2889/5000] | D Loss: 1.3731 | G Loss: 0.7074\n",
      "Epoch [2890/5000] | D Loss: 1.3630 | G Loss: 0.7543\n",
      "Epoch [2891/5000] | D Loss: 1.3320 | G Loss: 0.7208\n",
      "Epoch [2892/5000] | D Loss: 1.3338 | G Loss: 0.7440\n",
      "Epoch [2893/5000] | D Loss: 1.3266 | G Loss: 0.7015\n",
      "Epoch [2894/5000] | D Loss: 1.3171 | G Loss: 0.7353\n",
      "Epoch [2895/5000] | D Loss: 1.3369 | G Loss: 0.7541\n",
      "Epoch [2896/5000] | D Loss: 1.2935 | G Loss: 0.7237\n",
      "Epoch [2897/5000] | D Loss: 1.3477 | G Loss: 0.7058\n",
      "Epoch [2898/5000] | D Loss: 1.3638 | G Loss: 0.7200\n",
      "Epoch [2899/5000] | D Loss: 1.3519 | G Loss: 0.7066\n",
      "Epoch [2900/5000] | D Loss: 1.3659 | G Loss: 0.7388\n",
      "Epoch 2900 FID Score: 82.1623\n",
      "Saved improved model at Epoch 2900 with FID 82.1623\n",
      "Epoch [2901/5000] | D Loss: 1.3495 | G Loss: 0.6773\n",
      "Epoch [2902/5000] | D Loss: 1.3279 | G Loss: 0.6902\n",
      "Epoch [2903/5000] | D Loss: 1.3619 | G Loss: 0.7784\n",
      "Epoch [2904/5000] | D Loss: 1.3024 | G Loss: 0.6841\n",
      "Epoch [2905/5000] | D Loss: 1.3320 | G Loss: 0.7637\n",
      "Epoch [2906/5000] | D Loss: 1.3366 | G Loss: 0.7742\n",
      "Epoch [2907/5000] | D Loss: 1.3783 | G Loss: 0.6937\n",
      "Epoch [2908/5000] | D Loss: 1.3630 | G Loss: 0.6889\n",
      "Epoch [2909/5000] | D Loss: 1.3614 | G Loss: 0.7224\n",
      "Epoch [2910/5000] | D Loss: 1.3089 | G Loss: 0.7396\n",
      "Epoch [2911/5000] | D Loss: 1.3695 | G Loss: 0.7724\n",
      "Epoch [2912/5000] | D Loss: 1.3258 | G Loss: 0.7142\n",
      "Epoch [2913/5000] | D Loss: 1.3298 | G Loss: 0.6943\n",
      "Epoch [2914/5000] | D Loss: 1.3316 | G Loss: 0.6893\n",
      "Epoch [2915/5000] | D Loss: 1.3218 | G Loss: 0.6890\n",
      "Epoch [2916/5000] | D Loss: 1.3440 | G Loss: 0.6986\n",
      "Epoch [2917/5000] | D Loss: 1.3834 | G Loss: 0.7472\n",
      "Epoch [2918/5000] | D Loss: 1.3130 | G Loss: 0.7406\n",
      "Epoch [2919/5000] | D Loss: 1.3832 | G Loss: 0.6867\n",
      "Epoch [2920/5000] | D Loss: 1.3254 | G Loss: 0.7232\n",
      "Epoch [2921/5000] | D Loss: 1.3912 | G Loss: 0.6566\n",
      "Epoch [2922/5000] | D Loss: 1.3084 | G Loss: 0.7362\n",
      "Epoch [2923/5000] | D Loss: 1.3452 | G Loss: 0.7758\n",
      "Epoch [2924/5000] | D Loss: 1.3872 | G Loss: 0.7260\n",
      "Epoch [2925/5000] | D Loss: 1.3076 | G Loss: 0.7156\n",
      "Epoch [2926/5000] | D Loss: 1.3175 | G Loss: 0.6833\n",
      "Epoch [2927/5000] | D Loss: 1.3363 | G Loss: 0.6969\n",
      "Epoch [2928/5000] | D Loss: 1.3831 | G Loss: 0.6726\n",
      "Epoch [2929/5000] | D Loss: 1.2950 | G Loss: 0.6621\n",
      "Epoch [2930/5000] | D Loss: 1.3485 | G Loss: 0.7382\n",
      "Epoch [2931/5000] | D Loss: 1.2797 | G Loss: 0.6916\n",
      "Epoch [2932/5000] | D Loss: 1.3772 | G Loss: 0.7043\n",
      "Epoch [2933/5000] | D Loss: 1.2608 | G Loss: 0.8234\n",
      "Epoch [2934/5000] | D Loss: 1.3588 | G Loss: 0.7602\n",
      "Epoch [2935/5000] | D Loss: 1.3408 | G Loss: 0.7461\n",
      "Epoch [2936/5000] | D Loss: 1.3351 | G Loss: 0.7564\n",
      "Epoch [2937/5000] | D Loss: 1.3211 | G Loss: 0.7578\n",
      "Epoch [2938/5000] | D Loss: 1.3150 | G Loss: 0.7705\n",
      "Epoch [2939/5000] | D Loss: 1.2841 | G Loss: 0.7579\n",
      "Epoch [2940/5000] | D Loss: 1.3285 | G Loss: 0.7059\n",
      "Epoch [2941/5000] | D Loss: 1.3320 | G Loss: 0.7187\n",
      "Epoch [2942/5000] | D Loss: 1.3206 | G Loss: 0.6860\n",
      "Epoch [2943/5000] | D Loss: 1.2952 | G Loss: 0.7431\n",
      "Epoch [2944/5000] | D Loss: 1.3749 | G Loss: 0.7460\n",
      "Epoch [2945/5000] | D Loss: 1.3280 | G Loss: 0.6231\n",
      "Epoch [2946/5000] | D Loss: 1.3308 | G Loss: 0.7001\n",
      "Epoch [2947/5000] | D Loss: 1.3783 | G Loss: 0.7557\n",
      "Epoch [2948/5000] | D Loss: 1.3561 | G Loss: 0.6944\n",
      "Epoch [2949/5000] | D Loss: 1.1848 | G Loss: 0.8428\n",
      "Epoch [2950/5000] | D Loss: 1.3583 | G Loss: 0.6787\n",
      "Epoch [2951/5000] | D Loss: 1.3945 | G Loss: 0.6856\n",
      "Epoch [2952/5000] | D Loss: 1.3762 | G Loss: 0.7166\n",
      "Epoch [2953/5000] | D Loss: 1.3381 | G Loss: 0.8225\n",
      "Epoch [2954/5000] | D Loss: 1.3476 | G Loss: 0.7040\n",
      "Epoch [2955/5000] | D Loss: 1.3621 | G Loss: 0.7421\n",
      "Epoch [2956/5000] | D Loss: 1.3605 | G Loss: 0.7768\n",
      "Epoch [2957/5000] | D Loss: 1.3678 | G Loss: 0.7411\n",
      "Epoch [2958/5000] | D Loss: 1.3508 | G Loss: 0.6821\n",
      "Epoch [2959/5000] | D Loss: 1.2795 | G Loss: 0.7433\n",
      "Epoch [2960/5000] | D Loss: 1.3039 | G Loss: 0.7866\n",
      "Epoch [2961/5000] | D Loss: 1.3648 | G Loss: 0.7371\n",
      "Epoch [2962/5000] | D Loss: 1.3326 | G Loss: 0.7331\n",
      "Epoch [2963/5000] | D Loss: 1.3785 | G Loss: 0.7006\n",
      "Epoch [2964/5000] | D Loss: 1.3091 | G Loss: 0.7403\n",
      "Epoch [2965/5000] | D Loss: 1.3377 | G Loss: 0.7785\n",
      "Epoch [2966/5000] | D Loss: 1.3445 | G Loss: 0.7268\n",
      "Epoch [2967/5000] | D Loss: 1.3960 | G Loss: 0.6888\n",
      "Epoch [2968/5000] | D Loss: 1.3439 | G Loss: 0.7899\n",
      "Epoch [2969/5000] | D Loss: 1.3628 | G Loss: 0.7021\n",
      "Epoch [2970/5000] | D Loss: 1.3171 | G Loss: 0.7404\n",
      "Epoch [2971/5000] | D Loss: 1.4157 | G Loss: 0.6693\n",
      "Epoch [2972/5000] | D Loss: 1.2968 | G Loss: 0.7264\n",
      "Epoch [2973/5000] | D Loss: 1.3500 | G Loss: 0.7636\n",
      "Epoch [2974/5000] | D Loss: 1.3186 | G Loss: 0.7683\n",
      "Epoch [2975/5000] | D Loss: 1.3225 | G Loss: 0.7756\n",
      "Epoch [2976/5000] | D Loss: 1.3310 | G Loss: 0.7506\n",
      "Epoch [2977/5000] | D Loss: 1.3611 | G Loss: 0.7084\n",
      "Epoch [2978/5000] | D Loss: 1.2892 | G Loss: 0.7510\n",
      "Epoch [2979/5000] | D Loss: 1.3978 | G Loss: 0.7427\n",
      "Epoch [2980/5000] | D Loss: 1.3779 | G Loss: 0.7946\n",
      "Epoch [2981/5000] | D Loss: 1.3301 | G Loss: 0.7087\n",
      "Epoch [2982/5000] | D Loss: 1.2849 | G Loss: 0.7413\n",
      "Epoch [2983/5000] | D Loss: 1.3615 | G Loss: 0.7463\n",
      "Epoch [2984/5000] | D Loss: 1.3636 | G Loss: 0.7739\n",
      "Epoch [2985/5000] | D Loss: 1.3637 | G Loss: 0.7374\n",
      "Epoch [2986/5000] | D Loss: 1.3950 | G Loss: 0.6772\n",
      "Epoch [2987/5000] | D Loss: 1.4045 | G Loss: 0.7716\n",
      "Epoch [2988/5000] | D Loss: 1.3628 | G Loss: 0.6648\n",
      "Epoch [2989/5000] | D Loss: 1.3038 | G Loss: 0.7757\n",
      "Epoch [2990/5000] | D Loss: 1.3801 | G Loss: 0.7412\n",
      "Epoch [2991/5000] | D Loss: 1.3817 | G Loss: 0.7119\n",
      "Epoch [2992/5000] | D Loss: 1.3126 | G Loss: 0.7057\n",
      "Epoch [2993/5000] | D Loss: 1.3468 | G Loss: 0.7193\n",
      "Epoch [2994/5000] | D Loss: 1.3360 | G Loss: 0.7576\n",
      "Epoch [2995/5000] | D Loss: 1.3745 | G Loss: 0.7300\n",
      "Epoch [2996/5000] | D Loss: 1.3302 | G Loss: 0.6887\n",
      "Epoch [2997/5000] | D Loss: 1.4493 | G Loss: 0.7144\n",
      "Epoch [2998/5000] | D Loss: 1.3494 | G Loss: 0.7271\n",
      "Epoch [2999/5000] | D Loss: 1.3374 | G Loss: 0.8125\n",
      "Epoch [3000/5000] | D Loss: 1.3421 | G Loss: 0.7104\n",
      "Epoch 3000 FID Score: 80.1236\n",
      "Saved improved model at Epoch 3000 with FID 80.1236\n",
      "Epoch [3001/5000] | D Loss: 1.4028 | G Loss: 0.7284\n",
      "Epoch [3002/5000] | D Loss: 1.2955 | G Loss: 0.7577\n",
      "Epoch [3003/5000] | D Loss: 1.4027 | G Loss: 0.7277\n",
      "Epoch [3004/5000] | D Loss: 1.3962 | G Loss: 0.6763\n",
      "Epoch [3005/5000] | D Loss: 1.3740 | G Loss: 0.7755\n",
      "Epoch [3006/5000] | D Loss: 1.3578 | G Loss: 0.7053\n",
      "Epoch [3007/5000] | D Loss: 1.3766 | G Loss: 0.7255\n",
      "Epoch [3008/5000] | D Loss: 1.3073 | G Loss: 0.7691\n",
      "Epoch [3009/5000] | D Loss: 1.3697 | G Loss: 0.7781\n",
      "Epoch [3010/5000] | D Loss: 1.3962 | G Loss: 0.7076\n",
      "Epoch [3011/5000] | D Loss: 1.2577 | G Loss: 0.7912\n",
      "Epoch [3012/5000] | D Loss: 1.3383 | G Loss: 0.7035\n",
      "Epoch [3013/5000] | D Loss: 1.3041 | G Loss: 0.7285\n",
      "Epoch [3014/5000] | D Loss: 1.3817 | G Loss: 0.7506\n",
      "Epoch [3015/5000] | D Loss: 1.3721 | G Loss: 0.7445\n",
      "Epoch [3016/5000] | D Loss: 1.3616 | G Loss: 0.7301\n",
      "Epoch [3017/5000] | D Loss: 1.3563 | G Loss: 0.7600\n",
      "Epoch [3018/5000] | D Loss: 1.2988 | G Loss: 0.6953\n",
      "Epoch [3019/5000] | D Loss: 1.3232 | G Loss: 0.7318\n",
      "Epoch [3020/5000] | D Loss: 1.3331 | G Loss: 0.6999\n",
      "Epoch [3021/5000] | D Loss: 1.2945 | G Loss: 0.8355\n",
      "Epoch [3022/5000] | D Loss: 1.3228 | G Loss: 0.7113\n",
      "Epoch [3023/5000] | D Loss: 1.3857 | G Loss: 0.7292\n",
      "Epoch [3024/5000] | D Loss: 1.3814 | G Loss: 0.7388\n",
      "Epoch [3025/5000] | D Loss: 1.3714 | G Loss: 0.7075\n",
      "Epoch [3026/5000] | D Loss: 1.3238 | G Loss: 0.7846\n",
      "Epoch [3027/5000] | D Loss: 1.3432 | G Loss: 0.7179\n",
      "Epoch [3028/5000] | D Loss: 1.3158 | G Loss: 0.6792\n",
      "Epoch [3029/5000] | D Loss: 1.3344 | G Loss: 0.7118\n",
      "Epoch [3030/5000] | D Loss: 1.3266 | G Loss: 0.7345\n",
      "Epoch [3031/5000] | D Loss: 1.4176 | G Loss: 0.7135\n",
      "Epoch [3032/5000] | D Loss: 1.3509 | G Loss: 0.7567\n",
      "Epoch [3033/5000] | D Loss: 1.2919 | G Loss: 0.7337\n",
      "Epoch [3034/5000] | D Loss: 1.3481 | G Loss: 0.7512\n",
      "Epoch [3035/5000] | D Loss: 1.3034 | G Loss: 0.6836\n",
      "Epoch [3036/5000] | D Loss: 1.3320 | G Loss: 0.7025\n",
      "Epoch [3037/5000] | D Loss: 1.3924 | G Loss: 0.6736\n",
      "Epoch [3038/5000] | D Loss: 1.3715 | G Loss: 0.7219\n",
      "Epoch [3039/5000] | D Loss: 1.3352 | G Loss: 0.7547\n",
      "Epoch [3040/5000] | D Loss: 1.3685 | G Loss: 0.7634\n",
      "Epoch [3041/5000] | D Loss: 1.3736 | G Loss: 0.7334\n",
      "Epoch [3042/5000] | D Loss: 1.3791 | G Loss: 0.7188\n",
      "Epoch [3043/5000] | D Loss: 1.3826 | G Loss: 0.6930\n",
      "Epoch [3044/5000] | D Loss: 1.3252 | G Loss: 0.6894\n",
      "Epoch [3045/5000] | D Loss: 1.3440 | G Loss: 0.7141\n",
      "Epoch [3046/5000] | D Loss: 1.3193 | G Loss: 0.7293\n",
      "Epoch [3047/5000] | D Loss: 1.3353 | G Loss: 0.6964\n",
      "Epoch [3048/5000] | D Loss: 1.3252 | G Loss: 0.7598\n",
      "Epoch [3049/5000] | D Loss: 1.3452 | G Loss: 0.7194\n",
      "Epoch [3050/5000] | D Loss: 1.3010 | G Loss: 0.7179\n",
      "Epoch [3051/5000] | D Loss: 1.3119 | G Loss: 0.7102\n",
      "Epoch [3052/5000] | D Loss: 1.3541 | G Loss: 0.7976\n",
      "Epoch [3053/5000] | D Loss: 1.3294 | G Loss: 0.7604\n",
      "Epoch [3054/5000] | D Loss: 1.3771 | G Loss: 0.7718\n",
      "Epoch [3055/5000] | D Loss: 1.3533 | G Loss: 0.7968\n",
      "Epoch [3056/5000] | D Loss: 1.3299 | G Loss: 0.7342\n",
      "Epoch [3057/5000] | D Loss: 1.2929 | G Loss: 0.7072\n",
      "Epoch [3058/5000] | D Loss: 1.3373 | G Loss: 0.8205\n",
      "Epoch [3059/5000] | D Loss: 1.3217 | G Loss: 0.7323\n",
      "Epoch [3060/5000] | D Loss: 1.3743 | G Loss: 0.7080\n",
      "Epoch [3061/5000] | D Loss: 1.3088 | G Loss: 0.7256\n",
      "Epoch [3062/5000] | D Loss: 1.3171 | G Loss: 0.7726\n",
      "Epoch [3063/5000] | D Loss: 1.3083 | G Loss: 0.8327\n",
      "Epoch [3064/5000] | D Loss: 1.3555 | G Loss: 0.7330\n",
      "Epoch [3065/5000] | D Loss: 1.2266 | G Loss: 0.7140\n",
      "Epoch [3066/5000] | D Loss: 1.3943 | G Loss: 0.7144\n",
      "Epoch [3067/5000] | D Loss: 1.3933 | G Loss: 0.6521\n",
      "Epoch [3068/5000] | D Loss: 1.3113 | G Loss: 0.7995\n",
      "Epoch [3069/5000] | D Loss: 1.3419 | G Loss: 0.6789\n",
      "Epoch [3070/5000] | D Loss: 1.3634 | G Loss: 0.7179\n",
      "Epoch [3071/5000] | D Loss: 1.2732 | G Loss: 0.7481\n",
      "Epoch [3072/5000] | D Loss: 1.3523 | G Loss: 0.7169\n",
      "Epoch [3073/5000] | D Loss: 1.3874 | G Loss: 0.7937\n",
      "Epoch [3074/5000] | D Loss: 1.3304 | G Loss: 0.7716\n",
      "Epoch [3075/5000] | D Loss: 1.3583 | G Loss: 0.7965\n",
      "Epoch [3076/5000] | D Loss: 1.3873 | G Loss: 0.6813\n",
      "Epoch [3077/5000] | D Loss: 1.3508 | G Loss: 0.6529\n",
      "Epoch [3078/5000] | D Loss: 1.3195 | G Loss: 0.7136\n",
      "Epoch [3079/5000] | D Loss: 1.3966 | G Loss: 0.6904\n",
      "Epoch [3080/5000] | D Loss: 1.4156 | G Loss: 0.7154\n",
      "Epoch [3081/5000] | D Loss: 1.3631 | G Loss: 0.7190\n",
      "Epoch [3082/5000] | D Loss: 1.3762 | G Loss: 0.6881\n",
      "Epoch [3083/5000] | D Loss: 1.3398 | G Loss: 0.7272\n",
      "Epoch [3084/5000] | D Loss: 1.3822 | G Loss: 0.7376\n",
      "Epoch [3085/5000] | D Loss: 1.3670 | G Loss: 0.7090\n",
      "Epoch [3086/5000] | D Loss: 1.3465 | G Loss: 0.7115\n",
      "Epoch [3087/5000] | D Loss: 1.3790 | G Loss: 0.7445\n",
      "Epoch [3088/5000] | D Loss: 1.3546 | G Loss: 0.6480\n",
      "Epoch [3089/5000] | D Loss: 1.3900 | G Loss: 0.6320\n",
      "Epoch [3090/5000] | D Loss: 1.3394 | G Loss: 0.7569\n",
      "Epoch [3091/5000] | D Loss: 1.3497 | G Loss: 0.7803\n",
      "Epoch [3092/5000] | D Loss: 1.3983 | G Loss: 0.7198\n",
      "Epoch [3093/5000] | D Loss: 1.3352 | G Loss: 0.7539\n",
      "Epoch [3094/5000] | D Loss: 1.3744 | G Loss: 0.7012\n",
      "Epoch [3095/5000] | D Loss: 1.3103 | G Loss: 0.6943\n",
      "Epoch [3096/5000] | D Loss: 1.3288 | G Loss: 0.7169\n",
      "Epoch [3097/5000] | D Loss: 1.3131 | G Loss: 0.7451\n",
      "Epoch [3098/5000] | D Loss: 1.3155 | G Loss: 0.7261\n",
      "Epoch [3099/5000] | D Loss: 1.3663 | G Loss: 0.7175\n",
      "Epoch [3100/5000] | D Loss: 1.3090 | G Loss: 0.7346\n",
      "Epoch 3100 FID Score: 76.6544\n",
      "Saved improved model at Epoch 3100 with FID 76.6544\n",
      "Epoch [3101/5000] | D Loss: 1.3567 | G Loss: 0.7390\n",
      "Epoch [3102/5000] | D Loss: 1.3996 | G Loss: 0.7352\n",
      "Epoch [3103/5000] | D Loss: 1.3200 | G Loss: 0.7543\n",
      "Epoch [3104/5000] | D Loss: 1.2927 | G Loss: 0.7699\n",
      "Epoch [3105/5000] | D Loss: 1.3519 | G Loss: 0.7374\n",
      "Epoch [3106/5000] | D Loss: 1.3332 | G Loss: 0.7543\n",
      "Epoch [3107/5000] | D Loss: 1.2956 | G Loss: 0.7541\n",
      "Epoch [3108/5000] | D Loss: 1.3663 | G Loss: 0.6725\n",
      "Epoch [3109/5000] | D Loss: 1.3399 | G Loss: 0.7174\n",
      "Epoch [3110/5000] | D Loss: 1.3270 | G Loss: 0.6940\n",
      "Epoch [3111/5000] | D Loss: 1.3517 | G Loss: 0.6947\n",
      "Epoch [3112/5000] | D Loss: 1.3485 | G Loss: 0.6829\n",
      "Epoch [3113/5000] | D Loss: 1.3288 | G Loss: 0.7873\n",
      "Epoch [3114/5000] | D Loss: 1.3690 | G Loss: 0.7666\n",
      "Epoch [3115/5000] | D Loss: 1.3130 | G Loss: 0.7737\n",
      "Epoch [3116/5000] | D Loss: 1.3290 | G Loss: 0.7329\n",
      "Epoch [3117/5000] | D Loss: 1.3478 | G Loss: 0.7327\n",
      "Epoch [3118/5000] | D Loss: 1.3289 | G Loss: 0.7281\n",
      "Epoch [3119/5000] | D Loss: 1.3349 | G Loss: 0.6838\n",
      "Epoch [3120/5000] | D Loss: 1.3598 | G Loss: 0.7066\n",
      "Epoch [3121/5000] | D Loss: 1.3507 | G Loss: 0.7441\n",
      "Epoch [3122/5000] | D Loss: 1.3271 | G Loss: 0.7357\n",
      "Epoch [3123/5000] | D Loss: 1.3603 | G Loss: 0.6834\n",
      "Epoch [3124/5000] | D Loss: 1.3710 | G Loss: 0.7305\n",
      "Epoch [3125/5000] | D Loss: 1.3748 | G Loss: 0.7290\n",
      "Epoch [3126/5000] | D Loss: 1.3141 | G Loss: 0.7144\n",
      "Epoch [3127/5000] | D Loss: 1.3440 | G Loss: 0.6989\n",
      "Epoch [3128/5000] | D Loss: 1.3323 | G Loss: 0.7059\n",
      "Epoch [3129/5000] | D Loss: 1.3754 | G Loss: 0.7057\n",
      "Epoch [3130/5000] | D Loss: 1.3229 | G Loss: 0.7297\n",
      "Epoch [3131/5000] | D Loss: 1.3384 | G Loss: 0.7507\n",
      "Epoch [3132/5000] | D Loss: 1.2878 | G Loss: 0.7777\n",
      "Epoch [3133/5000] | D Loss: 1.2948 | G Loss: 0.7347\n",
      "Epoch [3134/5000] | D Loss: 1.3909 | G Loss: 0.7187\n",
      "Epoch [3135/5000] | D Loss: 1.3161 | G Loss: 0.7664\n",
      "Epoch [3136/5000] | D Loss: 1.3958 | G Loss: 0.7270\n",
      "Epoch [3137/5000] | D Loss: 1.3091 | G Loss: 0.7662\n",
      "Epoch [3138/5000] | D Loss: 1.3277 | G Loss: 0.7054\n",
      "Epoch [3139/5000] | D Loss: 1.3960 | G Loss: 0.6632\n",
      "Epoch [3140/5000] | D Loss: 1.3300 | G Loss: 0.7027\n",
      "Epoch [3141/5000] | D Loss: 1.3399 | G Loss: 0.7348\n",
      "Epoch [3142/5000] | D Loss: 1.3561 | G Loss: 0.7273\n",
      "Epoch [3143/5000] | D Loss: 1.3962 | G Loss: 0.6845\n",
      "Epoch [3144/5000] | D Loss: 1.3470 | G Loss: 0.6990\n",
      "Epoch [3145/5000] | D Loss: 1.3067 | G Loss: 0.7016\n",
      "Epoch [3146/5000] | D Loss: 1.3779 | G Loss: 0.7349\n",
      "Epoch [3147/5000] | D Loss: 1.2723 | G Loss: 0.7557\n",
      "Epoch [3148/5000] | D Loss: 1.3819 | G Loss: 0.7107\n",
      "Epoch [3149/5000] | D Loss: 1.3363 | G Loss: 0.7866\n",
      "Epoch [3150/5000] | D Loss: 1.3221 | G Loss: 0.7153\n",
      "Epoch [3151/5000] | D Loss: 1.3459 | G Loss: 0.6681\n",
      "Epoch [3152/5000] | D Loss: 1.3723 | G Loss: 0.7639\n",
      "Epoch [3153/5000] | D Loss: 1.4019 | G Loss: 0.6984\n",
      "Epoch [3154/5000] | D Loss: 1.2986 | G Loss: 0.7223\n",
      "Epoch [3155/5000] | D Loss: 1.3173 | G Loss: 0.6997\n",
      "Epoch [3156/5000] | D Loss: 1.3790 | G Loss: 0.7524\n",
      "Epoch [3157/5000] | D Loss: 1.3402 | G Loss: 0.7434\n",
      "Epoch [3158/5000] | D Loss: 1.3699 | G Loss: 0.7119\n",
      "Epoch [3159/5000] | D Loss: 1.3570 | G Loss: 0.6880\n",
      "Epoch [3160/5000] | D Loss: 1.3592 | G Loss: 0.7002\n",
      "Epoch [3161/5000] | D Loss: 1.3690 | G Loss: 0.7650\n",
      "Epoch [3162/5000] | D Loss: 1.3648 | G Loss: 0.7431\n",
      "Epoch [3163/5000] | D Loss: 1.2957 | G Loss: 0.7507\n",
      "Epoch [3164/5000] | D Loss: 1.3513 | G Loss: 0.6687\n",
      "Epoch [3165/5000] | D Loss: 1.3869 | G Loss: 0.7804\n",
      "Epoch [3166/5000] | D Loss: 1.3415 | G Loss: 0.7021\n",
      "Epoch [3167/5000] | D Loss: 1.3643 | G Loss: 0.7273\n",
      "Epoch [3168/5000] | D Loss: 1.2993 | G Loss: 0.7877\n",
      "Epoch [3169/5000] | D Loss: 1.3166 | G Loss: 0.7188\n",
      "Epoch [3170/5000] | D Loss: 1.3682 | G Loss: 0.8019\n",
      "Epoch [3171/5000] | D Loss: 1.3334 | G Loss: 0.7223\n",
      "Epoch [3172/5000] | D Loss: 1.3070 | G Loss: 0.6886\n",
      "Epoch [3173/5000] | D Loss: 1.3622 | G Loss: 0.7009\n",
      "Epoch [3174/5000] | D Loss: 1.3679 | G Loss: 0.7478\n",
      "Epoch [3175/5000] | D Loss: 1.3970 | G Loss: 0.7067\n",
      "Epoch [3176/5000] | D Loss: 1.3038 | G Loss: 0.7399\n",
      "Epoch [3177/5000] | D Loss: 1.3775 | G Loss: 0.6983\n",
      "Epoch [3178/5000] | D Loss: 1.3216 | G Loss: 0.7454\n",
      "Epoch [3179/5000] | D Loss: 1.3620 | G Loss: 0.7038\n",
      "Epoch [3180/5000] | D Loss: 1.3875 | G Loss: 0.6643\n",
      "Epoch [3181/5000] | D Loss: 1.3252 | G Loss: 0.7724\n",
      "Epoch [3182/5000] | D Loss: 1.3970 | G Loss: 0.7103\n",
      "Epoch [3183/5000] | D Loss: 1.3494 | G Loss: 0.6795\n",
      "Epoch [3184/5000] | D Loss: 1.3605 | G Loss: 0.7236\n",
      "Epoch [3185/5000] | D Loss: 1.3345 | G Loss: 0.6819\n",
      "Epoch [3186/5000] | D Loss: 1.3214 | G Loss: 0.7397\n",
      "Epoch [3187/5000] | D Loss: 1.3338 | G Loss: 0.7570\n",
      "Epoch [3188/5000] | D Loss: 1.3462 | G Loss: 0.7451\n",
      "Epoch [3189/5000] | D Loss: 1.3815 | G Loss: 0.7041\n",
      "Epoch [3190/5000] | D Loss: 1.3531 | G Loss: 0.7211\n",
      "Epoch [3191/5000] | D Loss: 1.3750 | G Loss: 0.7676\n",
      "Epoch [3192/5000] | D Loss: 1.3257 | G Loss: 0.7193\n",
      "Epoch [3193/5000] | D Loss: 1.2880 | G Loss: 0.7829\n",
      "Epoch [3194/5000] | D Loss: 1.3300 | G Loss: 0.7104\n",
      "Epoch [3195/5000] | D Loss: 1.3604 | G Loss: 0.7335\n",
      "Epoch [3196/5000] | D Loss: 1.3383 | G Loss: 0.6959\n",
      "Epoch [3197/5000] | D Loss: 1.3538 | G Loss: 0.7270\n",
      "Epoch [3198/5000] | D Loss: 1.3582 | G Loss: 0.7253\n",
      "Epoch [3199/5000] | D Loss: 1.3220 | G Loss: 0.7025\n",
      "Epoch [3200/5000] | D Loss: 1.3946 | G Loss: 0.7442\n",
      "Epoch 3200 FID Score: 75.3786\n",
      "Saved improved model at Epoch 3200 with FID 75.3786\n",
      "Epoch [3201/5000] | D Loss: 1.3504 | G Loss: 0.7408\n",
      "Epoch [3202/5000] | D Loss: 1.3665 | G Loss: 0.6959\n",
      "Epoch [3203/5000] | D Loss: 1.3230 | G Loss: 0.7394\n",
      "Epoch [3204/5000] | D Loss: 1.3056 | G Loss: 0.6986\n",
      "Epoch [3205/5000] | D Loss: 1.3481 | G Loss: 0.7201\n",
      "Epoch [3206/5000] | D Loss: 1.3377 | G Loss: 0.7359\n",
      "Epoch [3207/5000] | D Loss: 1.3743 | G Loss: 0.6901\n",
      "Epoch [3208/5000] | D Loss: 1.3990 | G Loss: 0.7170\n",
      "Epoch [3209/5000] | D Loss: 1.3229 | G Loss: 0.7215\n",
      "Epoch [3210/5000] | D Loss: 1.3689 | G Loss: 0.6998\n",
      "Epoch [3211/5000] | D Loss: 1.3795 | G Loss: 0.7194\n",
      "Epoch [3212/5000] | D Loss: 1.3541 | G Loss: 0.7176\n",
      "Epoch [3213/5000] | D Loss: 1.3316 | G Loss: 0.7374\n",
      "Epoch [3214/5000] | D Loss: 1.3665 | G Loss: 0.7018\n",
      "Epoch [3215/5000] | D Loss: 1.3953 | G Loss: 0.7283\n",
      "Epoch [3216/5000] | D Loss: 1.3426 | G Loss: 0.7195\n",
      "Epoch [3217/5000] | D Loss: 1.3423 | G Loss: 0.7198\n",
      "Epoch [3218/5000] | D Loss: 1.3177 | G Loss: 0.7761\n",
      "Epoch [3219/5000] | D Loss: 1.3530 | G Loss: 0.7202\n",
      "Epoch [3220/5000] | D Loss: 1.3824 | G Loss: 0.6964\n",
      "Epoch [3221/5000] | D Loss: 1.3667 | G Loss: 0.6758\n",
      "Epoch [3222/5000] | D Loss: 1.3627 | G Loss: 0.7020\n",
      "Epoch [3223/5000] | D Loss: 1.3199 | G Loss: 0.6973\n",
      "Epoch [3224/5000] | D Loss: 1.3228 | G Loss: 0.7418\n",
      "Epoch [3225/5000] | D Loss: 1.3567 | G Loss: 0.7504\n",
      "Epoch [3226/5000] | D Loss: 1.3582 | G Loss: 0.7839\n",
      "Epoch [3227/5000] | D Loss: 1.3516 | G Loss: 0.7225\n",
      "Epoch [3228/5000] | D Loss: 1.3364 | G Loss: 0.7509\n",
      "Epoch [3229/5000] | D Loss: 1.2928 | G Loss: 0.7200\n",
      "Epoch [3230/5000] | D Loss: 1.3581 | G Loss: 0.7093\n",
      "Epoch [3231/5000] | D Loss: 1.3473 | G Loss: 0.6708\n",
      "Epoch [3232/5000] | D Loss: 1.3441 | G Loss: 0.7033\n",
      "Epoch [3233/5000] | D Loss: 1.3274 | G Loss: 0.7419\n",
      "Epoch [3234/5000] | D Loss: 1.3838 | G Loss: 0.7382\n",
      "Epoch [3235/5000] | D Loss: 1.3776 | G Loss: 0.7354\n",
      "Epoch [3236/5000] | D Loss: 1.3959 | G Loss: 0.6701\n",
      "Epoch [3237/5000] | D Loss: 1.3288 | G Loss: 0.6880\n",
      "Epoch [3238/5000] | D Loss: 1.3487 | G Loss: 0.7230\n",
      "Epoch [3239/5000] | D Loss: 1.3500 | G Loss: 0.7158\n",
      "Epoch [3240/5000] | D Loss: 1.3470 | G Loss: 0.7414\n",
      "Epoch [3241/5000] | D Loss: 1.3435 | G Loss: 0.7270\n",
      "Epoch [3242/5000] | D Loss: 1.3469 | G Loss: 0.7268\n",
      "Epoch [3243/5000] | D Loss: 1.3754 | G Loss: 0.7203\n",
      "Epoch [3244/5000] | D Loss: 1.3450 | G Loss: 0.6940\n",
      "Epoch [3245/5000] | D Loss: 1.3721 | G Loss: 0.7005\n",
      "Epoch [3246/5000] | D Loss: 1.3559 | G Loss: 0.7491\n",
      "Epoch [3247/5000] | D Loss: 1.3124 | G Loss: 0.7450\n",
      "Epoch [3248/5000] | D Loss: 1.3874 | G Loss: 0.7319\n",
      "Epoch [3249/5000] | D Loss: 1.3650 | G Loss: 0.7113\n",
      "Epoch [3250/5000] | D Loss: 1.3347 | G Loss: 0.6843\n",
      "Epoch [3251/5000] | D Loss: 1.3569 | G Loss: 0.7900\n",
      "Epoch [3252/5000] | D Loss: 1.3630 | G Loss: 0.7393\n",
      "Epoch [3253/5000] | D Loss: 1.3423 | G Loss: 0.6787\n",
      "Epoch [3254/5000] | D Loss: 1.3323 | G Loss: 0.7283\n",
      "Epoch [3255/5000] | D Loss: 1.3632 | G Loss: 0.7484\n",
      "Epoch [3256/5000] | D Loss: 1.3915 | G Loss: 0.7492\n",
      "Epoch [3257/5000] | D Loss: 1.3291 | G Loss: 0.7054\n",
      "Epoch [3258/5000] | D Loss: 1.3365 | G Loss: 0.7271\n",
      "Epoch [3259/5000] | D Loss: 1.3194 | G Loss: 0.7086\n",
      "Epoch [3260/5000] | D Loss: 1.3660 | G Loss: 0.7677\n",
      "Epoch [3261/5000] | D Loss: 1.3068 | G Loss: 0.7533\n",
      "Epoch [3262/5000] | D Loss: 1.4169 | G Loss: 0.6883\n",
      "Epoch [3263/5000] | D Loss: 1.3591 | G Loss: 0.6856\n",
      "Epoch [3264/5000] | D Loss: 1.3577 | G Loss: 0.6773\n",
      "Epoch [3265/5000] | D Loss: 1.2899 | G Loss: 0.7595\n",
      "Epoch [3266/5000] | D Loss: 1.3651 | G Loss: 0.7095\n",
      "Epoch [3267/5000] | D Loss: 1.3110 | G Loss: 0.7288\n",
      "Epoch [3268/5000] | D Loss: 1.3527 | G Loss: 0.7777\n",
      "Epoch [3269/5000] | D Loss: 1.3699 | G Loss: 0.6350\n",
      "Epoch [3270/5000] | D Loss: 1.3199 | G Loss: 0.7791\n",
      "Epoch [3271/5000] | D Loss: 1.3479 | G Loss: 0.7265\n",
      "Epoch [3272/5000] | D Loss: 1.3536 | G Loss: 0.7497\n",
      "Epoch [3273/5000] | D Loss: 1.3737 | G Loss: 0.7338\n",
      "Epoch [3274/5000] | D Loss: 1.3477 | G Loss: 0.7181\n",
      "Epoch [3275/5000] | D Loss: 1.3729 | G Loss: 0.7421\n",
      "Epoch [3276/5000] | D Loss: 1.3751 | G Loss: 0.6921\n",
      "Epoch [3277/5000] | D Loss: 1.3466 | G Loss: 0.6686\n",
      "Epoch [3278/5000] | D Loss: 1.3203 | G Loss: 0.7048\n",
      "Epoch [3279/5000] | D Loss: 1.3313 | G Loss: 0.7259\n",
      "Epoch [3280/5000] | D Loss: 1.3428 | G Loss: 0.7175\n",
      "Epoch [3281/5000] | D Loss: 1.3315 | G Loss: 0.7656\n",
      "Epoch [3282/5000] | D Loss: 1.3420 | G Loss: 0.7300\n",
      "Epoch [3283/5000] | D Loss: 1.3513 | G Loss: 0.7680\n",
      "Epoch [3284/5000] | D Loss: 1.4033 | G Loss: 0.7341\n",
      "Epoch [3285/5000] | D Loss: 1.3723 | G Loss: 0.7175\n",
      "Epoch [3286/5000] | D Loss: 1.3251 | G Loss: 0.7140\n",
      "Epoch [3287/5000] | D Loss: 1.3114 | G Loss: 0.7680\n",
      "Epoch [3288/5000] | D Loss: 1.3455 | G Loss: 0.6796\n",
      "Epoch [3289/5000] | D Loss: 1.3711 | G Loss: 0.7664\n",
      "Epoch [3290/5000] | D Loss: 1.3820 | G Loss: 0.7872\n",
      "Epoch [3291/5000] | D Loss: 1.3047 | G Loss: 0.7715\n",
      "Epoch [3292/5000] | D Loss: 1.3420 | G Loss: 0.7366\n",
      "Epoch [3293/5000] | D Loss: 1.3607 | G Loss: 0.7308\n",
      "Epoch [3294/5000] | D Loss: 1.3371 | G Loss: 0.7923\n",
      "Epoch [3295/5000] | D Loss: 1.3504 | G Loss: 0.7691\n",
      "Epoch [3296/5000] | D Loss: 1.3783 | G Loss: 0.7056\n",
      "Epoch [3297/5000] | D Loss: 1.2900 | G Loss: 0.8218\n",
      "Epoch [3298/5000] | D Loss: 1.3121 | G Loss: 0.7062\n",
      "Epoch [3299/5000] | D Loss: 1.3596 | G Loss: 0.7103\n",
      "Epoch [3300/5000] | D Loss: 1.3701 | G Loss: 0.7812\n",
      "Epoch 3300 FID Score: 75.2349\n",
      "Saved improved model at Epoch 3300 with FID 75.2349\n",
      "Epoch [3301/5000] | D Loss: 1.3424 | G Loss: 0.7513\n",
      "Epoch [3302/5000] | D Loss: 1.3330 | G Loss: 0.6881\n",
      "Epoch [3303/5000] | D Loss: 1.3684 | G Loss: 0.7024\n",
      "Epoch [3304/5000] | D Loss: 1.3540 | G Loss: 0.7157\n",
      "Epoch [3305/5000] | D Loss: 1.3349 | G Loss: 0.6972\n",
      "Epoch [3306/5000] | D Loss: 1.3426 | G Loss: 0.7304\n",
      "Epoch [3307/5000] | D Loss: 1.3326 | G Loss: 0.7521\n",
      "Epoch [3308/5000] | D Loss: 1.3230 | G Loss: 0.7398\n",
      "Epoch [3309/5000] | D Loss: 1.3285 | G Loss: 0.6734\n",
      "Epoch [3310/5000] | D Loss: 1.3320 | G Loss: 0.6746\n",
      "Epoch [3311/5000] | D Loss: 1.3942 | G Loss: 0.7537\n",
      "Epoch [3312/5000] | D Loss: 1.3440 | G Loss: 0.7458\n",
      "Epoch [3313/5000] | D Loss: 1.3701 | G Loss: 0.7271\n",
      "Epoch [3314/5000] | D Loss: 1.3346 | G Loss: 0.7375\n",
      "Epoch [3315/5000] | D Loss: 1.4117 | G Loss: 0.7145\n",
      "Epoch [3316/5000] | D Loss: 1.3018 | G Loss: 0.7580\n",
      "Epoch [3317/5000] | D Loss: 1.3605 | G Loss: 0.6905\n",
      "Epoch [3318/5000] | D Loss: 1.3494 | G Loss: 0.7391\n",
      "Epoch [3319/5000] | D Loss: 1.2956 | G Loss: 0.7266\n",
      "Epoch [3320/5000] | D Loss: 1.3179 | G Loss: 0.7503\n",
      "Epoch [3321/5000] | D Loss: 1.3267 | G Loss: 0.7135\n",
      "Epoch [3322/5000] | D Loss: 1.3764 | G Loss: 0.7286\n",
      "Epoch [3323/5000] | D Loss: 1.3309 | G Loss: 0.7078\n",
      "Epoch [3324/5000] | D Loss: 1.3443 | G Loss: 0.7468\n",
      "Epoch [3325/5000] | D Loss: 1.3674 | G Loss: 0.7952\n",
      "Epoch [3326/5000] | D Loss: 1.3831 | G Loss: 0.7054\n",
      "Epoch [3327/5000] | D Loss: 1.3183 | G Loss: 0.7850\n",
      "Epoch [3328/5000] | D Loss: 1.3241 | G Loss: 0.6951\n",
      "Epoch [3329/5000] | D Loss: 1.2715 | G Loss: 0.7940\n",
      "Epoch [3330/5000] | D Loss: 1.3605 | G Loss: 0.7128\n",
      "Epoch [3331/5000] | D Loss: 1.3775 | G Loss: 0.7183\n",
      "Epoch [3332/5000] | D Loss: 1.3078 | G Loss: 0.6764\n",
      "Epoch [3333/5000] | D Loss: 1.3997 | G Loss: 0.6975\n",
      "Epoch [3334/5000] | D Loss: 1.3528 | G Loss: 0.7055\n",
      "Epoch [3335/5000] | D Loss: 1.3455 | G Loss: 0.7120\n",
      "Epoch [3336/5000] | D Loss: 1.3778 | G Loss: 0.7581\n",
      "Epoch [3337/5000] | D Loss: 1.3412 | G Loss: 0.7512\n",
      "Epoch [3338/5000] | D Loss: 1.3683 | G Loss: 0.7542\n",
      "Epoch [3339/5000] | D Loss: 1.2640 | G Loss: 0.7577\n",
      "Epoch [3340/5000] | D Loss: 1.3324 | G Loss: 0.7304\n",
      "Epoch [3341/5000] | D Loss: 1.2925 | G Loss: 0.6674\n",
      "Epoch [3342/5000] | D Loss: 1.3474 | G Loss: 0.6821\n",
      "Epoch [3343/5000] | D Loss: 1.3805 | G Loss: 0.7265\n",
      "Epoch [3344/5000] | D Loss: 1.3872 | G Loss: 0.7222\n",
      "Epoch [3345/5000] | D Loss: 1.4204 | G Loss: 0.7421\n",
      "Epoch [3346/5000] | D Loss: 1.3350 | G Loss: 0.7409\n",
      "Epoch [3347/5000] | D Loss: 1.3452 | G Loss: 0.7662\n",
      "Epoch [3348/5000] | D Loss: 1.2982 | G Loss: 0.7301\n",
      "Epoch [3349/5000] | D Loss: 1.3211 | G Loss: 0.7027\n",
      "Epoch [3350/5000] | D Loss: 1.3242 | G Loss: 0.7231\n",
      "Epoch [3351/5000] | D Loss: 1.3431 | G Loss: 0.7100\n",
      "Epoch [3352/5000] | D Loss: 1.3101 | G Loss: 0.7347\n",
      "Epoch [3353/5000] | D Loss: 1.3583 | G Loss: 0.6937\n",
      "Epoch [3354/5000] | D Loss: 1.3844 | G Loss: 0.7316\n",
      "Epoch [3355/5000] | D Loss: 1.3348 | G Loss: 0.7675\n",
      "Epoch [3356/5000] | D Loss: 1.3999 | G Loss: 0.7272\n",
      "Epoch [3357/5000] | D Loss: 1.4341 | G Loss: 0.7047\n",
      "Epoch [3358/5000] | D Loss: 1.3138 | G Loss: 0.6967\n",
      "Epoch [3359/5000] | D Loss: 1.3247 | G Loss: 0.7234\n",
      "Epoch [3360/5000] | D Loss: 1.3727 | G Loss: 0.7159\n",
      "Epoch [3361/5000] | D Loss: 1.3535 | G Loss: 0.7584\n",
      "Epoch [3362/5000] | D Loss: 1.3400 | G Loss: 0.7261\n",
      "Epoch [3363/5000] | D Loss: 1.3782 | G Loss: 0.7406\n",
      "Epoch [3364/5000] | D Loss: 1.3314 | G Loss: 0.7214\n",
      "Epoch [3365/5000] | D Loss: 1.3289 | G Loss: 0.6910\n",
      "Epoch [3366/5000] | D Loss: 1.3647 | G Loss: 0.7216\n",
      "Epoch [3367/5000] | D Loss: 1.3308 | G Loss: 0.7413\n",
      "Epoch [3368/5000] | D Loss: 1.3698 | G Loss: 0.7081\n",
      "Epoch [3369/5000] | D Loss: 1.3178 | G Loss: 0.6540\n",
      "Epoch [3370/5000] | D Loss: 1.3899 | G Loss: 0.7431\n",
      "Epoch [3371/5000] | D Loss: 1.2964 | G Loss: 0.7379\n",
      "Epoch [3372/5000] | D Loss: 1.4124 | G Loss: 0.7486\n",
      "Epoch [3373/5000] | D Loss: 1.3461 | G Loss: 0.7307\n",
      "Epoch [3374/5000] | D Loss: 1.3627 | G Loss: 0.7298\n",
      "Epoch [3375/5000] | D Loss: 1.3157 | G Loss: 0.7099\n",
      "Epoch [3376/5000] | D Loss: 1.3591 | G Loss: 0.7293\n",
      "Epoch [3377/5000] | D Loss: 1.3097 | G Loss: 0.6643\n",
      "Epoch [3378/5000] | D Loss: 1.3443 | G Loss: 0.7497\n",
      "Epoch [3379/5000] | D Loss: 1.3689 | G Loss: 0.7879\n",
      "Epoch [3380/5000] | D Loss: 1.3325 | G Loss: 0.7674\n",
      "Epoch [3381/5000] | D Loss: 1.3491 | G Loss: 0.7494\n",
      "Epoch [3382/5000] | D Loss: 1.3476 | G Loss: 0.7262\n",
      "Epoch [3383/5000] | D Loss: 1.3513 | G Loss: 0.7693\n",
      "Epoch [3384/5000] | D Loss: 1.3830 | G Loss: 0.7008\n",
      "Epoch [3385/5000] | D Loss: 1.3898 | G Loss: 0.7762\n",
      "Epoch [3386/5000] | D Loss: 1.3837 | G Loss: 0.7280\n",
      "Epoch [3387/5000] | D Loss: 1.3937 | G Loss: 0.7312\n",
      "Epoch [3388/5000] | D Loss: 1.3874 | G Loss: 0.7310\n",
      "Epoch [3389/5000] | D Loss: 1.3505 | G Loss: 0.7064\n",
      "Epoch [3390/5000] | D Loss: 1.3626 | G Loss: 0.7588\n",
      "Epoch [3391/5000] | D Loss: 1.3672 | G Loss: 0.7343\n",
      "Epoch [3392/5000] | D Loss: 1.3202 | G Loss: 0.7914\n",
      "Epoch [3393/5000] | D Loss: 1.3374 | G Loss: 0.7354\n",
      "Epoch [3394/5000] | D Loss: 1.3359 | G Loss: 0.6950\n",
      "Epoch [3395/5000] | D Loss: 1.3383 | G Loss: 0.7622\n",
      "Epoch [3396/5000] | D Loss: 1.3051 | G Loss: 0.7273\n",
      "Epoch [3397/5000] | D Loss: 1.3256 | G Loss: 0.7013\n",
      "Epoch [3398/5000] | D Loss: 1.3220 | G Loss: 0.7708\n",
      "Epoch [3399/5000] | D Loss: 1.3601 | G Loss: 0.7293\n",
      "Epoch [3400/5000] | D Loss: 1.3576 | G Loss: 0.6741\n",
      "Epoch 3400 FID Score: 71.4985\n",
      "Saved improved model at Epoch 3400 with FID 71.4985\n",
      "Epoch [3401/5000] | D Loss: 1.3739 | G Loss: 0.7264\n",
      "Epoch [3402/5000] | D Loss: 1.3699 | G Loss: 0.6968\n",
      "Epoch [3403/5000] | D Loss: 1.3511 | G Loss: 0.6812\n",
      "Epoch [3404/5000] | D Loss: 1.3743 | G Loss: 0.7005\n",
      "Epoch [3405/5000] | D Loss: 1.3647 | G Loss: 0.7080\n",
      "Epoch [3406/5000] | D Loss: 1.3403 | G Loss: 0.7010\n",
      "Epoch [3407/5000] | D Loss: 1.3296 | G Loss: 0.6831\n",
      "Epoch [3408/5000] | D Loss: 1.3050 | G Loss: 0.7531\n",
      "Epoch [3409/5000] | D Loss: 1.3431 | G Loss: 0.7654\n",
      "Epoch [3410/5000] | D Loss: 1.3139 | G Loss: 0.7268\n",
      "Epoch [3411/5000] | D Loss: 1.3403 | G Loss: 0.7250\n",
      "Epoch [3412/5000] | D Loss: 1.3967 | G Loss: 0.7142\n",
      "Epoch [3413/5000] | D Loss: 1.3241 | G Loss: 0.6928\n",
      "Epoch [3414/5000] | D Loss: 1.3562 | G Loss: 0.6922\n",
      "Epoch [3415/5000] | D Loss: 1.4018 | G Loss: 0.6683\n",
      "Epoch [3416/5000] | D Loss: 1.4077 | G Loss: 0.6854\n",
      "Epoch [3417/5000] | D Loss: 1.3201 | G Loss: 0.7284\n",
      "Epoch [3418/5000] | D Loss: 1.3816 | G Loss: 0.7028\n",
      "Epoch [3419/5000] | D Loss: 1.3308 | G Loss: 0.7328\n",
      "Epoch [3420/5000] | D Loss: 1.3698 | G Loss: 0.7261\n",
      "Epoch [3421/5000] | D Loss: 1.3926 | G Loss: 0.6900\n",
      "Epoch [3422/5000] | D Loss: 1.3333 | G Loss: 0.6979\n",
      "Epoch [3423/5000] | D Loss: 1.2993 | G Loss: 0.6909\n",
      "Epoch [3424/5000] | D Loss: 1.3307 | G Loss: 0.7128\n",
      "Epoch [3425/5000] | D Loss: 1.3120 | G Loss: 0.7356\n",
      "Epoch [3426/5000] | D Loss: 1.3959 | G Loss: 0.7523\n",
      "Epoch [3427/5000] | D Loss: 1.3668 | G Loss: 0.8038\n",
      "Epoch [3428/5000] | D Loss: 1.3799 | G Loss: 0.7412\n",
      "Epoch [3429/5000] | D Loss: 1.2919 | G Loss: 0.7447\n",
      "Epoch [3430/5000] | D Loss: 1.3625 | G Loss: 0.6951\n",
      "Epoch [3431/5000] | D Loss: 1.3071 | G Loss: 0.6994\n",
      "Epoch [3432/5000] | D Loss: 1.3513 | G Loss: 0.7220\n",
      "Epoch [3433/5000] | D Loss: 1.3548 | G Loss: 0.7505\n",
      "Epoch [3434/5000] | D Loss: 1.3328 | G Loss: 0.6971\n",
      "Epoch [3435/5000] | D Loss: 1.3466 | G Loss: 0.7456\n",
      "Epoch [3436/5000] | D Loss: 1.3567 | G Loss: 0.7120\n",
      "Epoch [3437/5000] | D Loss: 1.3333 | G Loss: 0.7149\n",
      "Epoch [3438/5000] | D Loss: 1.3688 | G Loss: 0.7424\n",
      "Epoch [3439/5000] | D Loss: 1.3305 | G Loss: 0.6981\n",
      "Epoch [3440/5000] | D Loss: 1.3595 | G Loss: 0.7040\n",
      "Epoch [3441/5000] | D Loss: 1.3517 | G Loss: 0.7752\n",
      "Epoch [3442/5000] | D Loss: 1.3487 | G Loss: 0.7362\n",
      "Epoch [3443/5000] | D Loss: 1.3419 | G Loss: 0.7120\n",
      "Epoch [3444/5000] | D Loss: 1.3160 | G Loss: 0.7621\n",
      "Epoch [3445/5000] | D Loss: 1.3351 | G Loss: 0.7089\n",
      "Epoch [3446/5000] | D Loss: 1.3225 | G Loss: 0.6866\n",
      "Epoch [3447/5000] | D Loss: 1.3292 | G Loss: 0.7598\n",
      "Epoch [3448/5000] | D Loss: 1.3991 | G Loss: 0.7399\n",
      "Epoch [3449/5000] | D Loss: 1.3680 | G Loss: 0.7182\n",
      "Epoch [3450/5000] | D Loss: 1.3330 | G Loss: 0.7524\n",
      "Epoch [3451/5000] | D Loss: 1.3568 | G Loss: 0.7222\n",
      "Epoch [3452/5000] | D Loss: 1.3707 | G Loss: 0.7282\n",
      "Epoch [3453/5000] | D Loss: 1.3399 | G Loss: 0.7011\n",
      "Epoch [3454/5000] | D Loss: 1.3549 | G Loss: 0.7130\n",
      "Epoch [3455/5000] | D Loss: 1.3335 | G Loss: 0.7025\n",
      "Epoch [3456/5000] | D Loss: 1.3257 | G Loss: 0.7657\n",
      "Epoch [3457/5000] | D Loss: 1.3869 | G Loss: 0.6885\n",
      "Epoch [3458/5000] | D Loss: 1.3973 | G Loss: 0.7303\n",
      "Epoch [3459/5000] | D Loss: 1.3943 | G Loss: 0.7375\n",
      "Epoch [3460/5000] | D Loss: 1.3561 | G Loss: 0.7531\n",
      "Epoch [3461/5000] | D Loss: 1.3693 | G Loss: 0.7171\n",
      "Epoch [3462/5000] | D Loss: 1.3229 | G Loss: 0.7551\n",
      "Epoch [3463/5000] | D Loss: 1.3783 | G Loss: 0.6886\n",
      "Epoch [3464/5000] | D Loss: 1.3098 | G Loss: 0.7606\n",
      "Epoch [3465/5000] | D Loss: 1.3922 | G Loss: 0.7261\n",
      "Epoch [3466/5000] | D Loss: 1.3393 | G Loss: 0.7445\n",
      "Epoch [3467/5000] | D Loss: 1.3585 | G Loss: 0.7102\n",
      "Epoch [3468/5000] | D Loss: 1.3533 | G Loss: 0.6623\n",
      "Epoch [3469/5000] | D Loss: 1.3404 | G Loss: 0.7411\n",
      "Epoch [3470/5000] | D Loss: 1.3711 | G Loss: 0.6891\n",
      "Epoch [3471/5000] | D Loss: 1.3702 | G Loss: 0.6509\n",
      "Epoch [3472/5000] | D Loss: 1.3623 | G Loss: 0.7647\n",
      "Epoch [3473/5000] | D Loss: 1.3827 | G Loss: 0.6991\n",
      "Epoch [3474/5000] | D Loss: 1.4094 | G Loss: 0.7420\n",
      "Epoch [3475/5000] | D Loss: 1.3764 | G Loss: 0.7047\n",
      "Epoch [3476/5000] | D Loss: 1.3541 | G Loss: 0.6941\n",
      "Epoch [3477/5000] | D Loss: 1.3020 | G Loss: 0.7199\n",
      "Epoch [3478/5000] | D Loss: 1.3797 | G Loss: 0.6968\n",
      "Epoch [3479/5000] | D Loss: 1.3517 | G Loss: 0.6982\n",
      "Epoch [3480/5000] | D Loss: 1.3491 | G Loss: 0.7984\n",
      "Epoch [3481/5000] | D Loss: 1.3616 | G Loss: 0.7061\n",
      "Epoch [3482/5000] | D Loss: 1.3287 | G Loss: 0.7487\n",
      "Epoch [3483/5000] | D Loss: 1.3289 | G Loss: 0.7068\n",
      "Epoch [3484/5000] | D Loss: 1.3719 | G Loss: 0.6913\n",
      "Epoch [3485/5000] | D Loss: 1.3676 | G Loss: 0.6892\n",
      "Epoch [3486/5000] | D Loss: 1.3336 | G Loss: 0.7296\n",
      "Epoch [3487/5000] | D Loss: 1.3692 | G Loss: 0.7466\n",
      "Epoch [3488/5000] | D Loss: 1.3526 | G Loss: 0.7078\n",
      "Epoch [3489/5000] | D Loss: 1.3578 | G Loss: 0.7579\n",
      "Epoch [3490/5000] | D Loss: 1.3090 | G Loss: 0.7221\n",
      "Epoch [3491/5000] | D Loss: 1.3337 | G Loss: 0.6854\n",
      "Epoch [3492/5000] | D Loss: 1.3330 | G Loss: 0.7416\n",
      "Epoch [3493/5000] | D Loss: 1.3394 | G Loss: 0.7500\n",
      "Epoch [3494/5000] | D Loss: 1.3557 | G Loss: 0.7366\n",
      "Epoch [3495/5000] | D Loss: 1.3841 | G Loss: 0.6900\n",
      "Epoch [3496/5000] | D Loss: 1.3512 | G Loss: 0.6913\n",
      "Epoch [3497/5000] | D Loss: 1.3300 | G Loss: 0.7547\n",
      "Epoch [3498/5000] | D Loss: 1.3749 | G Loss: 0.7069\n",
      "Epoch [3499/5000] | D Loss: 1.4106 | G Loss: 0.7391\n",
      "Epoch [3500/5000] | D Loss: 1.3388 | G Loss: 0.6939\n",
      "Epoch 3500 FID Score: 72.8951\n",
      "Epoch [3501/5000] | D Loss: 1.3521 | G Loss: 0.7567\n",
      "Epoch [3502/5000] | D Loss: 1.3759 | G Loss: 0.7380\n",
      "Epoch [3503/5000] | D Loss: 1.3444 | G Loss: 0.7846\n",
      "Epoch [3504/5000] | D Loss: 1.3578 | G Loss: 0.7247\n",
      "Epoch [3505/5000] | D Loss: 1.3443 | G Loss: 0.7029\n",
      "Epoch [3506/5000] | D Loss: 1.3255 | G Loss: 0.7486\n",
      "Epoch [3507/5000] | D Loss: 1.3920 | G Loss: 0.7529\n",
      "Epoch [3508/5000] | D Loss: 1.3459 | G Loss: 0.7403\n",
      "Epoch [3509/5000] | D Loss: 1.4018 | G Loss: 0.7418\n",
      "Epoch [3510/5000] | D Loss: 1.3436 | G Loss: 0.7429\n",
      "Epoch [3511/5000] | D Loss: 1.3445 | G Loss: 0.7136\n",
      "Epoch [3512/5000] | D Loss: 1.3331 | G Loss: 0.7192\n",
      "Epoch [3513/5000] | D Loss: 1.3451 | G Loss: 0.7347\n",
      "Epoch [3514/5000] | D Loss: 1.3681 | G Loss: 0.6801\n",
      "Epoch [3515/5000] | D Loss: 1.3614 | G Loss: 0.7195\n",
      "Epoch [3516/5000] | D Loss: 1.3300 | G Loss: 0.7274\n",
      "Epoch [3517/5000] | D Loss: 1.3718 | G Loss: 0.7325\n",
      "Epoch [3518/5000] | D Loss: 1.3414 | G Loss: 0.7733\n",
      "Epoch [3519/5000] | D Loss: 1.3812 | G Loss: 0.7118\n",
      "Epoch [3520/5000] | D Loss: 1.3245 | G Loss: 0.7012\n",
      "Epoch [3521/5000] | D Loss: 1.3686 | G Loss: 0.7414\n",
      "Epoch [3522/5000] | D Loss: 1.3670 | G Loss: 0.7359\n",
      "Epoch [3523/5000] | D Loss: 1.3917 | G Loss: 0.7325\n",
      "Epoch [3524/5000] | D Loss: 1.3511 | G Loss: 0.7014\n",
      "Epoch [3525/5000] | D Loss: 1.3410 | G Loss: 0.7459\n",
      "Epoch [3526/5000] | D Loss: 1.2967 | G Loss: 0.7851\n",
      "Epoch [3527/5000] | D Loss: 1.2810 | G Loss: 0.7379\n",
      "Epoch [3528/5000] | D Loss: 1.3074 | G Loss: 0.7192\n",
      "Epoch [3529/5000] | D Loss: 1.3622 | G Loss: 0.7685\n",
      "Epoch [3530/5000] | D Loss: 1.3544 | G Loss: 0.7065\n",
      "Epoch [3531/5000] | D Loss: 1.3189 | G Loss: 0.7068\n",
      "Epoch [3532/5000] | D Loss: 1.3308 | G Loss: 0.7723\n",
      "Epoch [3533/5000] | D Loss: 1.3659 | G Loss: 0.7057\n",
      "Epoch [3534/5000] | D Loss: 1.3373 | G Loss: 0.7140\n",
      "Epoch [3535/5000] | D Loss: 1.3733 | G Loss: 0.7513\n",
      "Epoch [3536/5000] | D Loss: 1.3172 | G Loss: 0.7390\n",
      "Epoch [3537/5000] | D Loss: 1.3569 | G Loss: 0.7323\n",
      "Epoch [3538/5000] | D Loss: 1.3869 | G Loss: 0.7582\n",
      "Epoch [3539/5000] | D Loss: 1.3373 | G Loss: 0.7122\n",
      "Epoch [3540/5000] | D Loss: 1.3603 | G Loss: 0.6736\n",
      "Epoch [3541/5000] | D Loss: 1.3549 | G Loss: 0.7157\n",
      "Epoch [3542/5000] | D Loss: 1.3467 | G Loss: 0.7151\n",
      "Epoch [3543/5000] | D Loss: 1.3334 | G Loss: 0.7022\n",
      "Epoch [3544/5000] | D Loss: 1.3737 | G Loss: 0.7387\n",
      "Epoch [3545/5000] | D Loss: 1.3638 | G Loss: 0.7528\n",
      "Epoch [3546/5000] | D Loss: 1.3845 | G Loss: 0.6931\n",
      "Epoch [3547/5000] | D Loss: 1.3669 | G Loss: 0.7533\n",
      "Epoch [3548/5000] | D Loss: 1.3672 | G Loss: 0.6659\n",
      "Epoch [3549/5000] | D Loss: 1.3829 | G Loss: 0.7241\n",
      "Epoch [3550/5000] | D Loss: 1.3549 | G Loss: 0.7167\n",
      "Epoch [3551/5000] | D Loss: 1.3606 | G Loss: 0.7259\n",
      "Epoch [3552/5000] | D Loss: 1.4018 | G Loss: 0.6956\n",
      "Epoch [3553/5000] | D Loss: 1.3591 | G Loss: 0.6832\n",
      "Epoch [3554/5000] | D Loss: 1.3573 | G Loss: 0.7558\n",
      "Epoch [3555/5000] | D Loss: 1.4004 | G Loss: 0.7261\n",
      "Epoch [3556/5000] | D Loss: 1.3568 | G Loss: 0.7580\n",
      "Epoch [3557/5000] | D Loss: 1.4000 | G Loss: 0.6932\n",
      "Epoch [3558/5000] | D Loss: 1.4109 | G Loss: 0.7141\n",
      "Epoch [3559/5000] | D Loss: 1.3257 | G Loss: 0.6800\n",
      "Epoch [3560/5000] | D Loss: 1.3588 | G Loss: 0.7360\n",
      "Epoch [3561/5000] | D Loss: 1.3848 | G Loss: 0.7242\n",
      "Epoch [3562/5000] | D Loss: 1.3224 | G Loss: 0.7261\n",
      "Epoch [3563/5000] | D Loss: 1.3640 | G Loss: 0.7291\n",
      "Epoch [3564/5000] | D Loss: 1.3792 | G Loss: 0.6865\n",
      "Epoch [3565/5000] | D Loss: 1.3992 | G Loss: 0.7032\n",
      "Epoch [3566/5000] | D Loss: 1.3686 | G Loss: 0.7485\n",
      "Epoch [3567/5000] | D Loss: 1.3140 | G Loss: 0.7244\n",
      "Epoch [3568/5000] | D Loss: 1.3565 | G Loss: 0.7291\n",
      "Epoch [3569/5000] | D Loss: 1.3630 | G Loss: 0.6755\n",
      "Epoch [3570/5000] | D Loss: 1.3147 | G Loss: 0.7590\n",
      "Epoch [3571/5000] | D Loss: 1.3507 | G Loss: 0.7218\n",
      "Epoch [3572/5000] | D Loss: 1.3556 | G Loss: 0.7177\n",
      "Epoch [3573/5000] | D Loss: 1.3409 | G Loss: 0.7613\n",
      "Epoch [3574/5000] | D Loss: 1.3373 | G Loss: 0.7644\n",
      "Epoch [3575/5000] | D Loss: 1.3144 | G Loss: 0.7230\n",
      "Epoch [3576/5000] | D Loss: 1.3295 | G Loss: 0.7487\n",
      "Epoch [3577/5000] | D Loss: 1.3799 | G Loss: 0.7757\n",
      "Epoch [3578/5000] | D Loss: 1.3436 | G Loss: 0.7247\n",
      "Epoch [3579/5000] | D Loss: 1.2631 | G Loss: 0.7802\n",
      "Epoch [3580/5000] | D Loss: 1.3485 | G Loss: 0.7550\n",
      "Epoch [3581/5000] | D Loss: 1.3748 | G Loss: 0.7115\n",
      "Epoch [3582/5000] | D Loss: 1.3350 | G Loss: 0.6905\n",
      "Epoch [3583/5000] | D Loss: 1.3917 | G Loss: 0.6998\n",
      "Epoch [3584/5000] | D Loss: 1.3627 | G Loss: 0.7171\n",
      "Epoch [3585/5000] | D Loss: 1.4304 | G Loss: 0.7158\n",
      "Epoch [3586/5000] | D Loss: 1.3223 | G Loss: 0.7307\n",
      "Epoch [3587/5000] | D Loss: 1.3764 | G Loss: 0.6964\n",
      "Epoch [3588/5000] | D Loss: 1.4096 | G Loss: 0.7379\n",
      "Epoch [3589/5000] | D Loss: 1.3309 | G Loss: 0.7475\n",
      "Epoch [3590/5000] | D Loss: 1.3105 | G Loss: 0.7304\n",
      "Epoch [3591/5000] | D Loss: 1.3821 | G Loss: 0.7015\n",
      "Epoch [3592/5000] | D Loss: 1.3530 | G Loss: 0.7338\n",
      "Epoch [3593/5000] | D Loss: 1.3789 | G Loss: 0.7486\n",
      "Epoch [3594/5000] | D Loss: 1.3863 | G Loss: 0.7139\n",
      "Epoch [3595/5000] | D Loss: 1.3468 | G Loss: 0.7557\n",
      "Epoch [3596/5000] | D Loss: 1.3685 | G Loss: 0.7422\n",
      "Epoch [3597/5000] | D Loss: 1.3653 | G Loss: 0.7472\n",
      "Epoch [3598/5000] | D Loss: 1.3662 | G Loss: 0.6992\n",
      "Epoch [3599/5000] | D Loss: 1.3481 | G Loss: 0.7506\n",
      "Epoch [3600/5000] | D Loss: 1.3195 | G Loss: 0.7641\n",
      "Epoch 3600 FID Score: 72.5202\n",
      "Reducing learning rates to Generator: 4e-05, Discriminator: 4e-05\n",
      "Epoch [3601/5000] | D Loss: 1.3872 | G Loss: 0.7286\n",
      "Epoch [3602/5000] | D Loss: 1.3526 | G Loss: 0.6921\n",
      "Epoch [3603/5000] | D Loss: 1.3669 | G Loss: 0.7514\n",
      "Epoch [3604/5000] | D Loss: 1.3520 | G Loss: 0.7278\n",
      "Epoch [3605/5000] | D Loss: 1.3624 | G Loss: 0.7240\n",
      "Epoch [3606/5000] | D Loss: 1.3629 | G Loss: 0.7259\n",
      "Epoch [3607/5000] | D Loss: 1.3180 | G Loss: 0.7638\n",
      "Epoch [3608/5000] | D Loss: 1.3587 | G Loss: 0.7458\n",
      "Epoch [3609/5000] | D Loss: 1.3798 | G Loss: 0.7151\n",
      "Epoch [3610/5000] | D Loss: 1.3473 | G Loss: 0.7473\n",
      "Epoch [3611/5000] | D Loss: 1.4013 | G Loss: 0.6994\n",
      "Epoch [3612/5000] | D Loss: 1.3074 | G Loss: 0.7435\n",
      "Epoch [3613/5000] | D Loss: 1.3588 | G Loss: 0.6918\n",
      "Epoch [3614/5000] | D Loss: 1.3522 | G Loss: 0.7583\n",
      "Epoch [3615/5000] | D Loss: 1.4141 | G Loss: 0.7084\n",
      "Epoch [3616/5000] | D Loss: 1.3664 | G Loss: 0.7424\n",
      "Epoch [3617/5000] | D Loss: 1.3704 | G Loss: 0.7459\n",
      "Epoch [3618/5000] | D Loss: 1.3303 | G Loss: 0.6957\n",
      "Epoch [3619/5000] | D Loss: 1.3569 | G Loss: 0.7100\n",
      "Epoch [3620/5000] | D Loss: 1.3583 | G Loss: 0.7292\n",
      "Epoch [3621/5000] | D Loss: 1.3724 | G Loss: 0.7306\n",
      "Epoch [3622/5000] | D Loss: 1.3896 | G Loss: 0.7231\n",
      "Epoch [3623/5000] | D Loss: 1.4014 | G Loss: 0.7547\n",
      "Epoch [3624/5000] | D Loss: 1.3454 | G Loss: 0.7684\n",
      "Epoch [3625/5000] | D Loss: 1.3470 | G Loss: 0.6912\n",
      "Epoch [3626/5000] | D Loss: 1.3244 | G Loss: 0.7405\n",
      "Epoch [3627/5000] | D Loss: 1.3554 | G Loss: 0.7525\n",
      "Epoch [3628/5000] | D Loss: 1.3874 | G Loss: 0.6960\n",
      "Epoch [3629/5000] | D Loss: 1.4041 | G Loss: 0.7622\n",
      "Epoch [3630/5000] | D Loss: 1.3939 | G Loss: 0.7159\n",
      "Epoch [3631/5000] | D Loss: 1.3549 | G Loss: 0.7291\n",
      "Epoch [3632/5000] | D Loss: 1.3328 | G Loss: 0.7347\n",
      "Epoch [3633/5000] | D Loss: 1.3438 | G Loss: 0.7038\n",
      "Epoch [3634/5000] | D Loss: 1.3746 | G Loss: 0.7169\n",
      "Epoch [3635/5000] | D Loss: 1.3821 | G Loss: 0.7636\n",
      "Epoch [3636/5000] | D Loss: 1.3152 | G Loss: 0.7361\n",
      "Epoch [3637/5000] | D Loss: 1.3609 | G Loss: 0.7118\n",
      "Epoch [3638/5000] | D Loss: 1.3889 | G Loss: 0.7379\n",
      "Epoch [3639/5000] | D Loss: 1.3795 | G Loss: 0.7449\n",
      "Epoch [3640/5000] | D Loss: 1.3813 | G Loss: 0.6993\n",
      "Epoch [3641/5000] | D Loss: 1.3367 | G Loss: 0.7275\n",
      "Epoch [3642/5000] | D Loss: 1.3904 | G Loss: 0.7133\n",
      "Epoch [3643/5000] | D Loss: 1.3620 | G Loss: 0.7300\n",
      "Epoch [3644/5000] | D Loss: 1.3325 | G Loss: 0.7167\n",
      "Epoch [3645/5000] | D Loss: 1.3628 | G Loss: 0.7588\n",
      "Epoch [3646/5000] | D Loss: 1.3981 | G Loss: 0.7132\n",
      "Epoch [3647/5000] | D Loss: 1.3801 | G Loss: 0.7233\n",
      "Epoch [3648/5000] | D Loss: 1.3673 | G Loss: 0.7456\n",
      "Epoch [3649/5000] | D Loss: 1.3995 | G Loss: 0.6834\n",
      "Epoch [3650/5000] | D Loss: 1.3388 | G Loss: 0.7617\n",
      "Epoch [3651/5000] | D Loss: 1.3535 | G Loss: 0.6856\n",
      "Epoch [3652/5000] | D Loss: 1.3494 | G Loss: 0.7083\n",
      "Epoch [3653/5000] | D Loss: 1.3322 | G Loss: 0.6991\n",
      "Epoch [3654/5000] | D Loss: 1.3939 | G Loss: 0.7399\n",
      "Epoch [3655/5000] | D Loss: 1.4137 | G Loss: 0.7310\n",
      "Epoch [3656/5000] | D Loss: 1.4276 | G Loss: 0.7159\n",
      "Epoch [3657/5000] | D Loss: 1.3768 | G Loss: 0.7418\n",
      "Epoch [3658/5000] | D Loss: 1.4034 | G Loss: 0.6916\n",
      "Epoch [3659/5000] | D Loss: 1.4072 | G Loss: 0.7627\n",
      "Epoch [3660/5000] | D Loss: 1.3319 | G Loss: 0.7251\n",
      "Epoch [3661/5000] | D Loss: 1.3423 | G Loss: 0.7170\n",
      "Epoch [3662/5000] | D Loss: 1.3435 | G Loss: 0.6942\n",
      "Epoch [3663/5000] | D Loss: 1.3351 | G Loss: 0.7179\n",
      "Epoch [3664/5000] | D Loss: 1.3384 | G Loss: 0.7454\n",
      "Epoch [3665/5000] | D Loss: 1.3695 | G Loss: 0.7608\n",
      "Epoch [3666/5000] | D Loss: 1.3776 | G Loss: 0.7175\n",
      "Epoch [3667/5000] | D Loss: 1.3794 | G Loss: 0.6926\n",
      "Epoch [3668/5000] | D Loss: 1.3259 | G Loss: 0.7304\n",
      "Epoch [3669/5000] | D Loss: 1.3524 | G Loss: 0.7033\n",
      "Epoch [3670/5000] | D Loss: 1.2796 | G Loss: 0.7723\n",
      "Epoch [3671/5000] | D Loss: 1.3326 | G Loss: 0.7321\n",
      "Epoch [3672/5000] | D Loss: 1.3511 | G Loss: 0.7556\n",
      "Epoch [3673/5000] | D Loss: 1.3444 | G Loss: 0.7447\n",
      "Epoch [3674/5000] | D Loss: 1.3650 | G Loss: 0.7424\n",
      "Epoch [3675/5000] | D Loss: 1.3725 | G Loss: 0.7479\n",
      "Epoch [3676/5000] | D Loss: 1.3632 | G Loss: 0.7065\n",
      "Epoch [3677/5000] | D Loss: 1.3575 | G Loss: 0.7263\n",
      "Epoch [3678/5000] | D Loss: 1.3700 | G Loss: 0.6833\n",
      "Epoch [3679/5000] | D Loss: 1.3856 | G Loss: 0.7151\n",
      "Epoch [3680/5000] | D Loss: 1.3727 | G Loss: 0.7171\n",
      "Epoch [3681/5000] | D Loss: 1.3150 | G Loss: 0.7159\n",
      "Epoch [3682/5000] | D Loss: 1.3288 | G Loss: 0.7264\n",
      "Epoch [3683/5000] | D Loss: 1.3500 | G Loss: 0.7262\n",
      "Epoch [3684/5000] | D Loss: 1.3537 | G Loss: 0.7349\n",
      "Epoch [3685/5000] | D Loss: 1.3833 | G Loss: 0.7390\n",
      "Epoch [3686/5000] | D Loss: 1.3285 | G Loss: 0.7694\n",
      "Epoch [3687/5000] | D Loss: 1.3436 | G Loss: 0.6872\n",
      "Epoch [3688/5000] | D Loss: 1.3287 | G Loss: 0.6969\n",
      "Epoch [3689/5000] | D Loss: 1.3472 | G Loss: 0.7096\n",
      "Epoch [3690/5000] | D Loss: 1.3925 | G Loss: 0.7083\n",
      "Epoch [3691/5000] | D Loss: 1.3817 | G Loss: 0.7307\n",
      "Epoch [3692/5000] | D Loss: 1.3265 | G Loss: 0.7134\n",
      "Epoch [3693/5000] | D Loss: 1.3660 | G Loss: 0.7112\n",
      "Epoch [3694/5000] | D Loss: 1.3580 | G Loss: 0.7248\n",
      "Epoch [3695/5000] | D Loss: 1.3304 | G Loss: 0.7187\n",
      "Epoch [3696/5000] | D Loss: 1.3286 | G Loss: 0.7274\n",
      "Epoch [3697/5000] | D Loss: 1.3418 | G Loss: 0.7322\n",
      "Epoch [3698/5000] | D Loss: 1.3940 | G Loss: 0.6880\n",
      "Epoch [3699/5000] | D Loss: 1.3884 | G Loss: 0.7414\n",
      "Epoch [3700/5000] | D Loss: 1.3859 | G Loss: 0.7293\n",
      "Epoch 3700 FID Score: 69.3440\n",
      "Saved improved model at Epoch 3700 with FID 69.3440\n",
      "Epoch [3701/5000] | D Loss: 1.3762 | G Loss: 0.7227\n",
      "Epoch [3702/5000] | D Loss: 1.3420 | G Loss: 0.7156\n",
      "Epoch [3703/5000] | D Loss: 1.3757 | G Loss: 0.7358\n",
      "Epoch [3704/5000] | D Loss: 1.3605 | G Loss: 0.7104\n",
      "Epoch [3705/5000] | D Loss: 1.3632 | G Loss: 0.7112\n",
      "Epoch [3706/5000] | D Loss: 1.3608 | G Loss: 0.7406\n",
      "Epoch [3707/5000] | D Loss: 1.4316 | G Loss: 0.7180\n",
      "Epoch [3708/5000] | D Loss: 1.3692 | G Loss: 0.7391\n",
      "Epoch [3709/5000] | D Loss: 1.3497 | G Loss: 0.6982\n",
      "Epoch [3710/5000] | D Loss: 1.3539 | G Loss: 0.7493\n",
      "Epoch [3711/5000] | D Loss: 1.3495 | G Loss: 0.7491\n",
      "Epoch [3712/5000] | D Loss: 1.3838 | G Loss: 0.7285\n",
      "Epoch [3713/5000] | D Loss: 1.3252 | G Loss: 0.7140\n",
      "Epoch [3714/5000] | D Loss: 1.3547 | G Loss: 0.7370\n",
      "Epoch [3715/5000] | D Loss: 1.3734 | G Loss: 0.7565\n",
      "Epoch [3716/5000] | D Loss: 1.3344 | G Loss: 0.7314\n",
      "Epoch [3717/5000] | D Loss: 1.3204 | G Loss: 0.7122\n",
      "Epoch [3718/5000] | D Loss: 1.3935 | G Loss: 0.7298\n",
      "Epoch [3719/5000] | D Loss: 1.3292 | G Loss: 0.7199\n",
      "Epoch [3720/5000] | D Loss: 1.3523 | G Loss: 0.7350\n",
      "Epoch [3721/5000] | D Loss: 1.3429 | G Loss: 0.7337\n",
      "Epoch [3722/5000] | D Loss: 1.3391 | G Loss: 0.7187\n",
      "Epoch [3723/5000] | D Loss: 1.3033 | G Loss: 0.7379\n",
      "Epoch [3724/5000] | D Loss: 1.3059 | G Loss: 0.7547\n",
      "Epoch [3725/5000] | D Loss: 1.3742 | G Loss: 0.7275\n",
      "Epoch [3726/5000] | D Loss: 1.3869 | G Loss: 0.7352\n",
      "Epoch [3727/5000] | D Loss: 1.3583 | G Loss: 0.7522\n",
      "Epoch [3728/5000] | D Loss: 1.3260 | G Loss: 0.7081\n",
      "Epoch [3729/5000] | D Loss: 1.3879 | G Loss: 0.7154\n",
      "Epoch [3730/5000] | D Loss: 1.3850 | G Loss: 0.7313\n",
      "Epoch [3731/5000] | D Loss: 1.3932 | G Loss: 0.7107\n",
      "Epoch [3732/5000] | D Loss: 1.3240 | G Loss: 0.7120\n",
      "Epoch [3733/5000] | D Loss: 1.3337 | G Loss: 0.7201\n",
      "Epoch [3734/5000] | D Loss: 1.3279 | G Loss: 0.7328\n",
      "Epoch [3735/5000] | D Loss: 1.3298 | G Loss: 0.7094\n",
      "Epoch [3736/5000] | D Loss: 1.3459 | G Loss: 0.7239\n",
      "Epoch [3737/5000] | D Loss: 1.3207 | G Loss: 0.7195\n",
      "Epoch [3738/5000] | D Loss: 1.3470 | G Loss: 0.7308\n",
      "Epoch [3739/5000] | D Loss: 1.3174 | G Loss: 0.7304\n",
      "Epoch [3740/5000] | D Loss: 1.3370 | G Loss: 0.7280\n",
      "Epoch [3741/5000] | D Loss: 1.3927 | G Loss: 0.7299\n",
      "Epoch [3742/5000] | D Loss: 1.3710 | G Loss: 0.7068\n",
      "Epoch [3743/5000] | D Loss: 1.3481 | G Loss: 0.7467\n",
      "Epoch [3744/5000] | D Loss: 1.3364 | G Loss: 0.7236\n",
      "Epoch [3745/5000] | D Loss: 1.3766 | G Loss: 0.7110\n",
      "Epoch [3746/5000] | D Loss: 1.3649 | G Loss: 0.7619\n",
      "Epoch [3747/5000] | D Loss: 1.3392 | G Loss: 0.7229\n",
      "Epoch [3748/5000] | D Loss: 1.3442 | G Loss: 0.7250\n",
      "Epoch [3749/5000] | D Loss: 1.3553 | G Loss: 0.7275\n",
      "Epoch [3750/5000] | D Loss: 1.3562 | G Loss: 0.6953\n",
      "Epoch [3751/5000] | D Loss: 1.3838 | G Loss: 0.7211\n",
      "Epoch [3752/5000] | D Loss: 1.3566 | G Loss: 0.7218\n",
      "Epoch [3753/5000] | D Loss: 1.3411 | G Loss: 0.7103\n",
      "Epoch [3754/5000] | D Loss: 1.3349 | G Loss: 0.7235\n",
      "Epoch [3755/5000] | D Loss: 1.3631 | G Loss: 0.7364\n",
      "Epoch [3756/5000] | D Loss: 1.3580 | G Loss: 0.7251\n",
      "Epoch [3757/5000] | D Loss: 1.3323 | G Loss: 0.7319\n",
      "Epoch [3758/5000] | D Loss: 1.3439 | G Loss: 0.7242\n",
      "Epoch [3759/5000] | D Loss: 1.3371 | G Loss: 0.7429\n",
      "Epoch [3760/5000] | D Loss: 1.3719 | G Loss: 0.7371\n",
      "Epoch [3761/5000] | D Loss: 1.3593 | G Loss: 0.7422\n",
      "Epoch [3762/5000] | D Loss: 1.3857 | G Loss: 0.6941\n",
      "Epoch [3763/5000] | D Loss: 1.3535 | G Loss: 0.7459\n",
      "Epoch [3764/5000] | D Loss: 1.3843 | G Loss: 0.7185\n",
      "Epoch [3765/5000] | D Loss: 1.3626 | G Loss: 0.7425\n",
      "Epoch [3766/5000] | D Loss: 1.3439 | G Loss: 0.7252\n",
      "Epoch [3767/5000] | D Loss: 1.3615 | G Loss: 0.7462\n",
      "Epoch [3768/5000] | D Loss: 1.3335 | G Loss: 0.7643\n",
      "Epoch [3769/5000] | D Loss: 1.3588 | G Loss: 0.7290\n",
      "Epoch [3770/5000] | D Loss: 1.3831 | G Loss: 0.7287\n",
      "Epoch [3771/5000] | D Loss: 1.3399 | G Loss: 0.7037\n",
      "Epoch [3772/5000] | D Loss: 1.3455 | G Loss: 0.7397\n",
      "Epoch [3773/5000] | D Loss: 1.3579 | G Loss: 0.7190\n",
      "Epoch [3774/5000] | D Loss: 1.3364 | G Loss: 0.6953\n",
      "Epoch [3775/5000] | D Loss: 1.3776 | G Loss: 0.7124\n",
      "Epoch [3776/5000] | D Loss: 1.3198 | G Loss: 0.7705\n",
      "Epoch [3777/5000] | D Loss: 1.3323 | G Loss: 0.7382\n",
      "Epoch [3778/5000] | D Loss: 1.3513 | G Loss: 0.7176\n",
      "Epoch [3779/5000] | D Loss: 1.3427 | G Loss: 0.6939\n",
      "Epoch [3780/5000] | D Loss: 1.3605 | G Loss: 0.7331\n",
      "Epoch [3781/5000] | D Loss: 1.3635 | G Loss: 0.7300\n",
      "Epoch [3782/5000] | D Loss: 1.3360 | G Loss: 0.7258\n",
      "Epoch [3783/5000] | D Loss: 1.3712 | G Loss: 0.7335\n",
      "Epoch [3784/5000] | D Loss: 1.3830 | G Loss: 0.7025\n",
      "Epoch [3785/5000] | D Loss: 1.4097 | G Loss: 0.7199\n",
      "Epoch [3786/5000] | D Loss: 1.3709 | G Loss: 0.6888\n",
      "Epoch [3787/5000] | D Loss: 1.3755 | G Loss: 0.7332\n",
      "Epoch [3788/5000] | D Loss: 1.3504 | G Loss: 0.7212\n",
      "Epoch [3789/5000] | D Loss: 1.3518 | G Loss: 0.7053\n",
      "Epoch [3790/5000] | D Loss: 1.3719 | G Loss: 0.7383\n",
      "Epoch [3791/5000] | D Loss: 1.3391 | G Loss: 0.7430\n",
      "Epoch [3792/5000] | D Loss: 1.3758 | G Loss: 0.7335\n",
      "Epoch [3793/5000] | D Loss: 1.3720 | G Loss: 0.7220\n",
      "Epoch [3794/5000] | D Loss: 1.3770 | G Loss: 0.7238\n",
      "Epoch [3795/5000] | D Loss: 1.4204 | G Loss: 0.7303\n",
      "Epoch [3796/5000] | D Loss: 1.3647 | G Loss: 0.7106\n",
      "Epoch [3797/5000] | D Loss: 1.4028 | G Loss: 0.7205\n",
      "Epoch [3798/5000] | D Loss: 1.3625 | G Loss: 0.7019\n",
      "Epoch [3799/5000] | D Loss: 1.3566 | G Loss: 0.7324\n",
      "Epoch [3800/5000] | D Loss: 1.3734 | G Loss: 0.7278\n",
      "Epoch 3800 FID Score: 69.9609\n",
      "Epoch [3801/5000] | D Loss: 1.3655 | G Loss: 0.7424\n",
      "Epoch [3802/5000] | D Loss: 1.3599 | G Loss: 0.7474\n",
      "Epoch [3803/5000] | D Loss: 1.3828 | G Loss: 0.7544\n",
      "Epoch [3804/5000] | D Loss: 1.3511 | G Loss: 0.7071\n",
      "Epoch [3805/5000] | D Loss: 1.3916 | G Loss: 0.7191\n",
      "Epoch [3806/5000] | D Loss: 1.3637 | G Loss: 0.7168\n",
      "Epoch [3807/5000] | D Loss: 1.3296 | G Loss: 0.7188\n",
      "Epoch [3808/5000] | D Loss: 1.3455 | G Loss: 0.7327\n",
      "Epoch [3809/5000] | D Loss: 1.3490 | G Loss: 0.7206\n",
      "Epoch [3810/5000] | D Loss: 1.3668 | G Loss: 0.7129\n",
      "Epoch [3811/5000] | D Loss: 1.4150 | G Loss: 0.7036\n",
      "Epoch [3812/5000] | D Loss: 1.3549 | G Loss: 0.7645\n",
      "Epoch [3813/5000] | D Loss: 1.3556 | G Loss: 0.7440\n",
      "Epoch [3814/5000] | D Loss: 1.3334 | G Loss: 0.7129\n",
      "Epoch [3815/5000] | D Loss: 1.3528 | G Loss: 0.7058\n",
      "Epoch [3816/5000] | D Loss: 1.3709 | G Loss: 0.7251\n",
      "Epoch [3817/5000] | D Loss: 1.3827 | G Loss: 0.7570\n",
      "Epoch [3818/5000] | D Loss: 1.3355 | G Loss: 0.7457\n",
      "Epoch [3819/5000] | D Loss: 1.3486 | G Loss: 0.7181\n",
      "Epoch [3820/5000] | D Loss: 1.3454 | G Loss: 0.7626\n",
      "Epoch [3821/5000] | D Loss: 1.3767 | G Loss: 0.6855\n",
      "Epoch [3822/5000] | D Loss: 1.3483 | G Loss: 0.7530\n",
      "Epoch [3823/5000] | D Loss: 1.3602 | G Loss: 0.7236\n",
      "Epoch [3824/5000] | D Loss: 1.3554 | G Loss: 0.6893\n",
      "Epoch [3825/5000] | D Loss: 1.3828 | G Loss: 0.7070\n",
      "Epoch [3826/5000] | D Loss: 1.3900 | G Loss: 0.7274\n",
      "Epoch [3827/5000] | D Loss: 1.3611 | G Loss: 0.7592\n",
      "Epoch [3828/5000] | D Loss: 1.3595 | G Loss: 0.7146\n",
      "Epoch [3829/5000] | D Loss: 1.3530 | G Loss: 0.7131\n",
      "Epoch [3830/5000] | D Loss: 1.3758 | G Loss: 0.7561\n",
      "Epoch [3831/5000] | D Loss: 1.3167 | G Loss: 0.7117\n",
      "Epoch [3832/5000] | D Loss: 1.3766 | G Loss: 0.7469\n",
      "Epoch [3833/5000] | D Loss: 1.3560 | G Loss: 0.7475\n",
      "Epoch [3834/5000] | D Loss: 1.3589 | G Loss: 0.7385\n",
      "Epoch [3835/5000] | D Loss: 1.3599 | G Loss: 0.7194\n",
      "Epoch [3836/5000] | D Loss: 1.3774 | G Loss: 0.7408\n",
      "Epoch [3837/5000] | D Loss: 1.3715 | G Loss: 0.7304\n",
      "Epoch [3838/5000] | D Loss: 1.3867 | G Loss: 0.7278\n",
      "Epoch [3839/5000] | D Loss: 1.3696 | G Loss: 0.7077\n",
      "Epoch [3840/5000] | D Loss: 1.3389 | G Loss: 0.7504\n",
      "Epoch [3841/5000] | D Loss: 1.3824 | G Loss: 0.7355\n",
      "Epoch [3842/5000] | D Loss: 1.3741 | G Loss: 0.7381\n",
      "Epoch [3843/5000] | D Loss: 1.3539 | G Loss: 0.7173\n",
      "Epoch [3844/5000] | D Loss: 1.3684 | G Loss: 0.7426\n",
      "Epoch [3845/5000] | D Loss: 1.3753 | G Loss: 0.7016\n",
      "Epoch [3846/5000] | D Loss: 1.3416 | G Loss: 0.7517\n",
      "Epoch [3847/5000] | D Loss: 1.3868 | G Loss: 0.7004\n",
      "Epoch [3848/5000] | D Loss: 1.3497 | G Loss: 0.7357\n",
      "Epoch [3849/5000] | D Loss: 1.3721 | G Loss: 0.7571\n",
      "Epoch [3850/5000] | D Loss: 1.3848 | G Loss: 0.7373\n",
      "Epoch [3851/5000] | D Loss: 1.3501 | G Loss: 0.6865\n",
      "Epoch [3852/5000] | D Loss: 1.3666 | G Loss: 0.7016\n",
      "Epoch [3853/5000] | D Loss: 1.3440 | G Loss: 0.7263\n",
      "Epoch [3854/5000] | D Loss: 1.3482 | G Loss: 0.7327\n",
      "Epoch [3855/5000] | D Loss: 1.3529 | G Loss: 0.7335\n",
      "Epoch [3856/5000] | D Loss: 1.3513 | G Loss: 0.6909\n",
      "Epoch [3857/5000] | D Loss: 1.3677 | G Loss: 0.7122\n",
      "Epoch [3858/5000] | D Loss: 1.3532 | G Loss: 0.7152\n",
      "Epoch [3859/5000] | D Loss: 1.3532 | G Loss: 0.7239\n",
      "Epoch [3860/5000] | D Loss: 1.3514 | G Loss: 0.6788\n",
      "Epoch [3861/5000] | D Loss: 1.3504 | G Loss: 0.7219\n",
      "Epoch [3862/5000] | D Loss: 1.3713 | G Loss: 0.6972\n",
      "Epoch [3863/5000] | D Loss: 1.3365 | G Loss: 0.7365\n",
      "Epoch [3864/5000] | D Loss: 1.3355 | G Loss: 0.7304\n",
      "Epoch [3865/5000] | D Loss: 1.3615 | G Loss: 0.7128\n",
      "Epoch [3866/5000] | D Loss: 1.3175 | G Loss: 0.6721\n",
      "Epoch [3867/5000] | D Loss: 1.3358 | G Loss: 0.7291\n",
      "Epoch [3868/5000] | D Loss: 1.3458 | G Loss: 0.7029\n",
      "Epoch [3869/5000] | D Loss: 1.3476 | G Loss: 0.7252\n",
      "Epoch [3870/5000] | D Loss: 1.3594 | G Loss: 0.7207\n",
      "Epoch [3871/5000] | D Loss: 1.3460 | G Loss: 0.7560\n",
      "Epoch [3872/5000] | D Loss: 1.3616 | G Loss: 0.7301\n",
      "Epoch [3873/5000] | D Loss: 1.3470 | G Loss: 0.7011\n",
      "Epoch [3874/5000] | D Loss: 1.3161 | G Loss: 0.7398\n",
      "Epoch [3875/5000] | D Loss: 1.3709 | G Loss: 0.7413\n",
      "Epoch [3876/5000] | D Loss: 1.3446 | G Loss: 0.7480\n",
      "Epoch [3877/5000] | D Loss: 1.3235 | G Loss: 0.7335\n",
      "Epoch [3878/5000] | D Loss: 1.3671 | G Loss: 0.7031\n",
      "Epoch [3879/5000] | D Loss: 1.4114 | G Loss: 0.7330\n",
      "Epoch [3880/5000] | D Loss: 1.3857 | G Loss: 0.7253\n",
      "Epoch [3881/5000] | D Loss: 1.3633 | G Loss: 0.7258\n",
      "Epoch [3882/5000] | D Loss: 1.3595 | G Loss: 0.7501\n",
      "Epoch [3883/5000] | D Loss: 1.3728 | G Loss: 0.7318\n",
      "Epoch [3884/5000] | D Loss: 1.3798 | G Loss: 0.7096\n",
      "Epoch [3885/5000] | D Loss: 1.3552 | G Loss: 0.7592\n",
      "Epoch [3886/5000] | D Loss: 1.3889 | G Loss: 0.7221\n",
      "Epoch [3887/5000] | D Loss: 1.3985 | G Loss: 0.7370\n",
      "Epoch [3888/5000] | D Loss: 1.3641 | G Loss: 0.7527\n",
      "Epoch [3889/5000] | D Loss: 1.3579 | G Loss: 0.7310\n",
      "Epoch [3890/5000] | D Loss: 1.4213 | G Loss: 0.7234\n",
      "Epoch [3891/5000] | D Loss: 1.3595 | G Loss: 0.7263\n",
      "Epoch [3892/5000] | D Loss: 1.3483 | G Loss: 0.7415\n",
      "Epoch [3893/5000] | D Loss: 1.3985 | G Loss: 0.7366\n",
      "Epoch [3894/5000] | D Loss: 1.3792 | G Loss: 0.7306\n",
      "Epoch [3895/5000] | D Loss: 1.3921 | G Loss: 0.7532\n",
      "Epoch [3896/5000] | D Loss: 1.3663 | G Loss: 0.7307\n",
      "Epoch [3897/5000] | D Loss: 1.3723 | G Loss: 0.7034\n",
      "Epoch [3898/5000] | D Loss: 1.3151 | G Loss: 0.7298\n",
      "Epoch [3899/5000] | D Loss: 1.3996 | G Loss: 0.7004\n",
      "Epoch [3900/5000] | D Loss: 1.3906 | G Loss: 0.7159\n",
      "Epoch 3900 FID Score: 67.2578\n",
      "Saved improved model at Epoch 3900 with FID 67.2578\n",
      "Epoch [3901/5000] | D Loss: 1.3495 | G Loss: 0.7246\n",
      "Epoch [3902/5000] | D Loss: 1.3682 | G Loss: 0.7331\n",
      "Epoch [3903/5000] | D Loss: 1.3325 | G Loss: 0.7650\n",
      "Epoch [3904/5000] | D Loss: 1.3699 | G Loss: 0.7337\n",
      "Epoch [3905/5000] | D Loss: 1.3854 | G Loss: 0.7043\n",
      "Epoch [3906/5000] | D Loss: 1.3434 | G Loss: 0.7502\n",
      "Epoch [3907/5000] | D Loss: 1.3750 | G Loss: 0.7217\n",
      "Epoch [3908/5000] | D Loss: 1.3646 | G Loss: 0.7503\n",
      "Epoch [3909/5000] | D Loss: 1.3537 | G Loss: 0.7428\n",
      "Epoch [3910/5000] | D Loss: 1.3439 | G Loss: 0.7262\n",
      "Epoch [3911/5000] | D Loss: 1.3333 | G Loss: 0.7456\n",
      "Epoch [3912/5000] | D Loss: 1.3331 | G Loss: 0.7214\n",
      "Epoch [3913/5000] | D Loss: 1.3668 | G Loss: 0.7306\n",
      "Epoch [3914/5000] | D Loss: 1.3736 | G Loss: 0.6922\n",
      "Epoch [3915/5000] | D Loss: 1.3511 | G Loss: 0.7276\n",
      "Epoch [3916/5000] | D Loss: 1.3465 | G Loss: 0.7401\n",
      "Epoch [3917/5000] | D Loss: 1.3762 | G Loss: 0.6970\n",
      "Epoch [3918/5000] | D Loss: 1.3690 | G Loss: 0.7341\n",
      "Epoch [3919/5000] | D Loss: 1.3710 | G Loss: 0.7352\n",
      "Epoch [3920/5000] | D Loss: 1.3508 | G Loss: 0.7370\n",
      "Epoch [3921/5000] | D Loss: 1.3833 | G Loss: 0.7006\n",
      "Epoch [3922/5000] | D Loss: 1.4031 | G Loss: 0.7236\n",
      "Epoch [3923/5000] | D Loss: 1.3943 | G Loss: 0.7229\n",
      "Epoch [3924/5000] | D Loss: 1.3410 | G Loss: 0.7570\n",
      "Epoch [3925/5000] | D Loss: 1.3436 | G Loss: 0.7543\n",
      "Epoch [3926/5000] | D Loss: 1.3506 | G Loss: 0.7260\n",
      "Epoch [3927/5000] | D Loss: 1.3061 | G Loss: 0.7142\n",
      "Epoch [3928/5000] | D Loss: 1.3881 | G Loss: 0.7189\n",
      "Epoch [3929/5000] | D Loss: 1.3457 | G Loss: 0.7272\n",
      "Epoch [3930/5000] | D Loss: 1.3937 | G Loss: 0.7262\n",
      "Epoch [3931/5000] | D Loss: 1.3444 | G Loss: 0.7157\n",
      "Epoch [3932/5000] | D Loss: 1.3548 | G Loss: 0.7145\n",
      "Epoch [3933/5000] | D Loss: 1.3592 | G Loss: 0.7408\n",
      "Epoch [3934/5000] | D Loss: 1.3510 | G Loss: 0.7384\n",
      "Epoch [3935/5000] | D Loss: 1.3441 | G Loss: 0.7381\n",
      "Epoch [3936/5000] | D Loss: 1.3726 | G Loss: 0.7075\n",
      "Epoch [3937/5000] | D Loss: 1.3566 | G Loss: 0.7431\n",
      "Epoch [3938/5000] | D Loss: 1.3549 | G Loss: 0.7468\n",
      "Epoch [3939/5000] | D Loss: 1.3835 | G Loss: 0.7172\n",
      "Epoch [3940/5000] | D Loss: 1.3620 | G Loss: 0.7311\n",
      "Epoch [3941/5000] | D Loss: 1.3920 | G Loss: 0.6997\n",
      "Epoch [3942/5000] | D Loss: 1.3385 | G Loss: 0.7155\n",
      "Epoch [3943/5000] | D Loss: 1.3759 | G Loss: 0.7173\n",
      "Epoch [3944/5000] | D Loss: 1.3377 | G Loss: 0.7138\n",
      "Epoch [3945/5000] | D Loss: 1.3990 | G Loss: 0.7135\n",
      "Epoch [3946/5000] | D Loss: 1.3608 | G Loss: 0.7399\n",
      "Epoch [3947/5000] | D Loss: 1.3462 | G Loss: 0.7205\n",
      "Epoch [3948/5000] | D Loss: 1.3732 | G Loss: 0.7222\n",
      "Epoch [3949/5000] | D Loss: 1.3856 | G Loss: 0.7316\n",
      "Epoch [3950/5000] | D Loss: 1.3495 | G Loss: 0.7263\n",
      "Epoch [3951/5000] | D Loss: 1.3913 | G Loss: 0.7474\n",
      "Epoch [3952/5000] | D Loss: 1.3674 | G Loss: 0.7645\n",
      "Epoch [3953/5000] | D Loss: 1.3756 | G Loss: 0.7511\n",
      "Epoch [3954/5000] | D Loss: 1.3222 | G Loss: 0.7428\n",
      "Epoch [3955/5000] | D Loss: 1.4073 | G Loss: 0.7528\n",
      "Epoch [3956/5000] | D Loss: 1.3441 | G Loss: 0.6999\n",
      "Epoch [3957/5000] | D Loss: 1.3401 | G Loss: 0.7018\n",
      "Epoch [3958/5000] | D Loss: 1.3553 | G Loss: 0.7516\n",
      "Epoch [3959/5000] | D Loss: 1.3297 | G Loss: 0.7180\n",
      "Epoch [3960/5000] | D Loss: 1.3587 | G Loss: 0.7558\n",
      "Epoch [3961/5000] | D Loss: 1.3919 | G Loss: 0.7269\n",
      "Epoch [3962/5000] | D Loss: 1.3823 | G Loss: 0.7057\n",
      "Epoch [3963/5000] | D Loss: 1.2842 | G Loss: 0.7585\n",
      "Epoch [3964/5000] | D Loss: 1.3672 | G Loss: 0.7359\n",
      "Epoch [3965/5000] | D Loss: 1.3493 | G Loss: 0.7143\n",
      "Epoch [3966/5000] | D Loss: 1.2944 | G Loss: 0.7543\n",
      "Epoch [3967/5000] | D Loss: 1.3871 | G Loss: 0.7202\n",
      "Epoch [3968/5000] | D Loss: 1.3560 | G Loss: 0.7302\n",
      "Epoch [3969/5000] | D Loss: 1.3048 | G Loss: 0.7117\n",
      "Epoch [3970/5000] | D Loss: 1.3866 | G Loss: 0.7176\n",
      "Epoch [3971/5000] | D Loss: 1.3710 | G Loss: 0.7125\n",
      "Epoch [3972/5000] | D Loss: 1.4051 | G Loss: 0.7197\n",
      "Epoch [3973/5000] | D Loss: 1.3240 | G Loss: 0.7243\n",
      "Epoch [3974/5000] | D Loss: 1.3451 | G Loss: 0.7231\n",
      "Epoch [3975/5000] | D Loss: 1.3592 | G Loss: 0.7288\n",
      "Epoch [3976/5000] | D Loss: 1.3525 | G Loss: 0.7660\n",
      "Epoch [3977/5000] | D Loss: 1.3942 | G Loss: 0.7167\n",
      "Epoch [3978/5000] | D Loss: 1.3628 | G Loss: 0.7064\n",
      "Epoch [3979/5000] | D Loss: 1.3543 | G Loss: 0.7335\n",
      "Epoch [3980/5000] | D Loss: 1.3569 | G Loss: 0.7266\n",
      "Epoch [3981/5000] | D Loss: 1.3818 | G Loss: 0.7043\n",
      "Epoch [3982/5000] | D Loss: 1.3514 | G Loss: 0.7376\n",
      "Epoch [3983/5000] | D Loss: 1.2902 | G Loss: 0.7648\n",
      "Epoch [3984/5000] | D Loss: 1.3113 | G Loss: 0.7133\n",
      "Epoch [3985/5000] | D Loss: 1.3717 | G Loss: 0.7424\n",
      "Epoch [3986/5000] | D Loss: 1.3246 | G Loss: 0.7547\n",
      "Epoch [3987/5000] | D Loss: 1.4086 | G Loss: 0.6958\n",
      "Epoch [3988/5000] | D Loss: 1.3715 | G Loss: 0.7097\n",
      "Epoch [3989/5000] | D Loss: 1.3751 | G Loss: 0.7094\n",
      "Epoch [3990/5000] | D Loss: 1.3856 | G Loss: 0.7125\n",
      "Epoch [3991/5000] | D Loss: 1.3522 | G Loss: 0.7334\n",
      "Epoch [3992/5000] | D Loss: 1.4083 | G Loss: 0.7755\n",
      "Epoch [3993/5000] | D Loss: 1.3705 | G Loss: 0.7036\n",
      "Epoch [3994/5000] | D Loss: 1.3639 | G Loss: 0.7279\n",
      "Epoch [3995/5000] | D Loss: 1.3634 | G Loss: 0.7192\n",
      "Epoch [3996/5000] | D Loss: 1.3850 | G Loss: 0.7268\n",
      "Epoch [3997/5000] | D Loss: 1.3505 | G Loss: 0.7382\n",
      "Epoch [3998/5000] | D Loss: 1.3352 | G Loss: 0.7199\n",
      "Epoch [3999/5000] | D Loss: 1.3726 | G Loss: 0.7309\n",
      "Epoch [4000/5000] | D Loss: 1.3318 | G Loss: 0.7521\n",
      "Epoch 4000 FID Score: 66.9225\n",
      "Saved improved model at Epoch 4000 with FID 66.9225\n",
      "Epoch [4001/5000] | D Loss: 1.3202 | G Loss: 0.7118\n",
      "Epoch [4002/5000] | D Loss: 1.3696 | G Loss: 0.7012\n",
      "Epoch [4003/5000] | D Loss: 1.3593 | G Loss: 0.7466\n",
      "Epoch [4004/5000] | D Loss: 1.3772 | G Loss: 0.7268\n",
      "Epoch [4005/5000] | D Loss: 1.3846 | G Loss: 0.7265\n",
      "Epoch [4006/5000] | D Loss: 1.3702 | G Loss: 0.7131\n",
      "Epoch [4007/5000] | D Loss: 1.3691 | G Loss: 0.7498\n",
      "Epoch [4008/5000] | D Loss: 1.3917 | G Loss: 0.7551\n",
      "Epoch [4009/5000] | D Loss: 1.3510 | G Loss: 0.7037\n",
      "Epoch [4010/5000] | D Loss: 1.3783 | G Loss: 0.7189\n",
      "Epoch [4011/5000] | D Loss: 1.3826 | G Loss: 0.7529\n",
      "Epoch [4012/5000] | D Loss: 1.3128 | G Loss: 0.7253\n",
      "Epoch [4013/5000] | D Loss: 1.3406 | G Loss: 0.7510\n",
      "Epoch [4014/5000] | D Loss: 1.3361 | G Loss: 0.7137\n",
      "Epoch [4015/5000] | D Loss: 1.3511 | G Loss: 0.6994\n",
      "Epoch [4016/5000] | D Loss: 1.3803 | G Loss: 0.7096\n",
      "Epoch [4017/5000] | D Loss: 1.3769 | G Loss: 0.7079\n",
      "Epoch [4018/5000] | D Loss: 1.3532 | G Loss: 0.7049\n",
      "Epoch [4019/5000] | D Loss: 1.3901 | G Loss: 0.7308\n",
      "Epoch [4020/5000] | D Loss: 1.3267 | G Loss: 0.7436\n",
      "Epoch [4021/5000] | D Loss: 1.3565 | G Loss: 0.7591\n",
      "Epoch [4022/5000] | D Loss: 1.3490 | G Loss: 0.7462\n",
      "Epoch [4023/5000] | D Loss: 1.3533 | G Loss: 0.7218\n",
      "Epoch [4024/5000] | D Loss: 1.3761 | G Loss: 0.7502\n",
      "Epoch [4025/5000] | D Loss: 1.3820 | G Loss: 0.7193\n",
      "Epoch [4026/5000] | D Loss: 1.3674 | G Loss: 0.7104\n",
      "Epoch [4027/5000] | D Loss: 1.3490 | G Loss: 0.7276\n",
      "Epoch [4028/5000] | D Loss: 1.3479 | G Loss: 0.7458\n",
      "Epoch [4029/5000] | D Loss: 1.3618 | G Loss: 0.7110\n",
      "Epoch [4030/5000] | D Loss: 1.3456 | G Loss: 0.7329\n",
      "Epoch [4031/5000] | D Loss: 1.3588 | G Loss: 0.7234\n",
      "Epoch [4032/5000] | D Loss: 1.3589 | G Loss: 0.7276\n",
      "Epoch [4033/5000] | D Loss: 1.3329 | G Loss: 0.7179\n",
      "Epoch [4034/5000] | D Loss: 1.3860 | G Loss: 0.7324\n",
      "Epoch [4035/5000] | D Loss: 1.3525 | G Loss: 0.7156\n",
      "Epoch [4036/5000] | D Loss: 1.3742 | G Loss: 0.7229\n",
      "Epoch [4037/5000] | D Loss: 1.3742 | G Loss: 0.7236\n",
      "Epoch [4038/5000] | D Loss: 1.3713 | G Loss: 0.7402\n",
      "Epoch [4039/5000] | D Loss: 1.3551 | G Loss: 0.7228\n",
      "Epoch [4040/5000] | D Loss: 1.3816 | G Loss: 0.7378\n",
      "Epoch [4041/5000] | D Loss: 1.3496 | G Loss: 0.7100\n",
      "Epoch [4042/5000] | D Loss: 1.3811 | G Loss: 0.7561\n",
      "Epoch [4043/5000] | D Loss: 1.3805 | G Loss: 0.7286\n",
      "Epoch [4044/5000] | D Loss: 1.3719 | G Loss: 0.7185\n",
      "Epoch [4045/5000] | D Loss: 1.3537 | G Loss: 0.7186\n",
      "Epoch [4046/5000] | D Loss: 1.3482 | G Loss: 0.7050\n",
      "Epoch [4047/5000] | D Loss: 1.3466 | G Loss: 0.7270\n",
      "Epoch [4048/5000] | D Loss: 1.3739 | G Loss: 0.7025\n",
      "Epoch [4049/5000] | D Loss: 1.3681 | G Loss: 0.7226\n",
      "Epoch [4050/5000] | D Loss: 1.3878 | G Loss: 0.7198\n",
      "Epoch [4051/5000] | D Loss: 1.3641 | G Loss: 0.7213\n",
      "Epoch [4052/5000] | D Loss: 1.3677 | G Loss: 0.7045\n",
      "Epoch [4053/5000] | D Loss: 1.3322 | G Loss: 0.7368\n",
      "Epoch [4054/5000] | D Loss: 1.3428 | G Loss: 0.7259\n",
      "Epoch [4055/5000] | D Loss: 1.3649 | G Loss: 0.7377\n",
      "Epoch [4056/5000] | D Loss: 1.3450 | G Loss: 0.6954\n",
      "Epoch [4057/5000] | D Loss: 1.3509 | G Loss: 0.7393\n",
      "Epoch [4058/5000] | D Loss: 1.4108 | G Loss: 0.6962\n",
      "Epoch [4059/5000] | D Loss: 1.3799 | G Loss: 0.7445\n",
      "Epoch [4060/5000] | D Loss: 1.3816 | G Loss: 0.7295\n",
      "Epoch [4061/5000] | D Loss: 1.3443 | G Loss: 0.7571\n",
      "Epoch [4062/5000] | D Loss: 1.3587 | G Loss: 0.6871\n",
      "Epoch [4063/5000] | D Loss: 1.3452 | G Loss: 0.7347\n",
      "Epoch [4064/5000] | D Loss: 1.3709 | G Loss: 0.7137\n",
      "Epoch [4065/5000] | D Loss: 1.3492 | G Loss: 0.7149\n",
      "Epoch [4066/5000] | D Loss: 1.3448 | G Loss: 0.7199\n",
      "Epoch [4067/5000] | D Loss: 1.3735 | G Loss: 0.7278\n",
      "Epoch [4068/5000] | D Loss: 1.3821 | G Loss: 0.7455\n",
      "Epoch [4069/5000] | D Loss: 1.3504 | G Loss: 0.7138\n",
      "Epoch [4070/5000] | D Loss: 1.3317 | G Loss: 0.7218\n",
      "Epoch [4071/5000] | D Loss: 1.3622 | G Loss: 0.6952\n",
      "Epoch [4072/5000] | D Loss: 1.3481 | G Loss: 0.7056\n",
      "Epoch [4073/5000] | D Loss: 1.3171 | G Loss: 0.7148\n",
      "Epoch [4074/5000] | D Loss: 1.3971 | G Loss: 0.7314\n",
      "Epoch [4075/5000] | D Loss: 1.3488 | G Loss: 0.7555\n",
      "Epoch [4076/5000] | D Loss: 1.3565 | G Loss: 0.7157\n",
      "Epoch [4077/5000] | D Loss: 1.3593 | G Loss: 0.6938\n",
      "Epoch [4078/5000] | D Loss: 1.3709 | G Loss: 0.7068\n",
      "Epoch [4079/5000] | D Loss: 1.3715 | G Loss: 0.6971\n",
      "Epoch [4080/5000] | D Loss: 1.3710 | G Loss: 0.7300\n",
      "Epoch [4081/5000] | D Loss: 1.3774 | G Loss: 0.7162\n",
      "Epoch [4082/5000] | D Loss: 1.3657 | G Loss: 0.7123\n",
      "Epoch [4083/5000] | D Loss: 1.3811 | G Loss: 0.7313\n",
      "Epoch [4084/5000] | D Loss: 1.3631 | G Loss: 0.7303\n",
      "Epoch [4085/5000] | D Loss: 1.3491 | G Loss: 0.7561\n",
      "Epoch [4086/5000] | D Loss: 1.3876 | G Loss: 0.7255\n",
      "Epoch [4087/5000] | D Loss: 1.3571 | G Loss: 0.7270\n",
      "Epoch [4088/5000] | D Loss: 1.3549 | G Loss: 0.7498\n",
      "Epoch [4089/5000] | D Loss: 1.3320 | G Loss: 0.7273\n",
      "Epoch [4090/5000] | D Loss: 1.3911 | G Loss: 0.7413\n",
      "Epoch [4091/5000] | D Loss: 1.4228 | G Loss: 0.7335\n",
      "Epoch [4092/5000] | D Loss: 1.3837 | G Loss: 0.6897\n",
      "Epoch [4093/5000] | D Loss: 1.3732 | G Loss: 0.7374\n",
      "Epoch [4094/5000] | D Loss: 1.3600 | G Loss: 0.7219\n",
      "Epoch [4095/5000] | D Loss: 1.3737 | G Loss: 0.7126\n",
      "Epoch [4096/5000] | D Loss: 1.3566 | G Loss: 0.7510\n",
      "Epoch [4097/5000] | D Loss: 1.3835 | G Loss: 0.7188\n",
      "Epoch [4098/5000] | D Loss: 1.3676 | G Loss: 0.7153\n",
      "Epoch [4099/5000] | D Loss: 1.3724 | G Loss: 0.7358\n",
      "Epoch [4100/5000] | D Loss: 1.3719 | G Loss: 0.6867\n",
      "Epoch 4100 FID Score: 68.1595\n",
      "Epoch [4101/5000] | D Loss: 1.3758 | G Loss: 0.7449\n",
      "Epoch [4102/5000] | D Loss: 1.3877 | G Loss: 0.7120\n",
      "Epoch [4103/5000] | D Loss: 1.3514 | G Loss: 0.6906\n",
      "Epoch [4104/5000] | D Loss: 1.3694 | G Loss: 0.7323\n",
      "Epoch [4105/5000] | D Loss: 1.2917 | G Loss: 0.7324\n",
      "Epoch [4106/5000] | D Loss: 1.3597 | G Loss: 0.7439\n",
      "Epoch [4107/5000] | D Loss: 1.3385 | G Loss: 0.7141\n",
      "Epoch [4108/5000] | D Loss: 1.3551 | G Loss: 0.7170\n",
      "Epoch [4109/5000] | D Loss: 1.3522 | G Loss: 0.7355\n",
      "Epoch [4110/5000] | D Loss: 1.3452 | G Loss: 0.7187\n",
      "Epoch [4111/5000] | D Loss: 1.3794 | G Loss: 0.7157\n",
      "Epoch [4112/5000] | D Loss: 1.3187 | G Loss: 0.7479\n",
      "Epoch [4113/5000] | D Loss: 1.3259 | G Loss: 0.7031\n",
      "Epoch [4114/5000] | D Loss: 1.3825 | G Loss: 0.7312\n",
      "Epoch [4115/5000] | D Loss: 1.3518 | G Loss: 0.7229\n",
      "Epoch [4116/5000] | D Loss: 1.3627 | G Loss: 0.7373\n",
      "Epoch [4117/5000] | D Loss: 1.3472 | G Loss: 0.7558\n",
      "Epoch [4118/5000] | D Loss: 1.3507 | G Loss: 0.7633\n",
      "Epoch [4119/5000] | D Loss: 1.3909 | G Loss: 0.7114\n",
      "Epoch [4120/5000] | D Loss: 1.3610 | G Loss: 0.7195\n",
      "Epoch [4121/5000] | D Loss: 1.3633 | G Loss: 0.7143\n",
      "Epoch [4122/5000] | D Loss: 1.3559 | G Loss: 0.7175\n",
      "Epoch [4123/5000] | D Loss: 1.3472 | G Loss: 0.7252\n",
      "Epoch [4124/5000] | D Loss: 1.2869 | G Loss: 0.7367\n",
      "Epoch [4125/5000] | D Loss: 1.3563 | G Loss: 0.6781\n",
      "Epoch [4126/5000] | D Loss: 1.4066 | G Loss: 0.7258\n",
      "Epoch [4127/5000] | D Loss: 1.3755 | G Loss: 0.7341\n",
      "Epoch [4128/5000] | D Loss: 1.3572 | G Loss: 0.6898\n",
      "Epoch [4129/5000] | D Loss: 1.3743 | G Loss: 0.7138\n",
      "Epoch [4130/5000] | D Loss: 1.3744 | G Loss: 0.7233\n",
      "Epoch [4131/5000] | D Loss: 1.3386 | G Loss: 0.6975\n",
      "Epoch [4132/5000] | D Loss: 1.3368 | G Loss: 0.7134\n",
      "Epoch [4133/5000] | D Loss: 1.3298 | G Loss: 0.7475\n",
      "Epoch [4134/5000] | D Loss: 1.3678 | G Loss: 0.7201\n",
      "Epoch [4135/5000] | D Loss: 1.3698 | G Loss: 0.7323\n",
      "Epoch [4136/5000] | D Loss: 1.3686 | G Loss: 0.7538\n",
      "Epoch [4137/5000] | D Loss: 1.3579 | G Loss: 0.7283\n",
      "Epoch [4138/5000] | D Loss: 1.3713 | G Loss: 0.7067\n",
      "Epoch [4139/5000] | D Loss: 1.3575 | G Loss: 0.7249\n",
      "Epoch [4140/5000] | D Loss: 1.3842 | G Loss: 0.7034\n",
      "Epoch [4141/5000] | D Loss: 1.3958 | G Loss: 0.7052\n",
      "Epoch [4142/5000] | D Loss: 1.3336 | G Loss: 0.7072\n",
      "Epoch [4143/5000] | D Loss: 1.3429 | G Loss: 0.7261\n",
      "Epoch [4144/5000] | D Loss: 1.3615 | G Loss: 0.7119\n",
      "Epoch [4145/5000] | D Loss: 1.3507 | G Loss: 0.7273\n",
      "Epoch [4146/5000] | D Loss: 1.3634 | G Loss: 0.7467\n",
      "Epoch [4147/5000] | D Loss: 1.3710 | G Loss: 0.7133\n",
      "Epoch [4148/5000] | D Loss: 1.3507 | G Loss: 0.7122\n",
      "Epoch [4149/5000] | D Loss: 1.3261 | G Loss: 0.7338\n",
      "Epoch [4150/5000] | D Loss: 1.3815 | G Loss: 0.7296\n",
      "Epoch [4151/5000] | D Loss: 1.3563 | G Loss: 0.7141\n",
      "Epoch [4152/5000] | D Loss: 1.3952 | G Loss: 0.7230\n",
      "Epoch [4153/5000] | D Loss: 1.3438 | G Loss: 0.7346\n",
      "Epoch [4154/5000] | D Loss: 1.3343 | G Loss: 0.7390\n",
      "Epoch [4155/5000] | D Loss: 1.3556 | G Loss: 0.7025\n",
      "Epoch [4156/5000] | D Loss: 1.3674 | G Loss: 0.7108\n",
      "Epoch [4157/5000] | D Loss: 1.3058 | G Loss: 0.7184\n",
      "Epoch [4158/5000] | D Loss: 1.3372 | G Loss: 0.7171\n",
      "Epoch [4159/5000] | D Loss: 1.3747 | G Loss: 0.7275\n",
      "Epoch [4160/5000] | D Loss: 1.3396 | G Loss: 0.7453\n",
      "Epoch [4161/5000] | D Loss: 1.3189 | G Loss: 0.7150\n",
      "Epoch [4162/5000] | D Loss: 1.3620 | G Loss: 0.7270\n",
      "Epoch [4163/5000] | D Loss: 1.3446 | G Loss: 0.7136\n",
      "Epoch [4164/5000] | D Loss: 1.3490 | G Loss: 0.7256\n",
      "Epoch [4165/5000] | D Loss: 1.3340 | G Loss: 0.7115\n",
      "Epoch [4166/5000] | D Loss: 1.3363 | G Loss: 0.7168\n",
      "Epoch [4167/5000] | D Loss: 1.3572 | G Loss: 0.7388\n",
      "Epoch [4168/5000] | D Loss: 1.3448 | G Loss: 0.7316\n",
      "Epoch [4169/5000] | D Loss: 1.3392 | G Loss: 0.7182\n",
      "Epoch [4170/5000] | D Loss: 1.3368 | G Loss: 0.7135\n",
      "Epoch [4171/5000] | D Loss: 1.3448 | G Loss: 0.7193\n",
      "Epoch [4172/5000] | D Loss: 1.3500 | G Loss: 0.7205\n",
      "Epoch [4173/5000] | D Loss: 1.3312 | G Loss: 0.7413\n",
      "Epoch [4174/5000] | D Loss: 1.3182 | G Loss: 0.7204\n",
      "Epoch [4175/5000] | D Loss: 1.3404 | G Loss: 0.7290\n",
      "Epoch [4176/5000] | D Loss: 1.3448 | G Loss: 0.7418\n",
      "Epoch [4177/5000] | D Loss: 1.3440 | G Loss: 0.7087\n",
      "Epoch [4178/5000] | D Loss: 1.3403 | G Loss: 0.7199\n",
      "Epoch [4179/5000] | D Loss: 1.3823 | G Loss: 0.7142\n",
      "Epoch [4180/5000] | D Loss: 1.3369 | G Loss: 0.7490\n",
      "Epoch [4181/5000] | D Loss: 1.3433 | G Loss: 0.7525\n",
      "Epoch [4182/5000] | D Loss: 1.3974 | G Loss: 0.7063\n",
      "Epoch [4183/5000] | D Loss: 1.3589 | G Loss: 0.7226\n",
      "Epoch [4184/5000] | D Loss: 1.3703 | G Loss: 0.7006\n",
      "Epoch [4185/5000] | D Loss: 1.3768 | G Loss: 0.7102\n",
      "Epoch [4186/5000] | D Loss: 1.3679 | G Loss: 0.6990\n",
      "Epoch [4187/5000] | D Loss: 1.3488 | G Loss: 0.7014\n",
      "Epoch [4188/5000] | D Loss: 1.3558 | G Loss: 0.7356\n",
      "Epoch [4189/5000] | D Loss: 1.3539 | G Loss: 0.6918\n",
      "Epoch [4190/5000] | D Loss: 1.3776 | G Loss: 0.7366\n",
      "Epoch [4191/5000] | D Loss: 1.3732 | G Loss: 0.7371\n",
      "Epoch [4192/5000] | D Loss: 1.3639 | G Loss: 0.7103\n",
      "Epoch [4193/5000] | D Loss: 1.3382 | G Loss: 0.7335\n",
      "Epoch [4194/5000] | D Loss: 1.3351 | G Loss: 0.7451\n",
      "Epoch [4195/5000] | D Loss: 1.3805 | G Loss: 0.7167\n",
      "Epoch [4196/5000] | D Loss: 1.3718 | G Loss: 0.7312\n",
      "Epoch [4197/5000] | D Loss: 1.3451 | G Loss: 0.7389\n",
      "Epoch [4198/5000] | D Loss: 1.3765 | G Loss: 0.7135\n",
      "Epoch [4199/5000] | D Loss: 1.3652 | G Loss: 0.7138\n",
      "Epoch [4200/5000] | D Loss: 1.3650 | G Loss: 0.7404\n",
      "Epoch 4200 FID Score: 67.3821\n",
      "Reducing learning rates to Generator: 1.6000000000000003e-05, Discriminator: 1.6000000000000003e-05\n",
      "Epoch [4201/5000] | D Loss: 1.3358 | G Loss: 0.7044\n",
      "Epoch [4202/5000] | D Loss: 1.3537 | G Loss: 0.7274\n",
      "Epoch [4203/5000] | D Loss: 1.3360 | G Loss: 0.7143\n",
      "Epoch [4204/5000] | D Loss: 1.3897 | G Loss: 0.7196\n",
      "Epoch [4205/5000] | D Loss: 1.3720 | G Loss: 0.7325\n",
      "Epoch [4206/5000] | D Loss: 1.3110 | G Loss: 0.7157\n",
      "Epoch [4207/5000] | D Loss: 1.3703 | G Loss: 0.7468\n",
      "Epoch [4208/5000] | D Loss: 1.3779 | G Loss: 0.7490\n",
      "Epoch [4209/5000] | D Loss: 1.3543 | G Loss: 0.7230\n",
      "Epoch [4210/5000] | D Loss: 1.3717 | G Loss: 0.7059\n",
      "Epoch [4211/5000] | D Loss: 1.3568 | G Loss: 0.7434\n",
      "Epoch [4212/5000] | D Loss: 1.3731 | G Loss: 0.7292\n",
      "Epoch [4213/5000] | D Loss: 1.3773 | G Loss: 0.7688\n",
      "Epoch [4214/5000] | D Loss: 1.3423 | G Loss: 0.7467\n",
      "Epoch [4215/5000] | D Loss: 1.3610 | G Loss: 0.7165\n",
      "Epoch [4216/5000] | D Loss: 1.3932 | G Loss: 0.7185\n",
      "Epoch [4217/5000] | D Loss: 1.3669 | G Loss: 0.6850\n",
      "Epoch [4218/5000] | D Loss: 1.3605 | G Loss: 0.7552\n",
      "Epoch [4219/5000] | D Loss: 1.3451 | G Loss: 0.7406\n",
      "Epoch [4220/5000] | D Loss: 1.3192 | G Loss: 0.7550\n",
      "Epoch [4221/5000] | D Loss: 1.3479 | G Loss: 0.7189\n",
      "Epoch [4222/5000] | D Loss: 1.3293 | G Loss: 0.7166\n",
      "Epoch [4223/5000] | D Loss: 1.3481 | G Loss: 0.7543\n",
      "Epoch [4224/5000] | D Loss: 1.3737 | G Loss: 0.7011\n",
      "Epoch [4225/5000] | D Loss: 1.3425 | G Loss: 0.7123\n",
      "Epoch [4226/5000] | D Loss: 1.3451 | G Loss: 0.7036\n",
      "Epoch [4227/5000] | D Loss: 1.3394 | G Loss: 0.7560\n",
      "Epoch [4228/5000] | D Loss: 1.3744 | G Loss: 0.7468\n",
      "Epoch [4229/5000] | D Loss: 1.3619 | G Loss: 0.7371\n",
      "Epoch [4230/5000] | D Loss: 1.3869 | G Loss: 0.6990\n",
      "Epoch [4231/5000] | D Loss: 1.3388 | G Loss: 0.7306\n",
      "Epoch [4232/5000] | D Loss: 1.3695 | G Loss: 0.7144\n",
      "Epoch [4233/5000] | D Loss: 1.3844 | G Loss: 0.7314\n",
      "Epoch [4234/5000] | D Loss: 1.3360 | G Loss: 0.7243\n",
      "Epoch [4235/5000] | D Loss: 1.3794 | G Loss: 0.6915\n",
      "Epoch [4236/5000] | D Loss: 1.3510 | G Loss: 0.6963\n",
      "Epoch [4237/5000] | D Loss: 1.3620 | G Loss: 0.7712\n",
      "Epoch [4238/5000] | D Loss: 1.3519 | G Loss: 0.7403\n",
      "Epoch [4239/5000] | D Loss: 1.3622 | G Loss: 0.7306\n",
      "Epoch [4240/5000] | D Loss: 1.3532 | G Loss: 0.7400\n",
      "Epoch [4241/5000] | D Loss: 1.3417 | G Loss: 0.7234\n",
      "Epoch [4242/5000] | D Loss: 1.3692 | G Loss: 0.7386\n",
      "Epoch [4243/5000] | D Loss: 1.3929 | G Loss: 0.7125\n",
      "Epoch [4244/5000] | D Loss: 1.3742 | G Loss: 0.7037\n",
      "Epoch [4245/5000] | D Loss: 1.3710 | G Loss: 0.7498\n",
      "Epoch [4246/5000] | D Loss: 1.3620 | G Loss: 0.7194\n",
      "Epoch [4247/5000] | D Loss: 1.3554 | G Loss: 0.7467\n",
      "Epoch [4248/5000] | D Loss: 1.3603 | G Loss: 0.7527\n",
      "Epoch [4249/5000] | D Loss: 1.3553 | G Loss: 0.7628\n",
      "Epoch [4250/5000] | D Loss: 1.3827 | G Loss: 0.7091\n",
      "Epoch [4251/5000] | D Loss: 1.3780 | G Loss: 0.7374\n",
      "Epoch [4252/5000] | D Loss: 1.3626 | G Loss: 0.7070\n",
      "Epoch [4253/5000] | D Loss: 1.3832 | G Loss: 0.7360\n",
      "Epoch [4254/5000] | D Loss: 1.3433 | G Loss: 0.7078\n",
      "Epoch [4255/5000] | D Loss: 1.3824 | G Loss: 0.7447\n",
      "Epoch [4256/5000] | D Loss: 1.3649 | G Loss: 0.7228\n",
      "Epoch [4257/5000] | D Loss: 1.3724 | G Loss: 0.7557\n",
      "Epoch [4258/5000] | D Loss: 1.3461 | G Loss: 0.7429\n",
      "Epoch [4259/5000] | D Loss: 1.3323 | G Loss: 0.7437\n",
      "Epoch [4260/5000] | D Loss: 1.3891 | G Loss: 0.6994\n",
      "Epoch [4261/5000] | D Loss: 1.3664 | G Loss: 0.6945\n",
      "Epoch [4262/5000] | D Loss: 1.3592 | G Loss: 0.7477\n",
      "Epoch [4263/5000] | D Loss: 1.3608 | G Loss: 0.7340\n",
      "Epoch [4264/5000] | D Loss: 1.3444 | G Loss: 0.7288\n",
      "Epoch [4265/5000] | D Loss: 1.3622 | G Loss: 0.7142\n",
      "Epoch [4266/5000] | D Loss: 1.3282 | G Loss: 0.7507\n",
      "Epoch [4267/5000] | D Loss: 1.3414 | G Loss: 0.7187\n",
      "Epoch [4268/5000] | D Loss: 1.3819 | G Loss: 0.6939\n",
      "Epoch [4269/5000] | D Loss: 1.3827 | G Loss: 0.7521\n",
      "Epoch [4270/5000] | D Loss: 1.3525 | G Loss: 0.6987\n",
      "Epoch [4271/5000] | D Loss: 1.3297 | G Loss: 0.7161\n",
      "Epoch [4272/5000] | D Loss: 1.3508 | G Loss: 0.7451\n",
      "Epoch [4273/5000] | D Loss: 1.3649 | G Loss: 0.7166\n",
      "Epoch [4274/5000] | D Loss: 1.3494 | G Loss: 0.7357\n",
      "Epoch [4275/5000] | D Loss: 1.3685 | G Loss: 0.7283\n",
      "Epoch [4276/5000] | D Loss: 1.3847 | G Loss: 0.7206\n",
      "Epoch [4277/5000] | D Loss: 1.3594 | G Loss: 0.7056\n",
      "Epoch [4278/5000] | D Loss: 1.3553 | G Loss: 0.7285\n",
      "Epoch [4279/5000] | D Loss: 1.3454 | G Loss: 0.7423\n",
      "Epoch [4280/5000] | D Loss: 1.3618 | G Loss: 0.7350\n",
      "Epoch [4281/5000] | D Loss: 1.3511 | G Loss: 0.7322\n",
      "Epoch [4282/5000] | D Loss: 1.3736 | G Loss: 0.7280\n",
      "Epoch [4283/5000] | D Loss: 1.3620 | G Loss: 0.7502\n",
      "Epoch [4284/5000] | D Loss: 1.3515 | G Loss: 0.7168\n",
      "Epoch [4285/5000] | D Loss: 1.3688 | G Loss: 0.7231\n",
      "Epoch [4286/5000] | D Loss: 1.3569 | G Loss: 0.7310\n",
      "Epoch [4287/5000] | D Loss: 1.3850 | G Loss: 0.7279\n",
      "Epoch [4288/5000] | D Loss: 1.3668 | G Loss: 0.7238\n",
      "Epoch [4289/5000] | D Loss: 1.3634 | G Loss: 0.7066\n",
      "Epoch [4290/5000] | D Loss: 1.3534 | G Loss: 0.6963\n",
      "Epoch [4291/5000] | D Loss: 1.3905 | G Loss: 0.7240\n",
      "Epoch [4292/5000] | D Loss: 1.3827 | G Loss: 0.7363\n",
      "Epoch [4293/5000] | D Loss: 1.3102 | G Loss: 0.7228\n",
      "Epoch [4294/5000] | D Loss: 1.3623 | G Loss: 0.7351\n",
      "Epoch [4295/5000] | D Loss: 1.2967 | G Loss: 0.7191\n",
      "Epoch [4296/5000] | D Loss: 1.3433 | G Loss: 0.7323\n",
      "Epoch [4297/5000] | D Loss: 1.3590 | G Loss: 0.7156\n",
      "Epoch [4298/5000] | D Loss: 1.3573 | G Loss: 0.7257\n",
      "Epoch [4299/5000] | D Loss: 1.3506 | G Loss: 0.7150\n",
      "Epoch [4300/5000] | D Loss: 1.3712 | G Loss: 0.7579\n",
      "Epoch 4300 FID Score: 66.2017\n",
      "Saved improved model at Epoch 4300 with FID 66.2017\n",
      "Epoch [4301/5000] | D Loss: 1.3957 | G Loss: 0.6942\n",
      "Epoch [4302/5000] | D Loss: 1.3638 | G Loss: 0.7284\n",
      "Epoch [4303/5000] | D Loss: 1.3327 | G Loss: 0.7292\n",
      "Epoch [4304/5000] | D Loss: 1.3767 | G Loss: 0.7405\n",
      "Epoch [4305/5000] | D Loss: 1.3188 | G Loss: 0.6992\n",
      "Epoch [4306/5000] | D Loss: 1.3841 | G Loss: 0.7501\n",
      "Epoch [4307/5000] | D Loss: 1.3780 | G Loss: 0.7485\n",
      "Epoch [4308/5000] | D Loss: 1.3523 | G Loss: 0.7579\n",
      "Epoch [4309/5000] | D Loss: 1.3637 | G Loss: 0.7642\n",
      "Epoch [4310/5000] | D Loss: 1.3707 | G Loss: 0.6948\n",
      "Epoch [4311/5000] | D Loss: 1.3440 | G Loss: 0.7260\n",
      "Epoch [4312/5000] | D Loss: 1.3981 | G Loss: 0.7262\n",
      "Epoch [4313/5000] | D Loss: 1.3806 | G Loss: 0.7339\n",
      "Epoch [4314/5000] | D Loss: 1.3627 | G Loss: 0.7781\n",
      "Epoch [4315/5000] | D Loss: 1.3726 | G Loss: 0.7373\n",
      "Epoch [4316/5000] | D Loss: 1.3586 | G Loss: 0.7509\n",
      "Epoch [4317/5000] | D Loss: 1.3749 | G Loss: 0.7470\n",
      "Epoch [4318/5000] | D Loss: 1.3807 | G Loss: 0.7277\n",
      "Epoch [4319/5000] | D Loss: 1.3852 | G Loss: 0.7483\n",
      "Epoch [4320/5000] | D Loss: 1.3487 | G Loss: 0.7613\n",
      "Epoch [4321/5000] | D Loss: 1.3597 | G Loss: 0.7014\n",
      "Epoch [4322/5000] | D Loss: 1.3568 | G Loss: 0.7404\n",
      "Epoch [4323/5000] | D Loss: 1.3594 | G Loss: 0.7300\n",
      "Epoch [4324/5000] | D Loss: 1.3710 | G Loss: 0.7316\n",
      "Epoch [4325/5000] | D Loss: 1.3581 | G Loss: 0.7283\n",
      "Epoch [4326/5000] | D Loss: 1.3616 | G Loss: 0.7182\n",
      "Epoch [4327/5000] | D Loss: 1.3538 | G Loss: 0.7062\n",
      "Epoch [4328/5000] | D Loss: 1.3576 | G Loss: 0.6971\n",
      "Epoch [4329/5000] | D Loss: 1.3536 | G Loss: 0.7190\n",
      "Epoch [4330/5000] | D Loss: 1.3444 | G Loss: 0.7484\n",
      "Epoch [4331/5000] | D Loss: 1.3275 | G Loss: 0.7245\n",
      "Epoch [4332/5000] | D Loss: 1.3419 | G Loss: 0.7221\n",
      "Epoch [4333/5000] | D Loss: 1.3673 | G Loss: 0.7263\n",
      "Epoch [4334/5000] | D Loss: 1.3671 | G Loss: 0.6867\n",
      "Epoch [4335/5000] | D Loss: 1.3520 | G Loss: 0.7257\n",
      "Epoch [4336/5000] | D Loss: 1.3662 | G Loss: 0.7258\n",
      "Epoch [4337/5000] | D Loss: 1.3539 | G Loss: 0.7503\n",
      "Epoch [4338/5000] | D Loss: 1.3332 | G Loss: 0.7431\n",
      "Epoch [4339/5000] | D Loss: 1.3538 | G Loss: 0.7464\n",
      "Epoch [4340/5000] | D Loss: 1.3807 | G Loss: 0.7277\n",
      "Epoch [4341/5000] | D Loss: 1.3842 | G Loss: 0.7269\n",
      "Epoch [4342/5000] | D Loss: 1.3425 | G Loss: 0.6893\n",
      "Epoch [4343/5000] | D Loss: 1.3616 | G Loss: 0.7492\n",
      "Epoch [4344/5000] | D Loss: 1.3742 | G Loss: 0.7278\n",
      "Epoch [4345/5000] | D Loss: 1.3523 | G Loss: 0.7315\n",
      "Epoch [4346/5000] | D Loss: 1.3921 | G Loss: 0.7502\n",
      "Epoch [4347/5000] | D Loss: 1.3458 | G Loss: 0.7344\n",
      "Epoch [4348/5000] | D Loss: 1.3412 | G Loss: 0.7329\n",
      "Epoch [4349/5000] | D Loss: 1.3508 | G Loss: 0.7064\n",
      "Epoch [4350/5000] | D Loss: 1.3749 | G Loss: 0.7248\n",
      "Epoch [4351/5000] | D Loss: 1.3129 | G Loss: 0.7108\n",
      "Epoch [4352/5000] | D Loss: 1.3431 | G Loss: 0.7296\n",
      "Epoch [4353/5000] | D Loss: 1.3456 | G Loss: 0.7021\n",
      "Epoch [4354/5000] | D Loss: 1.3523 | G Loss: 0.7033\n",
      "Epoch [4355/5000] | D Loss: 1.3594 | G Loss: 0.7213\n",
      "Epoch [4356/5000] | D Loss: 1.3957 | G Loss: 0.7092\n",
      "Epoch [4357/5000] | D Loss: 1.3978 | G Loss: 0.7467\n",
      "Epoch [4358/5000] | D Loss: 1.3446 | G Loss: 0.7375\n",
      "Epoch [4359/5000] | D Loss: 1.3459 | G Loss: 0.7186\n",
      "Epoch [4360/5000] | D Loss: 1.3380 | G Loss: 0.7248\n",
      "Epoch [4361/5000] | D Loss: 1.3645 | G Loss: 0.7029\n",
      "Epoch [4362/5000] | D Loss: 1.3950 | G Loss: 0.7279\n",
      "Epoch [4363/5000] | D Loss: 1.3869 | G Loss: 0.7523\n",
      "Epoch [4364/5000] | D Loss: 1.3469 | G Loss: 0.7556\n",
      "Epoch [4365/5000] | D Loss: 1.3020 | G Loss: 0.7435\n",
      "Epoch [4366/5000] | D Loss: 1.4013 | G Loss: 0.7343\n",
      "Epoch [4367/5000] | D Loss: 1.3251 | G Loss: 0.6889\n",
      "Epoch [4368/5000] | D Loss: 1.3778 | G Loss: 0.7300\n",
      "Epoch [4369/5000] | D Loss: 1.3693 | G Loss: 0.7425\n",
      "Epoch [4370/5000] | D Loss: 1.3464 | G Loss: 0.7177\n",
      "Epoch [4371/5000] | D Loss: 1.3420 | G Loss: 0.6997\n",
      "Epoch [4372/5000] | D Loss: 1.3448 | G Loss: 0.7152\n",
      "Epoch [4373/5000] | D Loss: 1.3757 | G Loss: 0.6976\n",
      "Epoch [4374/5000] | D Loss: 1.3506 | G Loss: 0.7398\n",
      "Epoch [4375/5000] | D Loss: 1.3634 | G Loss: 0.7309\n",
      "Epoch [4376/5000] | D Loss: 1.3594 | G Loss: 0.7400\n",
      "Epoch [4377/5000] | D Loss: 1.3735 | G Loss: 0.7394\n",
      "Epoch [4378/5000] | D Loss: 1.3624 | G Loss: 0.7091\n",
      "Epoch [4379/5000] | D Loss: 1.3513 | G Loss: 0.7115\n",
      "Epoch [4380/5000] | D Loss: 1.3377 | G Loss: 0.7093\n",
      "Epoch [4381/5000] | D Loss: 1.3496 | G Loss: 0.7460\n",
      "Epoch [4382/5000] | D Loss: 1.3694 | G Loss: 0.7459\n",
      "Epoch [4383/5000] | D Loss: 1.3901 | G Loss: 0.7073\n",
      "Epoch [4384/5000] | D Loss: 1.3493 | G Loss: 0.7565\n",
      "Epoch [4385/5000] | D Loss: 1.3734 | G Loss: 0.7305\n",
      "Epoch [4386/5000] | D Loss: 1.3428 | G Loss: 0.7340\n",
      "Epoch [4387/5000] | D Loss: 1.4099 | G Loss: 0.7294\n",
      "Epoch [4388/5000] | D Loss: 1.3699 | G Loss: 0.7277\n",
      "Epoch [4389/5000] | D Loss: 1.3449 | G Loss: 0.7246\n",
      "Epoch [4390/5000] | D Loss: 1.3863 | G Loss: 0.7372\n",
      "Epoch [4391/5000] | D Loss: 1.3374 | G Loss: 0.7157\n",
      "Epoch [4392/5000] | D Loss: 1.3730 | G Loss: 0.7105\n",
      "Epoch [4393/5000] | D Loss: 1.3607 | G Loss: 0.7199\n",
      "Epoch [4394/5000] | D Loss: 1.3707 | G Loss: 0.7355\n",
      "Epoch [4395/5000] | D Loss: 1.3316 | G Loss: 0.8069\n",
      "Epoch [4396/5000] | D Loss: 1.3673 | G Loss: 0.7244\n",
      "Epoch [4397/5000] | D Loss: 1.3656 | G Loss: 0.7155\n",
      "Epoch [4398/5000] | D Loss: 1.3455 | G Loss: 0.7344\n",
      "Epoch [4399/5000] | D Loss: 1.3761 | G Loss: 0.7490\n",
      "Epoch [4400/5000] | D Loss: 1.3855 | G Loss: 0.7297\n",
      "Epoch 4400 FID Score: 67.1803\n",
      "Epoch [4401/5000] | D Loss: 1.3722 | G Loss: 0.7156\n",
      "Epoch [4402/5000] | D Loss: 1.3866 | G Loss: 0.7334\n",
      "Epoch [4403/5000] | D Loss: 1.3687 | G Loss: 0.7313\n",
      "Epoch [4404/5000] | D Loss: 1.3565 | G Loss: 0.7114\n",
      "Epoch [4405/5000] | D Loss: 1.3419 | G Loss: 0.7388\n",
      "Epoch [4406/5000] | D Loss: 1.3734 | G Loss: 0.6939\n",
      "Epoch [4407/5000] | D Loss: 1.3343 | G Loss: 0.7188\n",
      "Epoch [4408/5000] | D Loss: 1.3667 | G Loss: 0.7013\n",
      "Epoch [4409/5000] | D Loss: 1.3459 | G Loss: 0.7061\n",
      "Epoch [4410/5000] | D Loss: 1.3687 | G Loss: 0.7356\n",
      "Epoch [4411/5000] | D Loss: 1.3679 | G Loss: 0.7450\n",
      "Epoch [4412/5000] | D Loss: 1.3662 | G Loss: 0.7095\n",
      "Epoch [4413/5000] | D Loss: 1.3745 | G Loss: 0.7128\n",
      "Epoch [4414/5000] | D Loss: 1.3688 | G Loss: 0.7569\n",
      "Epoch [4415/5000] | D Loss: 1.3528 | G Loss: 0.7657\n",
      "Epoch [4416/5000] | D Loss: 1.3384 | G Loss: 0.7521\n",
      "Epoch [4417/5000] | D Loss: 1.3633 | G Loss: 0.7114\n",
      "Epoch [4418/5000] | D Loss: 1.3679 | G Loss: 0.7446\n",
      "Epoch [4419/5000] | D Loss: 1.3702 | G Loss: 0.7338\n",
      "Epoch [4420/5000] | D Loss: 1.3727 | G Loss: 0.7311\n",
      "Epoch [4421/5000] | D Loss: 1.3969 | G Loss: 0.7297\n",
      "Epoch [4422/5000] | D Loss: 1.3388 | G Loss: 0.7143\n",
      "Epoch [4423/5000] | D Loss: 1.3582 | G Loss: 0.7235\n",
      "Epoch [4424/5000] | D Loss: 1.4055 | G Loss: 0.7011\n",
      "Epoch [4425/5000] | D Loss: 1.3520 | G Loss: 0.7191\n",
      "Epoch [4426/5000] | D Loss: 1.3699 | G Loss: 0.7250\n",
      "Epoch [4427/5000] | D Loss: 1.3267 | G Loss: 0.7355\n",
      "Epoch [4428/5000] | D Loss: 1.3650 | G Loss: 0.7423\n",
      "Epoch [4429/5000] | D Loss: 1.3551 | G Loss: 0.7013\n",
      "Epoch [4430/5000] | D Loss: 1.3793 | G Loss: 0.7253\n",
      "Epoch [4431/5000] | D Loss: 1.3385 | G Loss: 0.7223\n",
      "Epoch [4432/5000] | D Loss: 1.3761 | G Loss: 0.7276\n",
      "Epoch [4433/5000] | D Loss: 1.3697 | G Loss: 0.7202\n",
      "Epoch [4434/5000] | D Loss: 1.3302 | G Loss: 0.7174\n",
      "Epoch [4435/5000] | D Loss: 1.3269 | G Loss: 0.7303\n",
      "Epoch [4436/5000] | D Loss: 1.3311 | G Loss: 0.7122\n",
      "Epoch [4437/5000] | D Loss: 1.3685 | G Loss: 0.7458\n",
      "Epoch [4438/5000] | D Loss: 1.3412 | G Loss: 0.7233\n",
      "Epoch [4439/5000] | D Loss: 1.3526 | G Loss: 0.7429\n",
      "Epoch [4440/5000] | D Loss: 1.3507 | G Loss: 0.7344\n",
      "Epoch [4441/5000] | D Loss: 1.3297 | G Loss: 0.7454\n",
      "Epoch [4442/5000] | D Loss: 1.3786 | G Loss: 0.7259\n",
      "Epoch [4443/5000] | D Loss: 1.3479 | G Loss: 0.7352\n",
      "Epoch [4444/5000] | D Loss: 1.3793 | G Loss: 0.7217\n",
      "Epoch [4445/5000] | D Loss: 1.3688 | G Loss: 0.6965\n",
      "Epoch [4446/5000] | D Loss: 1.3925 | G Loss: 0.7250\n",
      "Epoch [4447/5000] | D Loss: 1.3656 | G Loss: 0.7453\n",
      "Epoch [4448/5000] | D Loss: 1.3525 | G Loss: 0.7612\n",
      "Epoch [4449/5000] | D Loss: 1.3474 | G Loss: 0.7139\n",
      "Epoch [4450/5000] | D Loss: 1.3391 | G Loss: 0.7331\n",
      "Epoch [4451/5000] | D Loss: 1.3210 | G Loss: 0.7137\n",
      "Epoch [4452/5000] | D Loss: 1.3549 | G Loss: 0.7250\n",
      "Epoch [4453/5000] | D Loss: 1.3847 | G Loss: 0.7085\n",
      "Epoch [4454/5000] | D Loss: 1.3566 | G Loss: 0.7308\n",
      "Epoch [4455/5000] | D Loss: 1.3426 | G Loss: 0.7278\n",
      "Epoch [4456/5000] | D Loss: 1.3406 | G Loss: 0.7116\n",
      "Epoch [4457/5000] | D Loss: 1.3475 | G Loss: 0.7165\n",
      "Epoch [4458/5000] | D Loss: 1.3594 | G Loss: 0.7165\n",
      "Epoch [4459/5000] | D Loss: 1.3697 | G Loss: 0.7103\n",
      "Epoch [4460/5000] | D Loss: 1.3233 | G Loss: 0.7292\n",
      "Epoch [4461/5000] | D Loss: 1.3379 | G Loss: 0.7221\n",
      "Epoch [4462/5000] | D Loss: 1.3741 | G Loss: 0.6988\n",
      "Epoch [4463/5000] | D Loss: 1.3487 | G Loss: 0.7150\n",
      "Epoch [4464/5000] | D Loss: 1.3506 | G Loss: 0.7371\n",
      "Epoch [4465/5000] | D Loss: 1.3276 | G Loss: 0.7241\n",
      "Epoch [4466/5000] | D Loss: 1.3587 | G Loss: 0.6887\n",
      "Epoch [4467/5000] | D Loss: 1.3903 | G Loss: 0.7090\n",
      "Epoch [4468/5000] | D Loss: 1.3565 | G Loss: 0.7081\n",
      "Epoch [4469/5000] | D Loss: 1.3567 | G Loss: 0.7180\n",
      "Epoch [4470/5000] | D Loss: 1.3505 | G Loss: 0.7167\n",
      "Epoch [4471/5000] | D Loss: 1.3607 | G Loss: 0.7548\n",
      "Epoch [4472/5000] | D Loss: 1.3343 | G Loss: 0.7311\n",
      "Epoch [4473/5000] | D Loss: 1.3659 | G Loss: 0.7314\n",
      "Epoch [4474/5000] | D Loss: 1.3768 | G Loss: 0.7258\n",
      "Epoch [4475/5000] | D Loss: 1.3597 | G Loss: 0.7518\n",
      "Epoch [4476/5000] | D Loss: 1.3657 | G Loss: 0.7530\n",
      "Epoch [4477/5000] | D Loss: 1.3473 | G Loss: 0.7112\n",
      "Epoch [4478/5000] | D Loss: 1.3554 | G Loss: 0.6915\n",
      "Epoch [4479/5000] | D Loss: 1.3972 | G Loss: 0.7251\n",
      "Epoch [4480/5000] | D Loss: 1.3465 | G Loss: 0.7156\n",
      "Epoch [4481/5000] | D Loss: 1.3467 | G Loss: 0.7067\n",
      "Epoch [4482/5000] | D Loss: 1.4164 | G Loss: 0.7176\n",
      "Epoch [4483/5000] | D Loss: 1.3805 | G Loss: 0.6963\n",
      "Epoch [4484/5000] | D Loss: 1.3489 | G Loss: 0.7357\n",
      "Epoch [4485/5000] | D Loss: 1.3724 | G Loss: 0.7469\n",
      "Epoch [4486/5000] | D Loss: 1.3343 | G Loss: 0.7492\n",
      "Epoch [4487/5000] | D Loss: 1.3520 | G Loss: 0.7276\n",
      "Epoch [4488/5000] | D Loss: 1.3700 | G Loss: 0.7232\n",
      "Epoch [4489/5000] | D Loss: 1.3414 | G Loss: 0.7511\n",
      "Epoch [4490/5000] | D Loss: 1.3779 | G Loss: 0.7388\n",
      "Epoch [4491/5000] | D Loss: 1.3477 | G Loss: 0.7391\n",
      "Epoch [4492/5000] | D Loss: 1.3691 | G Loss: 0.7278\n",
      "Epoch [4493/5000] | D Loss: 1.3957 | G Loss: 0.7552\n",
      "Epoch [4494/5000] | D Loss: 1.3718 | G Loss: 0.7331\n",
      "Epoch [4495/5000] | D Loss: 1.3456 | G Loss: 0.7443\n",
      "Epoch [4496/5000] | D Loss: 1.3536 | G Loss: 0.7173\n",
      "Epoch [4497/5000] | D Loss: 1.3772 | G Loss: 0.7277\n",
      "Epoch [4498/5000] | D Loss: 1.3771 | G Loss: 0.7194\n",
      "Epoch [4499/5000] | D Loss: 1.3465 | G Loss: 0.7553\n",
      "Epoch [4500/5000] | D Loss: 1.3465 | G Loss: 0.7444\n",
      "Epoch 4500 FID Score: 67.5263\n",
      "Reducing learning rates to Generator: 6.400000000000001e-06, Discriminator: 6.400000000000001e-06\n",
      "Epoch [4501/5000] | D Loss: 1.3666 | G Loss: 0.7334\n",
      "Epoch [4502/5000] | D Loss: 1.3247 | G Loss: 0.7118\n",
      "Epoch [4503/5000] | D Loss: 1.3788 | G Loss: 0.7164\n",
      "Epoch [4504/5000] | D Loss: 1.3284 | G Loss: 0.7326\n",
      "Epoch [4505/5000] | D Loss: 1.3751 | G Loss: 0.7311\n",
      "Epoch [4506/5000] | D Loss: 1.3827 | G Loss: 0.7435\n",
      "Epoch [4507/5000] | D Loss: 1.3631 | G Loss: 0.7197\n",
      "Epoch [4508/5000] | D Loss: 1.3540 | G Loss: 0.7245\n",
      "Epoch [4509/5000] | D Loss: 1.3524 | G Loss: 0.7407\n",
      "Epoch [4510/5000] | D Loss: 1.3172 | G Loss: 0.7260\n",
      "Epoch [4511/5000] | D Loss: 1.3659 | G Loss: 0.7217\n",
      "Epoch [4512/5000] | D Loss: 1.4195 | G Loss: 0.7079\n",
      "Epoch [4513/5000] | D Loss: 1.3546 | G Loss: 0.7190\n",
      "Epoch [4514/5000] | D Loss: 1.3973 | G Loss: 0.7220\n",
      "Epoch [4515/5000] | D Loss: 1.3594 | G Loss: 0.6906\n",
      "Epoch [4516/5000] | D Loss: 1.3353 | G Loss: 0.7239\n",
      "Epoch [4517/5000] | D Loss: 1.3721 | G Loss: 0.7395\n",
      "Epoch [4518/5000] | D Loss: 1.3463 | G Loss: 0.7251\n",
      "Epoch [4519/5000] | D Loss: 1.3465 | G Loss: 0.7266\n",
      "Epoch [4520/5000] | D Loss: 1.3961 | G Loss: 0.7279\n",
      "Epoch [4521/5000] | D Loss: 1.4009 | G Loss: 0.7524\n",
      "Epoch [4522/5000] | D Loss: 1.3512 | G Loss: 0.7033\n",
      "Epoch [4523/5000] | D Loss: 1.3600 | G Loss: 0.7157\n",
      "Epoch [4524/5000] | D Loss: 1.3254 | G Loss: 0.7167\n",
      "Epoch [4525/5000] | D Loss: 1.3466 | G Loss: 0.7105\n",
      "Epoch [4526/5000] | D Loss: 1.3642 | G Loss: 0.6980\n",
      "Epoch [4527/5000] | D Loss: 1.3484 | G Loss: 0.7693\n",
      "Epoch [4528/5000] | D Loss: 1.3580 | G Loss: 0.7414\n",
      "Epoch [4529/5000] | D Loss: 1.3508 | G Loss: 0.7364\n",
      "Epoch [4530/5000] | D Loss: 1.3386 | G Loss: 0.7575\n",
      "Epoch [4531/5000] | D Loss: 1.3600 | G Loss: 0.7122\n",
      "Epoch [4532/5000] | D Loss: 1.3744 | G Loss: 0.7347\n",
      "Epoch [4533/5000] | D Loss: 1.3818 | G Loss: 0.7185\n",
      "Epoch [4534/5000] | D Loss: 1.3534 | G Loss: 0.7313\n",
      "Epoch [4535/5000] | D Loss: 1.3505 | G Loss: 0.7331\n",
      "Epoch [4536/5000] | D Loss: 1.3489 | G Loss: 0.7151\n",
      "Epoch [4537/5000] | D Loss: 1.3813 | G Loss: 0.7255\n",
      "Epoch [4538/5000] | D Loss: 1.3510 | G Loss: 0.7415\n",
      "Epoch [4539/5000] | D Loss: 1.3439 | G Loss: 0.7450\n",
      "Epoch [4540/5000] | D Loss: 1.3824 | G Loss: 0.7338\n",
      "Epoch [4541/5000] | D Loss: 1.3917 | G Loss: 0.7456\n",
      "Epoch [4542/5000] | D Loss: 1.3275 | G Loss: 0.7616\n",
      "Epoch [4543/5000] | D Loss: 1.3764 | G Loss: 0.7287\n",
      "Epoch [4544/5000] | D Loss: 1.3507 | G Loss: 0.6984\n",
      "Epoch [4545/5000] | D Loss: 1.3667 | G Loss: 0.7434\n",
      "Epoch [4546/5000] | D Loss: 1.3549 | G Loss: 0.7437\n",
      "Epoch [4547/5000] | D Loss: 1.3710 | G Loss: 0.7278\n",
      "Epoch [4548/5000] | D Loss: 1.3976 | G Loss: 0.7045\n",
      "Epoch [4549/5000] | D Loss: 1.3655 | G Loss: 0.7429\n",
      "Epoch [4550/5000] | D Loss: 1.3522 | G Loss: 0.7387\n",
      "Epoch [4551/5000] | D Loss: 1.3522 | G Loss: 0.7197\n",
      "Epoch [4552/5000] | D Loss: 1.3717 | G Loss: 0.7399\n",
      "Epoch [4553/5000] | D Loss: 1.3496 | G Loss: 0.7113\n",
      "Epoch [4554/5000] | D Loss: 1.3768 | G Loss: 0.7121\n",
      "Epoch [4555/5000] | D Loss: 1.3686 | G Loss: 0.7330\n",
      "Epoch [4556/5000] | D Loss: 1.3567 | G Loss: 0.7297\n",
      "Epoch [4557/5000] | D Loss: 1.3278 | G Loss: 0.7025\n",
      "Epoch [4558/5000] | D Loss: 1.3753 | G Loss: 0.7327\n",
      "Epoch [4559/5000] | D Loss: 1.3697 | G Loss: 0.7361\n",
      "Epoch [4560/5000] | D Loss: 1.3775 | G Loss: 0.7312\n",
      "Epoch [4561/5000] | D Loss: 1.3519 | G Loss: 0.7274\n",
      "Epoch [4562/5000] | D Loss: 1.3479 | G Loss: 0.7054\n",
      "Epoch [4563/5000] | D Loss: 1.3628 | G Loss: 0.7412\n",
      "Epoch [4564/5000] | D Loss: 1.3208 | G Loss: 0.7252\n",
      "Epoch [4565/5000] | D Loss: 1.3441 | G Loss: 0.7432\n",
      "Epoch [4566/5000] | D Loss: 1.3447 | G Loss: 0.7465\n",
      "Epoch [4567/5000] | D Loss: 1.3495 | G Loss: 0.7623\n",
      "Epoch [4568/5000] | D Loss: 1.3642 | G Loss: 0.7603\n",
      "Epoch [4569/5000] | D Loss: 1.3629 | G Loss: 0.7426\n",
      "Epoch [4570/5000] | D Loss: 1.3752 | G Loss: 0.7254\n",
      "Epoch [4571/5000] | D Loss: 1.3282 | G Loss: 0.7183\n",
      "Epoch [4572/5000] | D Loss: 1.3683 | G Loss: 0.7247\n",
      "Epoch [4573/5000] | D Loss: 1.3515 | G Loss: 0.7243\n",
      "Epoch [4574/5000] | D Loss: 1.3408 | G Loss: 0.7460\n",
      "Epoch [4575/5000] | D Loss: 1.3495 | G Loss: 0.7527\n",
      "Epoch [4576/5000] | D Loss: 1.3384 | G Loss: 0.7403\n",
      "Epoch [4577/5000] | D Loss: 1.3847 | G Loss: 0.7234\n",
      "Epoch [4578/5000] | D Loss: 1.3676 | G Loss: 0.7387\n",
      "Epoch [4579/5000] | D Loss: 1.3438 | G Loss: 0.7271\n",
      "Epoch [4580/5000] | D Loss: 1.3604 | G Loss: 0.7205\n",
      "Epoch [4581/5000] | D Loss: 1.3786 | G Loss: 0.6968\n",
      "Epoch [4582/5000] | D Loss: 1.3532 | G Loss: 0.7600\n",
      "Epoch [4583/5000] | D Loss: 1.3732 | G Loss: 0.7389\n",
      "Epoch [4584/5000] | D Loss: 1.3631 | G Loss: 0.7274\n",
      "Epoch [4585/5000] | D Loss: 1.3823 | G Loss: 0.7776\n",
      "Epoch [4586/5000] | D Loss: 1.3769 | G Loss: 0.7323\n",
      "Epoch [4587/5000] | D Loss: 1.3693 | G Loss: 0.7488\n",
      "Epoch [4588/5000] | D Loss: 1.3331 | G Loss: 0.7507\n",
      "Epoch [4589/5000] | D Loss: 1.3801 | G Loss: 0.7115\n",
      "Epoch [4590/5000] | D Loss: 1.3683 | G Loss: 0.7655\n",
      "Epoch [4591/5000] | D Loss: 1.3327 | G Loss: 0.7199\n",
      "Epoch [4592/5000] | D Loss: 1.3606 | G Loss: 0.7084\n",
      "Epoch [4593/5000] | D Loss: 1.3621 | G Loss: 0.7269\n",
      "Epoch [4594/5000] | D Loss: 1.3571 | G Loss: 0.7368\n",
      "Epoch [4595/5000] | D Loss: 1.3485 | G Loss: 0.7229\n",
      "Epoch [4596/5000] | D Loss: 1.3577 | G Loss: 0.7420\n",
      "Epoch [4597/5000] | D Loss: 1.3826 | G Loss: 0.7490\n",
      "Epoch [4598/5000] | D Loss: 1.3756 | G Loss: 0.7165\n",
      "Epoch [4599/5000] | D Loss: 1.3616 | G Loss: 0.7088\n",
      "Epoch [4600/5000] | D Loss: 1.4118 | G Loss: 0.7382\n",
      "Epoch 4600 FID Score: 68.1807\n",
      "Epoch [4601/5000] | D Loss: 1.3936 | G Loss: 0.7231\n",
      "Epoch [4602/5000] | D Loss: 1.3598 | G Loss: 0.7069\n",
      "Epoch [4603/5000] | D Loss: 1.3037 | G Loss: 0.7351\n",
      "Epoch [4604/5000] | D Loss: 1.3560 | G Loss: 0.7371\n",
      "Epoch [4605/5000] | D Loss: 1.3501 | G Loss: 0.7370\n",
      "Epoch [4606/5000] | D Loss: 1.3589 | G Loss: 0.6826\n",
      "Epoch [4607/5000] | D Loss: 1.3513 | G Loss: 0.7453\n",
      "Epoch [4608/5000] | D Loss: 1.3647 | G Loss: 0.7334\n",
      "Epoch [4609/5000] | D Loss: 1.3800 | G Loss: 0.7133\n",
      "Epoch [4610/5000] | D Loss: 1.3802 | G Loss: 0.7340\n",
      "Epoch [4611/5000] | D Loss: 1.3602 | G Loss: 0.7144\n",
      "Epoch [4612/5000] | D Loss: 1.3389 | G Loss: 0.7381\n",
      "Epoch [4613/5000] | D Loss: 1.3077 | G Loss: 0.7397\n",
      "Epoch [4614/5000] | D Loss: 1.3536 | G Loss: 0.7006\n",
      "Epoch [4615/5000] | D Loss: 1.3355 | G Loss: 0.7136\n",
      "Epoch [4616/5000] | D Loss: 1.3433 | G Loss: 0.7046\n",
      "Epoch [4617/5000] | D Loss: 1.3468 | G Loss: 0.7192\n",
      "Epoch [4618/5000] | D Loss: 1.3645 | G Loss: 0.7245\n",
      "Epoch [4619/5000] | D Loss: 1.3379 | G Loss: 0.7465\n",
      "Epoch [4620/5000] | D Loss: 1.3545 | G Loss: 0.7303\n",
      "Epoch [4621/5000] | D Loss: 1.3734 | G Loss: 0.7797\n",
      "Epoch [4622/5000] | D Loss: 1.3587 | G Loss: 0.7211\n",
      "Epoch [4623/5000] | D Loss: 1.3255 | G Loss: 0.7232\n",
      "Epoch [4624/5000] | D Loss: 1.3470 | G Loss: 0.7382\n",
      "Epoch [4625/5000] | D Loss: 1.3604 | G Loss: 0.7174\n",
      "Epoch [4626/5000] | D Loss: 1.3252 | G Loss: 0.6969\n",
      "Epoch [4627/5000] | D Loss: 1.3743 | G Loss: 0.7256\n",
      "Epoch [4628/5000] | D Loss: 1.3581 | G Loss: 0.7661\n",
      "Epoch [4629/5000] | D Loss: 1.3480 | G Loss: 0.7567\n",
      "Epoch [4630/5000] | D Loss: 1.3658 | G Loss: 0.7077\n",
      "Epoch [4631/5000] | D Loss: 1.3314 | G Loss: 0.7087\n",
      "Epoch [4632/5000] | D Loss: 1.3858 | G Loss: 0.7051\n",
      "Epoch [4633/5000] | D Loss: 1.3343 | G Loss: 0.7454\n",
      "Epoch [4634/5000] | D Loss: 1.3808 | G Loss: 0.7416\n",
      "Epoch [4635/5000] | D Loss: 1.3921 | G Loss: 0.7467\n",
      "Epoch [4636/5000] | D Loss: 1.3566 | G Loss: 0.7214\n",
      "Epoch [4637/5000] | D Loss: 1.3644 | G Loss: 0.7771\n",
      "Epoch [4638/5000] | D Loss: 1.3728 | G Loss: 0.7146\n",
      "Epoch [4639/5000] | D Loss: 1.3800 | G Loss: 0.7379\n",
      "Epoch [4640/5000] | D Loss: 1.3754 | G Loss: 0.7402\n",
      "Epoch [4641/5000] | D Loss: 1.3433 | G Loss: 0.7340\n",
      "Epoch [4642/5000] | D Loss: 1.3479 | G Loss: 0.7399\n",
      "Epoch [4643/5000] | D Loss: 1.3293 | G Loss: 0.7636\n",
      "Epoch [4644/5000] | D Loss: 1.3799 | G Loss: 0.7131\n",
      "Epoch [4645/5000] | D Loss: 1.3716 | G Loss: 0.7288\n",
      "Epoch [4646/5000] | D Loss: 1.3345 | G Loss: 0.7207\n",
      "Epoch [4647/5000] | D Loss: 1.3394 | G Loss: 0.7211\n",
      "Epoch [4648/5000] | D Loss: 1.3093 | G Loss: 0.6983\n",
      "Epoch [4649/5000] | D Loss: 1.3348 | G Loss: 0.7188\n",
      "Epoch [4650/5000] | D Loss: 1.3511 | G Loss: 0.7479\n",
      "Epoch [4651/5000] | D Loss: 1.3703 | G Loss: 0.7197\n",
      "Epoch [4652/5000] | D Loss: 1.3590 | G Loss: 0.7375\n",
      "Epoch [4653/5000] | D Loss: 1.3763 | G Loss: 0.7127\n",
      "Epoch [4654/5000] | D Loss: 1.3475 | G Loss: 0.7173\n",
      "Epoch [4655/5000] | D Loss: 1.3829 | G Loss: 0.7228\n",
      "Epoch [4656/5000] | D Loss: 1.3898 | G Loss: 0.7233\n",
      "Epoch [4657/5000] | D Loss: 1.3560 | G Loss: 0.7384\n",
      "Epoch [4658/5000] | D Loss: 1.3565 | G Loss: 0.7102\n",
      "Epoch [4659/5000] | D Loss: 1.4031 | G Loss: 0.7422\n",
      "Epoch [4660/5000] | D Loss: 1.3646 | G Loss: 0.7082\n",
      "Epoch [4661/5000] | D Loss: 1.3550 | G Loss: 0.7371\n",
      "Epoch [4662/5000] | D Loss: 1.3671 | G Loss: 0.7392\n",
      "Epoch [4663/5000] | D Loss: 1.3263 | G Loss: 0.7144\n",
      "Epoch [4664/5000] | D Loss: 1.3775 | G Loss: 0.7434\n",
      "Epoch [4665/5000] | D Loss: 1.3493 | G Loss: 0.7218\n",
      "Epoch [4666/5000] | D Loss: 1.3246 | G Loss: 0.7285\n",
      "Epoch [4667/5000] | D Loss: 1.3585 | G Loss: 0.7277\n",
      "Epoch [4668/5000] | D Loss: 1.3535 | G Loss: 0.7136\n",
      "Epoch [4669/5000] | D Loss: 1.3518 | G Loss: 0.7238\n",
      "Epoch [4670/5000] | D Loss: 1.3747 | G Loss: 0.7471\n",
      "Epoch [4671/5000] | D Loss: 1.3618 | G Loss: 0.7188\n",
      "Epoch [4672/5000] | D Loss: 1.3416 | G Loss: 0.7062\n",
      "Epoch [4673/5000] | D Loss: 1.3739 | G Loss: 0.7180\n",
      "Epoch [4674/5000] | D Loss: 1.4009 | G Loss: 0.7036\n",
      "Epoch [4675/5000] | D Loss: 1.3756 | G Loss: 0.7487\n",
      "Epoch [4676/5000] | D Loss: 1.3859 | G Loss: 0.7197\n",
      "Epoch [4677/5000] | D Loss: 1.3540 | G Loss: 0.7089\n",
      "Epoch [4678/5000] | D Loss: 1.3555 | G Loss: 0.7056\n",
      "Epoch [4679/5000] | D Loss: 1.3747 | G Loss: 0.7053\n",
      "Epoch [4680/5000] | D Loss: 1.3405 | G Loss: 0.7233\n",
      "Epoch [4681/5000] | D Loss: 1.3611 | G Loss: 0.7383\n",
      "Epoch [4682/5000] | D Loss: 1.3421 | G Loss: 0.7058\n",
      "Epoch [4683/5000] | D Loss: 1.3865 | G Loss: 0.7010\n",
      "Epoch [4684/5000] | D Loss: 1.3550 | G Loss: 0.6966\n",
      "Epoch [4685/5000] | D Loss: 1.3361 | G Loss: 0.7099\n",
      "Epoch [4686/5000] | D Loss: 1.3870 | G Loss: 0.6880\n",
      "Epoch [4687/5000] | D Loss: 1.3451 | G Loss: 0.7189\n",
      "Epoch [4688/5000] | D Loss: 1.3632 | G Loss: 0.7406\n",
      "Epoch [4689/5000] | D Loss: 1.3643 | G Loss: 0.7447\n",
      "Epoch [4690/5000] | D Loss: 1.3582 | G Loss: 0.7134\n",
      "Epoch [4691/5000] | D Loss: 1.3585 | G Loss: 0.6857\n",
      "Epoch [4692/5000] | D Loss: 1.3522 | G Loss: 0.6991\n",
      "Epoch [4693/5000] | D Loss: 1.3615 | G Loss: 0.7529\n",
      "Epoch [4694/5000] | D Loss: 1.3546 | G Loss: 0.7281\n",
      "Epoch [4695/5000] | D Loss: 1.3465 | G Loss: 0.7380\n",
      "Epoch [4696/5000] | D Loss: 1.3616 | G Loss: 0.7284\n",
      "Epoch [4697/5000] | D Loss: 1.3698 | G Loss: 0.7015\n",
      "Epoch [4698/5000] | D Loss: 1.3491 | G Loss: 0.7064\n",
      "Epoch [4699/5000] | D Loss: 1.3837 | G Loss: 0.7517\n",
      "Epoch [4700/5000] | D Loss: 1.3697 | G Loss: 0.7412\n",
      "Epoch 4700 FID Score: 67.4671\n",
      "Reducing learning rates to Generator: 2.560000000000001e-06, Discriminator: 2.560000000000001e-06\n",
      "Epoch [4701/5000] | D Loss: 1.3676 | G Loss: 0.7174\n",
      "Epoch [4702/5000] | D Loss: 1.3337 | G Loss: 0.6675\n",
      "Epoch [4703/5000] | D Loss: 1.3719 | G Loss: 0.7369\n",
      "Epoch [4704/5000] | D Loss: 1.3318 | G Loss: 0.7479\n",
      "Epoch [4705/5000] | D Loss: 1.2735 | G Loss: 0.7008\n",
      "Epoch [4706/5000] | D Loss: 1.3564 | G Loss: 0.7282\n",
      "Epoch [4707/5000] | D Loss: 1.3624 | G Loss: 0.7232\n",
      "Epoch [4708/5000] | D Loss: 1.3326 | G Loss: 0.7269\n",
      "Epoch [4709/5000] | D Loss: 1.3867 | G Loss: 0.7534\n",
      "Epoch [4710/5000] | D Loss: 1.3737 | G Loss: 0.7355\n",
      "Epoch [4711/5000] | D Loss: 1.4019 | G Loss: 0.7231\n",
      "Epoch [4712/5000] | D Loss: 1.3673 | G Loss: 0.7509\n",
      "Epoch [4713/5000] | D Loss: 1.3767 | G Loss: 0.7368\n",
      "Epoch [4714/5000] | D Loss: 1.3769 | G Loss: 0.7321\n",
      "Epoch [4715/5000] | D Loss: 1.3313 | G Loss: 0.7235\n",
      "Epoch [4716/5000] | D Loss: 1.3514 | G Loss: 0.7203\n",
      "Epoch [4717/5000] | D Loss: 1.3353 | G Loss: 0.7424\n",
      "Epoch [4718/5000] | D Loss: 1.3809 | G Loss: 0.7360\n",
      "Epoch [4719/5000] | D Loss: 1.3429 | G Loss: 0.7418\n",
      "Epoch [4720/5000] | D Loss: 1.3331 | G Loss: 0.7288\n",
      "Epoch [4721/5000] | D Loss: 1.3587 | G Loss: 0.7136\n",
      "Epoch [4722/5000] | D Loss: 1.3675 | G Loss: 0.7145\n",
      "Epoch [4723/5000] | D Loss: 1.3727 | G Loss: 0.7339\n",
      "Epoch [4724/5000] | D Loss: 1.3830 | G Loss: 0.7552\n",
      "Epoch [4725/5000] | D Loss: 1.3607 | G Loss: 0.7519\n",
      "Epoch [4726/5000] | D Loss: 1.3796 | G Loss: 0.7334\n",
      "Epoch [4727/5000] | D Loss: 1.3564 | G Loss: 0.7371\n",
      "Epoch [4728/5000] | D Loss: 1.3950 | G Loss: 0.7166\n",
      "Epoch [4729/5000] | D Loss: 1.3732 | G Loss: 0.7340\n",
      "Epoch [4730/5000] | D Loss: 1.3623 | G Loss: 0.7409\n",
      "Epoch [4731/5000] | D Loss: 1.4080 | G Loss: 0.7511\n",
      "Epoch [4732/5000] | D Loss: 1.3476 | G Loss: 0.7196\n",
      "Epoch [4733/5000] | D Loss: 1.3843 | G Loss: 0.7430\n",
      "Epoch [4734/5000] | D Loss: 1.3427 | G Loss: 0.7104\n",
      "Epoch [4735/5000] | D Loss: 1.3346 | G Loss: 0.7316\n",
      "Epoch [4736/5000] | D Loss: 1.3311 | G Loss: 0.7258\n",
      "Epoch [4737/5000] | D Loss: 1.3644 | G Loss: 0.7353\n",
      "Epoch [4738/5000] | D Loss: 1.3512 | G Loss: 0.7500\n",
      "Epoch [4739/5000] | D Loss: 1.3554 | G Loss: 0.7366\n",
      "Epoch [4740/5000] | D Loss: 1.3421 | G Loss: 0.7185\n",
      "Epoch [4741/5000] | D Loss: 1.3462 | G Loss: 0.7135\n",
      "Epoch [4742/5000] | D Loss: 1.3505 | G Loss: 0.7273\n",
      "Epoch [4743/5000] | D Loss: 1.3570 | G Loss: 0.7218\n",
      "Epoch [4744/5000] | D Loss: 1.3543 | G Loss: 0.7370\n",
      "Epoch [4745/5000] | D Loss: 1.3836 | G Loss: 0.6787\n",
      "Epoch [4746/5000] | D Loss: 1.3822 | G Loss: 0.7211\n",
      "Epoch [4747/5000] | D Loss: 1.3396 | G Loss: 0.7068\n",
      "Epoch [4748/5000] | D Loss: 1.3653 | G Loss: 0.7198\n",
      "Epoch [4749/5000] | D Loss: 1.3918 | G Loss: 0.7400\n",
      "Epoch [4750/5000] | D Loss: 1.3136 | G Loss: 0.7186\n",
      "Epoch [4751/5000] | D Loss: 1.3609 | G Loss: 0.7136\n",
      "Epoch [4752/5000] | D Loss: 1.3438 | G Loss: 0.7350\n",
      "Epoch [4753/5000] | D Loss: 1.3531 | G Loss: 0.6747\n",
      "Epoch [4754/5000] | D Loss: 1.3501 | G Loss: 0.7204\n",
      "Epoch [4755/5000] | D Loss: 1.3770 | G Loss: 0.7257\n",
      "Epoch [4756/5000] | D Loss: 1.3620 | G Loss: 0.7144\n",
      "Epoch [4757/5000] | D Loss: 1.3741 | G Loss: 0.6938\n",
      "Epoch [4758/5000] | D Loss: 1.3345 | G Loss: 0.7507\n",
      "Epoch [4759/5000] | D Loss: 1.4070 | G Loss: 0.6924\n",
      "Epoch [4760/5000] | D Loss: 1.3648 | G Loss: 0.7263\n",
      "Epoch [4761/5000] | D Loss: 1.3952 | G Loss: 0.7357\n",
      "Epoch [4762/5000] | D Loss: 1.3684 | G Loss: 0.7024\n",
      "Epoch [4763/5000] | D Loss: 1.3505 | G Loss: 0.7522\n",
      "Epoch [4764/5000] | D Loss: 1.3516 | G Loss: 0.7413\n",
      "Epoch [4765/5000] | D Loss: 1.3754 | G Loss: 0.7271\n",
      "Epoch [4766/5000] | D Loss: 1.3558 | G Loss: 0.7272\n",
      "Epoch [4767/5000] | D Loss: 1.3593 | G Loss: 0.7366\n",
      "Epoch [4768/5000] | D Loss: 1.3743 | G Loss: 0.7425\n",
      "Epoch [4769/5000] | D Loss: 1.3369 | G Loss: 0.7326\n",
      "Epoch [4770/5000] | D Loss: 1.3368 | G Loss: 0.7461\n",
      "Epoch [4771/5000] | D Loss: 1.3811 | G Loss: 0.7172\n",
      "Epoch [4772/5000] | D Loss: 1.3887 | G Loss: 0.7350\n",
      "Epoch [4773/5000] | D Loss: 1.3233 | G Loss: 0.7027\n",
      "Epoch [4774/5000] | D Loss: 1.3378 | G Loss: 0.7635\n",
      "Epoch [4775/5000] | D Loss: 1.3621 | G Loss: 0.7265\n",
      "Epoch [4776/5000] | D Loss: 1.3739 | G Loss: 0.7357\n",
      "Epoch [4777/5000] | D Loss: 1.3574 | G Loss: 0.7348\n",
      "Epoch [4778/5000] | D Loss: 1.3620 | G Loss: 0.7309\n",
      "Epoch [4779/5000] | D Loss: 1.3778 | G Loss: 0.7076\n",
      "Epoch [4780/5000] | D Loss: 1.3511 | G Loss: 0.7616\n",
      "Epoch [4781/5000] | D Loss: 1.3512 | G Loss: 0.7545\n",
      "Epoch [4782/5000] | D Loss: 1.3351 | G Loss: 0.7430\n",
      "Epoch [4783/5000] | D Loss: 1.3396 | G Loss: 0.7034\n",
      "Epoch [4784/5000] | D Loss: 1.3526 | G Loss: 0.7272\n",
      "Epoch [4785/5000] | D Loss: 1.3477 | G Loss: 0.7419\n",
      "Epoch [4786/5000] | D Loss: 1.3641 | G Loss: 0.7176\n",
      "Epoch [4787/5000] | D Loss: 1.3726 | G Loss: 0.7126\n",
      "Epoch [4788/5000] | D Loss: 1.3532 | G Loss: 0.7041\n",
      "Epoch [4789/5000] | D Loss: 1.3673 | G Loss: 0.6986\n",
      "Epoch [4790/5000] | D Loss: 1.3386 | G Loss: 0.7389\n",
      "Epoch [4791/5000] | D Loss: 1.3346 | G Loss: 0.7158\n",
      "Epoch [4792/5000] | D Loss: 1.3594 | G Loss: 0.7609\n",
      "Epoch [4793/5000] | D Loss: 1.3789 | G Loss: 0.7105\n",
      "Epoch [4794/5000] | D Loss: 1.3859 | G Loss: 0.7099\n",
      "Epoch [4795/5000] | D Loss: 1.3484 | G Loss: 0.7050\n",
      "Epoch [4796/5000] | D Loss: 1.3621 | G Loss: 0.7227\n",
      "Epoch [4797/5000] | D Loss: 1.3541 | G Loss: 0.7062\n",
      "Epoch [4798/5000] | D Loss: 1.3779 | G Loss: 0.6998\n",
      "Epoch [4799/5000] | D Loss: 1.4168 | G Loss: 0.7211\n",
      "Epoch [4800/5000] | D Loss: 1.3604 | G Loss: 0.7070\n",
      "Epoch 4800 FID Score: 67.4993\n",
      "Epoch [4801/5000] | D Loss: 1.3728 | G Loss: 0.7228\n",
      "Epoch [4802/5000] | D Loss: 1.3427 | G Loss: 0.6907\n",
      "Epoch [4803/5000] | D Loss: 1.3691 | G Loss: 0.7349\n",
      "Epoch [4804/5000] | D Loss: 1.3601 | G Loss: 0.7412\n",
      "Epoch [4805/5000] | D Loss: 1.3308 | G Loss: 0.6898\n",
      "Epoch [4806/5000] | D Loss: 1.3708 | G Loss: 0.7496\n",
      "Epoch [4807/5000] | D Loss: 1.3757 | G Loss: 0.7227\n",
      "Epoch [4808/5000] | D Loss: 1.3562 | G Loss: 0.7346\n",
      "Epoch [4809/5000] | D Loss: 1.3484 | G Loss: 0.7247\n",
      "Epoch [4810/5000] | D Loss: 1.3333 | G Loss: 0.7256\n",
      "Epoch [4811/5000] | D Loss: 1.3461 | G Loss: 0.7148\n",
      "Epoch [4812/5000] | D Loss: 1.3422 | G Loss: 0.7414\n",
      "Epoch [4813/5000] | D Loss: 1.3914 | G Loss: 0.7093\n",
      "Epoch [4814/5000] | D Loss: 1.3550 | G Loss: 0.7487\n",
      "Epoch [4815/5000] | D Loss: 1.3747 | G Loss: 0.7426\n",
      "Epoch [4816/5000] | D Loss: 1.3483 | G Loss: 0.7142\n",
      "Epoch [4817/5000] | D Loss: 1.3825 | G Loss: 0.7129\n",
      "Epoch [4818/5000] | D Loss: 1.3869 | G Loss: 0.7286\n",
      "Epoch [4819/5000] | D Loss: 1.3598 | G Loss: 0.7617\n",
      "Epoch [4820/5000] | D Loss: 1.3702 | G Loss: 0.7268\n",
      "Epoch [4821/5000] | D Loss: 1.3567 | G Loss: 0.7505\n",
      "Epoch [4822/5000] | D Loss: 1.3569 | G Loss: 0.7046\n",
      "Epoch [4823/5000] | D Loss: 1.3847 | G Loss: 0.7264\n",
      "Epoch [4824/5000] | D Loss: 1.3613 | G Loss: 0.7577\n",
      "Epoch [4825/5000] | D Loss: 1.3619 | G Loss: 0.6978\n",
      "Epoch [4826/5000] | D Loss: 1.3728 | G Loss: 0.7256\n",
      "Epoch [4827/5000] | D Loss: 1.3596 | G Loss: 0.7235\n",
      "Epoch [4828/5000] | D Loss: 1.3603 | G Loss: 0.7209\n",
      "Epoch [4829/5000] | D Loss: 1.3531 | G Loss: 0.7746\n",
      "Epoch [4830/5000] | D Loss: 1.3789 | G Loss: 0.7104\n",
      "Epoch [4831/5000] | D Loss: 1.3632 | G Loss: 0.7346\n",
      "Epoch [4832/5000] | D Loss: 1.3125 | G Loss: 0.7446\n",
      "Epoch [4833/5000] | D Loss: 1.3527 | G Loss: 0.7229\n",
      "Epoch [4834/5000] | D Loss: 1.3604 | G Loss: 0.7419\n",
      "Epoch [4835/5000] | D Loss: 1.3209 | G Loss: 0.7260\n",
      "Epoch [4836/5000] | D Loss: 1.3216 | G Loss: 0.7698\n",
      "Epoch [4837/5000] | D Loss: 1.3775 | G Loss: 0.7322\n",
      "Epoch [4838/5000] | D Loss: 1.3624 | G Loss: 0.7525\n",
      "Epoch [4839/5000] | D Loss: 1.3531 | G Loss: 0.7283\n",
      "Epoch [4840/5000] | D Loss: 1.3319 | G Loss: 0.7533\n",
      "Epoch [4841/5000] | D Loss: 1.3650 | G Loss: 0.6995\n",
      "Epoch [4842/5000] | D Loss: 1.3498 | G Loss: 0.7516\n",
      "Epoch [4843/5000] | D Loss: 1.3445 | G Loss: 0.7148\n",
      "Epoch [4844/5000] | D Loss: 1.3544 | G Loss: 0.7238\n",
      "Epoch [4845/5000] | D Loss: 1.3452 | G Loss: 0.7111\n",
      "Epoch [4846/5000] | D Loss: 1.3526 | G Loss: 0.7249\n",
      "Epoch [4847/5000] | D Loss: 1.3352 | G Loss: 0.7286\n",
      "Epoch [4848/5000] | D Loss: 1.3443 | G Loss: 0.7505\n",
      "Epoch [4849/5000] | D Loss: 1.3896 | G Loss: 0.7137\n",
      "Epoch [4850/5000] | D Loss: 1.3630 | G Loss: 0.7045\n",
      "Epoch [4851/5000] | D Loss: 1.3347 | G Loss: 0.7583\n",
      "Epoch [4852/5000] | D Loss: 1.3731 | G Loss: 0.6958\n",
      "Epoch [4853/5000] | D Loss: 1.3720 | G Loss: 0.7469\n",
      "Epoch [4854/5000] | D Loss: 1.3484 | G Loss: 0.7129\n",
      "Epoch [4855/5000] | D Loss: 1.3931 | G Loss: 0.7454\n",
      "Epoch [4856/5000] | D Loss: 1.3796 | G Loss: 0.7393\n",
      "Epoch [4857/5000] | D Loss: 1.3903 | G Loss: 0.7234\n",
      "Epoch [4858/5000] | D Loss: 1.3399 | G Loss: 0.7263\n",
      "Epoch [4859/5000] | D Loss: 1.3456 | G Loss: 0.7114\n",
      "Epoch [4860/5000] | D Loss: 1.3537 | G Loss: 0.7196\n",
      "Epoch [4861/5000] | D Loss: 1.3639 | G Loss: 0.7343\n",
      "Epoch [4862/5000] | D Loss: 1.3611 | G Loss: 0.7296\n",
      "Epoch [4863/5000] | D Loss: 1.3530 | G Loss: 0.6989\n",
      "Epoch [4864/5000] | D Loss: 1.3600 | G Loss: 0.7338\n",
      "Epoch [4865/5000] | D Loss: 1.3695 | G Loss: 0.7075\n",
      "Epoch [4866/5000] | D Loss: 1.3912 | G Loss: 0.7558\n",
      "Epoch [4867/5000] | D Loss: 1.3353 | G Loss: 0.7105\n",
      "Epoch [4868/5000] | D Loss: 1.3250 | G Loss: 0.7220\n",
      "Epoch [4869/5000] | D Loss: 1.3881 | G Loss: 0.7093\n",
      "Epoch [4870/5000] | D Loss: 1.3273 | G Loss: 0.7280\n",
      "Epoch [4871/5000] | D Loss: 1.3892 | G Loss: 0.7191\n",
      "Epoch [4872/5000] | D Loss: 1.3904 | G Loss: 0.7263\n",
      "Epoch [4873/5000] | D Loss: 1.3662 | G Loss: 0.7152\n",
      "Epoch [4874/5000] | D Loss: 1.3944 | G Loss: 0.7249\n",
      "Epoch [4875/5000] | D Loss: 1.3786 | G Loss: 0.7343\n",
      "Epoch [4876/5000] | D Loss: 1.3473 | G Loss: 0.7199\n",
      "Epoch [4877/5000] | D Loss: 1.3400 | G Loss: 0.7174\n",
      "Epoch [4878/5000] | D Loss: 1.3650 | G Loss: 0.7460\n",
      "Epoch [4879/5000] | D Loss: 1.3508 | G Loss: 0.6909\n",
      "Epoch [4880/5000] | D Loss: 1.3480 | G Loss: 0.7248\n",
      "Epoch [4881/5000] | D Loss: 1.3764 | G Loss: 0.7313\n",
      "Epoch [4882/5000] | D Loss: 1.3723 | G Loss: 0.7396\n",
      "Epoch [4883/5000] | D Loss: 1.3464 | G Loss: 0.7023\n",
      "Epoch [4884/5000] | D Loss: 1.3601 | G Loss: 0.7418\n",
      "Epoch [4885/5000] | D Loss: 1.3296 | G Loss: 0.7025\n",
      "Epoch [4886/5000] | D Loss: 1.3687 | G Loss: 0.7283\n",
      "Epoch [4887/5000] | D Loss: 1.3718 | G Loss: 0.7038\n",
      "Epoch [4888/5000] | D Loss: 1.3700 | G Loss: 0.7323\n",
      "Epoch [4889/5000] | D Loss: 1.3181 | G Loss: 0.7511\n",
      "Epoch [4890/5000] | D Loss: 1.3362 | G Loss: 0.7161\n",
      "Epoch [4891/5000] | D Loss: 1.3644 | G Loss: 0.7241\n",
      "Epoch [4892/5000] | D Loss: 1.3358 | G Loss: 0.7191\n",
      "Epoch [4893/5000] | D Loss: 1.3724 | G Loss: 0.7542\n",
      "Epoch [4894/5000] | D Loss: 1.3951 | G Loss: 0.7340\n",
      "Epoch [4895/5000] | D Loss: 1.3494 | G Loss: 0.7211\n",
      "Epoch [4896/5000] | D Loss: 1.3755 | G Loss: 0.7429\n",
      "Epoch [4897/5000] | D Loss: 1.3477 | G Loss: 0.7413\n",
      "Epoch [4898/5000] | D Loss: 1.3582 | G Loss: 0.7393\n",
      "Epoch [4899/5000] | D Loss: 1.3520 | G Loss: 0.6976\n",
      "Epoch [4900/5000] | D Loss: 1.3832 | G Loss: 0.7240\n",
      "Epoch 4900 FID Score: 67.9238\n",
      "Reducing learning rates to Generator: 1.0240000000000005e-06, Discriminator: 1.0240000000000005e-06\n",
      "No improvement in FID for 6 intervals. Stopping early at Epoch 4900.\n",
      "Training complete. Best model saved with FID: 66.20172112318816\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import models, transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.models import Inception_V3_Weights\n",
    "\n",
    "# Device Configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Using NVIDIA GPU (CUDA)')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print('Using Mac GPU (MPS)')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using CPU')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_parquet('../data/processed_emoji_dataset.parquet')\n",
    "df[\"combined_embedding\"] = df[\"combined_embedding\"].apply(lambda x: np.array(x, dtype=np.float32))\n",
    "\n",
    "# Custom Dataset Class\n",
    "class EmojiDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        embedding = torch.tensor(self.dataframe.iloc[idx]['combined_embedding']).float()\n",
    "        image_tensor = torch.load(self.dataframe.iloc[idx]['image_path']).float()\n",
    "        return embedding, image_tensor\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = EmojiDataset(df)\n",
    "# Splitting data to training and testing sets\n",
    "train_samples = int(round(len(dataset)*0.90))\n",
    "train_set, val_set = random_split(dataset, [train_samples, len(dataset) - train_samples])\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=64, shuffle=False)\n",
    "\n",
    "# Directories to save\n",
    "os.makedirs(\"../evaluation/emoji_cgan/train_output\", exist_ok=True)\n",
    "os.makedirs(\"../evaluation/emoji_cgan/val_output\", exist_ok=True)\n",
    "os.makedirs(\"../saved_models\", exist_ok=True)\n",
    "train_output_dir = \"../evaluation/emoji_cgan/train_output\"\n",
    "val_output_dir = \"../evaluation/emoji_cgan/val_output\"\n",
    "val_models_dir = \"../saved_models\"\n",
    "\n",
    "\"\"\"\n",
    "Reference: https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html\n",
    "Learning ConvTranspose2d layers for upsampling\n",
    "\"\"\"\n",
    "# Generator Model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, embedding_dim, image_channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.noise_fc = nn.Sequential(\n",
    "            nn.Linear(noise_dim, 256 * 4 * 4),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.embed_fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 256 * 4 * 4),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, image_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, noise, embed):\n",
    "        noise_features = self.noise_fc(noise).view(noise.size(0), 256, 4, 4)\n",
    "        embed_features = self.embed_fc(embed).view(embed.size(0), 256, 4, 4)\n",
    "        x = noise_features + embed_features\n",
    "        x = self.conv_blocks(x)\n",
    "        return x\n",
    "\n",
    "\"\"\"\n",
    "Reference: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "Learning Conv2d layers for downsampling\n",
    "\"\"\"\n",
    "# Discriminator Model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, embedding_dim, image_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.Conv2d(image_channels + 1, 64, kernel_size=4, stride=2, padding=1),  # added +1\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.1),  # Dropout to weaken discriminator\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.1),  # Dropout to weaken discriminator\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.1),  # Dropout to weaken discriminator\n",
    "        )\n",
    "        self.fc = nn.Linear(256 * 4 * 4, 1)\n",
    "        # Project embedding into a spatial format\n",
    "        self.embed_fc = nn.Linear(embedding_dim, 4 * 4)  # Map embeddings to 4x4 spatial size\n",
    "        \n",
    "    def forward(self, img, embed):\n",
    "        batch_size = img.size(0)\n",
    "        # Convert embedding into spatial form\n",
    "        embed_features = self.embed_fc(embed).view(batch_size, 1, 4, 4)\n",
    "        # Resize embedding map to match image dimensions\n",
    "        embed_features = torch.nn.functional.interpolate(embed_features, size=(img.shape[2], img.shape[3]))\n",
    "        # Concatenate embeddings as an extra channel\n",
    "        x = torch.cat((img, embed_features), dim=1)\n",
    "        x = self.conv_blocks(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        return torch.sigmoid(self.fc(x))\n",
    "\n",
    "# Model Initialization\n",
    "noise_dim = 100\n",
    "embedding_dim = len(df['combined_embedding'][0])\n",
    "generator = Generator(noise_dim, embedding_dim).to(device)\n",
    "discriminator = Discriminator(embedding_dim).to(device)\n",
    "gamma = 10.0  # R1 regularization coefficient\n",
    "\n",
    "patience = 5  # Number of epochs to wait before early stopping\n",
    "lr_patience = 1  # Number of epochs to wait before reducing learning rate\n",
    "lr_factor = 0.5  # Factor to reduce learning rate by\n",
    "min_lr = 1e-6  # Minimum learning rate threshold\n",
    "\n",
    "best_fid_score = float('inf')\n",
    "epochs_since_improvement = 0\n",
    "epochs_since_lr_reduce = 0\n",
    "\n",
    "# Loss and Optimizers\n",
    "criterion = nn.BCELoss()\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "\n",
    "# Prepare InceptionV3 model for FID calculation using the new weights API.\n",
    "inception_model = models.inception_v3(weights=Inception_V3_Weights.IMAGENET1K_V1,\n",
    "                                        transform_input=False,\n",
    "                                        aux_logits=True).to(device)\n",
    "# Replace the final fully connected layer with an identity so that we get features\n",
    "inception_model.fc = nn.Identity()\n",
    "inception_model.eval()\n",
    "\n",
    "def get_inception_features(images, model):\n",
    "    \"\"\"\n",
    "    Resizes images to 299x299, normalizes them with Inception's mean and std,\n",
    "    and returns the features from the model.\n",
    "    \"\"\"\n",
    "    # Resize to InceptionV3 expected input size\n",
    "    images = torch.nn.functional.interpolate(images, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "    # If images are from generator (range [-1, 1]), convert them to [0, 1]\n",
    "    images = (images + 1) / 2\n",
    "    # Normalize with ImageNet statistics\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    images = torch.stack([normalize(img) for img in images])\n",
    "    with torch.no_grad():\n",
    "        features = model(images.to(device))\n",
    "        # If model returns a tuple (due to aux logits), take the first element.\n",
    "        if isinstance(features, tuple):\n",
    "            features = features[0]\n",
    "        features = features.detach().cpu().numpy()\n",
    "    return features\n",
    "\n",
    "def compute_fid(real_images, generated_images, model):\n",
    "    \"\"\"\n",
    "    Computes the Frechet Inception Distance (FID) between two sets of images.\n",
    "    \"\"\"\n",
    "    # Get inception features for real and generated images\n",
    "    real_features = get_inception_features(real_images, model)\n",
    "    fake_features = get_inception_features(generated_images, model)\n",
    "    \n",
    "    # Compute mean and covariance statistics\n",
    "    mu_real = np.mean(real_features, axis=0)\n",
    "    sigma_real = np.cov(real_features, rowvar=False)\n",
    "    mu_fake = np.mean(fake_features, axis=0)\n",
    "    sigma_fake = np.cov(fake_features, rowvar=False)\n",
    "    \n",
    "    # Compute squared difference between means\n",
    "    diff = mu_real - mu_fake\n",
    "    diff_squared = diff.dot(diff)\n",
    "    \n",
    "    # Compute sqrt of product of covariance matrices\n",
    "    covmean, _ = sqrtm(sigma_real.dot(sigma_fake), disp=False)\n",
    "    # If the product is almost singular, sqrtm may return complex numbers\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    \n",
    "    fid = diff_squared + np.trace(sigma_real + sigma_fake - 2 * covmean)\n",
    "    return fid\n",
    "\n",
    "num_epochs = 5000\n",
    "save_interval = int(num_epochs / 50)\n",
    "d_losses, g_losses, d_fake_losses, d_real_losses = [], [], [], []\n",
    "fid_scores = []\n",
    "fid_epochs = []\n",
    "all_fid_scores = []\n",
    "all_generated_images = []\n",
    "all_real_images = []\n",
    "all_epochs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    for i, (combined_embeddings, real_images) in enumerate(train_loader):\n",
    "        combined_embeddings = combined_embeddings.to(device)\n",
    "        real_images = real_images.to(device)\n",
    "        # Ensure real images require gradients for R1 penalty computation.\n",
    "        real_images.requires_grad_()\n",
    "        \n",
    "        # Train Discriminator\n",
    "        noise = torch.randn(real_images.size(0), noise_dim).to(device)\n",
    "        fake_images = generator(noise, combined_embeddings)\n",
    "        \n",
    "        real_labels = torch.full((real_images.size(0), 1), 0.95).to(device)\n",
    "        fake_labels = torch.full((real_images.size(0), 1), 0.05).to(device)\n",
    "        \n",
    "        # Forward pass on real images.\n",
    "        real_outputs = discriminator(real_images, combined_embeddings)\n",
    "        d_loss_real = criterion(real_outputs, real_labels)\n",
    "        \n",
    "        # Compute R1 regularization: gradient penalty on real images.\n",
    "        grad_real = torch.autograd.grad(\n",
    "            outputs=real_outputs.sum(), \n",
    "            inputs=real_images, \n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True\n",
    "        )[0]\n",
    "        grad_penalty = grad_real.view(grad_real.size(0), -1).pow(2).sum(1).mean()\n",
    "        d_loss_real = d_loss_real + (gamma / 2) * grad_penalty\n",
    "        \n",
    "        # Forward pass on fake images.\n",
    "        fake_outputs = discriminator(fake_images.detach(), combined_embeddings)\n",
    "        d_loss_fake = criterion(fake_outputs, fake_labels)\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        \n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # Train Generator\n",
    "        fake_outputs = discriminator(fake_images, combined_embeddings)\n",
    "        g_loss = criterion(fake_outputs, real_labels)\n",
    "        \n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        # Turn off gradients for real_images after update.\n",
    "        real_images.requires_grad_(False)\n",
    "    \n",
    "    # Store loss values\n",
    "    d_losses.append(d_loss.item())\n",
    "    g_losses.append(g_loss.item())\n",
    "    d_real_losses.append(d_loss_real.item())\n",
    "    d_fake_losses.append(d_loss_fake.item())\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] | D Loss: {d_loss:.4f} | G Loss: {g_loss.item():.4f}\")\n",
    "    \n",
    "    # Save evaluation every save_interval\n",
    "    if (epoch + 1) % save_interval == 0 or (epoch + 1) == num_epochs:\n",
    "        os.makedirs(train_output_dir, exist_ok=True)\n",
    "        os.makedirs(val_output_dir, exist_ok=True)\n",
    "\n",
    "        generator.eval()\n",
    "\n",
    "        # --- Evaluate on Training Set ---\n",
    "        train_batch = next(iter(train_loader))\n",
    "        train_embeddings, train_images = train_batch\n",
    "        train_embeddings = train_embeddings.to(device)\n",
    "        noise_train = torch.randn(train_images.size(0), noise_dim).to(device)\n",
    "        with torch.no_grad():\n",
    "            generated_train = generator(noise_train, train_embeddings).cpu()\n",
    "        grid_train = make_grid(generated_train, nrow=8, normalize=True)\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(np.transpose(grid_train.numpy(), (1, 2, 0)))\n",
    "        plt.title(f\"Train Set Generated Images at Epoch {epoch+1}\")\n",
    "        plt.axis(\"off\")\n",
    "        train_image_path = os.path.join(train_output_dir, f\"generated_train_epoch_{epoch+1}.png\")\n",
    "        plt.savefig(train_image_path)\n",
    "        plt.close()\n",
    "\n",
    "        # --- Plot Loss Curves ---\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, len(g_losses) + 1), g_losses, label=\"G Loss\")\n",
    "        plt.plot(range(1, len(d_losses) + 1), d_losses, label=\"D Loss\")\n",
    "        plt.xlabel(\"Epoch (save interval count)\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(f\"Training Losses up to Epoch {epoch+1}\")\n",
    "        plt.legend()\n",
    "        loss_plot_path = os.path.join(train_output_dir, f\"loss_plot_epoch_{epoch+1}.png\")\n",
    "        plt.savefig(loss_plot_path)\n",
    "        plt.close()\n",
    "\n",
    "        # --- Evaluate on Validation Set ---\n",
    "        val_batch = next(iter(val_loader))\n",
    "        val_embeddings, val_images = val_batch\n",
    "        val_embeddings = val_embeddings.to(device)\n",
    "        noise_val = torch.randn(val_images.size(0), noise_dim).to(device)\n",
    "        with torch.no_grad():\n",
    "            generated_val = generator(noise_val, val_embeddings).cpu()\n",
    "        grid_val = make_grid(generated_val, nrow=8, normalize=True)\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(np.transpose(grid_val.numpy(), (1, 2, 0)))\n",
    "        plt.title(f\"Validation Set Generated Images at Epoch {epoch+1}\")\n",
    "        plt.axis(\"off\")\n",
    "        val_image_path = os.path.join(val_output_dir, f\"generated_val_epoch_{epoch+1}.png\")\n",
    "        plt.savefig(val_image_path)\n",
    "        plt.close()\n",
    "\n",
    "        # --- Compute FID Score on Validation Set ---\n",
    "        # Note: using the current batch from the validation set for demonstration.\n",
    "        fid_score = compute_fid(val_images.cpu(), generated_val, inception_model)\n",
    "        fid_scores.append(fid_score)\n",
    "        fid_epochs.append(epoch+1)\n",
    "        print(f\"Epoch {epoch+1} FID Score: {fid_score:.4f}\")\n",
    "\n",
    "        all_fid_scores.append(fid_score)\n",
    "        all_generated_images.append(generated_val.cpu())\n",
    "        all_real_images.append(val_images.cpu())\n",
    "        all_epochs.append(epoch + 1)\n",
    "        \n",
    "        # Early Stopping and LR Reduction Logic\n",
    "        if fid_score < best_fid_score and epoch > 2000:\n",
    "            best_fid_score = fid_score\n",
    "            epochs_since_improvement = 0\n",
    "            epochs_since_lr_reduce = 0\n",
    "    \n",
    "            # Save the best models\n",
    "            torch.save(generator.state_dict(), os.path.join(val_models_dir, \"cgan_emoji_generator.pth\"))\n",
    "            torch.save(discriminator.state_dict(), os.path.join(val_models_dir, \"cgan_emoji_discriminator.pth\"))\n",
    "            print(f\"Saved improved model at Epoch {epoch+1} with FID {fid_score:.4f}\")\n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "            epochs_since_lr_reduce += 1\n",
    "            \n",
    "        # Reduce learning rate if no improvement for 'lr_patience' epochs\n",
    "        if epochs_since_lr_reduce >= lr_patience and epoch > 2000:\n",
    "            new_g_lr = max(g_optimizer.param_groups[0]['lr'] * lr_factor, min_lr)\n",
    "            new_d_lr = max(d_optimizer.param_groups[0]['lr'] * lr_factor, min_lr)\n",
    "    \n",
    "            for param_group in g_optimizer.param_groups:\n",
    "                param_group['lr'] = new_g_lr\n",
    "            for param_group in d_optimizer.param_groups:\n",
    "                param_group['lr'] = new_d_lr\n",
    "    \n",
    "            print(f\"Reducing learning rates to Generator: {new_g_lr}, Discriminator: {new_d_lr}\")\n",
    "    \n",
    "            epochs_since_lr_reduce = 0\n",
    "    \n",
    "        # Early stopping if no improvement for 'patience' epochs\n",
    "        if epochs_since_improvement >= patience and epoch > 2000:\n",
    "            print(f\"No improvement in FID for {patience} intervals. Stopping early at Epoch {epoch+1}.\")\n",
    "            break\n",
    "\n",
    "        generator.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f83b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the FID scores over epochs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(fid_epochs, fid_scores, marker='o', label=\"FID Score\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"FID Score\")\n",
    "plt.title(\"FID Score Over Training\")\n",
    "plt.legend()\n",
    "fid_plot_path = os.path.join(val_output_dir, \"fid_score_plot.png\")\n",
    "plt.savefig(fid_plot_path)\n",
    "plt.close()\n",
    "print(\"Training complete. Best model saved with FID:\", best_fid_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4811cdf368249e",
   "metadata": {},
   "source": [
    "## Generating emojis from prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5263523f386324e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:04:11.457929Z",
     "start_time": "2025-03-20T20:04:07.334403Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generated emojis plot at: ../evaluation/emoji_cgan/val_output/emojis_from_prompts.png\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reference: ChatGPT-4.5\n",
    "Prompt: I want to generate emojis using prompts. Input 5 different prompts, and save a plot that displays the prompts and their corresponding emoji generated using my generator. I'm using CLIP embedding for my texts.\n",
    "\"\"\"\n",
    "\n",
    "# Define your prompts\n",
    "prompts = [\n",
    "    \"Crying face\",\n",
    "    \"A brown man running in the left direction\",\n",
    "    \"An angry red face\",\n",
    "    \"A face wearing sunglasses depicting a sense of coolness\",\n",
    "    \"A man and a woman in love\"\n",
    "]\n",
    "\n",
    "generated_images = []\n",
    "\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "\n",
    "# Load CLIP's tokenizer and text model.\n",
    "clip_tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model = clip_model.to(device)\n",
    "clip_model.eval()\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    \"\"\"Mean pool the token embeddings.\"\"\"\n",
    "    token_embeddings = model_output.last_hidden_state  # (batch_size, sequence_length, hidden_dim)\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, dim=1) / torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
    "\n",
    "def embed_text(text):\n",
    "    if pd.isna(text) or text.strip() == \"\":\n",
    "        # Adjust the zero vector size to match CLIP's output dimension (512 for clip-vit-base-patch32)\n",
    "        return np.zeros(512, dtype=np.float32)\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    inputs = clip_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=77)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    \n",
    "    # Disable gradients for inference\n",
    "    with torch.no_grad():\n",
    "        output = clip_model(**inputs)\n",
    "    \n",
    "    # Pool the token embeddings (mean pooling)\n",
    "    pooled_embedding = mean_pooling(output, inputs[\"attention_mask\"])\n",
    "    \n",
    "    # Optionally, you might want to L2 normalize the pooled embedding:\n",
    "    pooled_embedding = torch.nn.functional.normalize(pooled_embedding, p=2, dim=-1)\n",
    "    \n",
    "    return pooled_embedding.squeeze().cpu().numpy().astype(np.float32)\n",
    "\n",
    "\n",
    "# Generate an emoji for each prompt using the Hugging Face CLIP resources\n",
    "for prompt in prompts:\n",
    "    # Tokenize the prompt using the CLIPTokenizer and get the embedding as a numpy array\n",
    "    text_embedding = embed_text(prompt)\n",
    "    # Convert the numpy array back to a torch tensor and add the batch dimension\n",
    "    text_embedding = torch.tensor(text_embedding).to(device).unsqueeze(0)\n",
    "    \n",
    "    # Generate a random noise vector\n",
    "    noise = torch.randn(1, noise_dim).to(device)\n",
    "    \n",
    "    # Generate the emoji image using your generator\n",
    "    with torch.no_grad():\n",
    "        gen_image = generator(noise, text_embedding)\n",
    "        gen_image = gen_image.cpu()  # move to CPU for plotting\n",
    "    generated_images.append(gen_image)\n",
    "\n",
    "# Function to convert a tensor image to a numpy image (assumes [1, C, H, W] in range [-1, 1])\n",
    "def tensor_to_image(tensor):\n",
    "    image = tensor.squeeze(0)          # remove batch dimension\n",
    "    image = (image + 1) / 2            # scale to [0, 1]\n",
    "    image = image.permute(1, 2, 0).numpy()  # convert to HWC format\n",
    "    return np.clip(image, 0, 1)\n",
    "\n",
    "# Plot the generated emojis along with their corresponding prompts\n",
    "fig, axs = plt.subplots(1, len(prompts), figsize=(20, 4))\n",
    "for i, (prompt, gen_img) in enumerate(zip(prompts, generated_images)):\n",
    "    img_np = tensor_to_image(gen_img)\n",
    "    axs[i].imshow(img_np)\n",
    "    axs[i].set_title(prompt, fontsize=10)\n",
    "    axs[i].axis(\"off\")\n",
    "plt.suptitle(\"Emojis Generated from Text Prompts\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "# Save the figure to disk\n",
    "save_path = os.path.join(val_output_dir, \"emojis_from_prompts.png\")\n",
    "plt.savefig(save_path)\n",
    "print(f\"Saved generated emojis plot at: {save_path}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6d370ba8f354d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
