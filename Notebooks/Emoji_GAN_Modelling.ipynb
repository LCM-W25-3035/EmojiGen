{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2302aa94befd47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T19:32:27.904954Z",
     "start_time": "2025-03-20T16:03:22.338189Z"
    }
   },
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import models, transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.models import Inception_V3_Weights\n",
    "\n",
    "# Device Configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Using NVIDIA GPU (CUDA)')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print('Using Mac GPU (MPS)')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using CPU')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_parquet('../data/processed_emoji_dataset.parquet')\n",
    "df[\"combined_embedding\"] = df[\"combined_embedding\"].apply(lambda x: np.array(x, dtype=np.float32))\n",
    "\n",
    "# Custom Dataset Class\n",
    "class EmojiDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        embedding = torch.tensor(self.dataframe.iloc[idx]['combined_embedding']).float()\n",
    "        image_tensor = torch.load(self.dataframe.iloc[idx]['image_path']).float()\n",
    "        return embedding, image_tensor\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = EmojiDataset(df)\n",
    "# Splitting data to training and testing sets\n",
    "train_samples = int(round(len(dataset)*0.90))\n",
    "train_set, val_set = random_split(dataset, [train_samples, len(dataset) - train_samples])\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=64, shuffle=False)\n",
    "\n",
    "# Directories to save\n",
    "os.makedirs(\"../evaluation/emoji_cgan/train_output\", exist_ok=True)\n",
    "os.makedirs(\"../evaluation/emoji_cgan/val_output\", exist_ok=True)\n",
    "os.makedirs(\"../saved_models\", exist_ok=True)\n",
    "train_output_dir = \"../evaluation/emoji_cgan/train_output\"\n",
    "val_output_dir = \"../evaluation/emoji_cgan/val_output\"\n",
    "val_models_dir = \"../saved_models\"\n",
    "\n",
    "\"\"\"\n",
    "Reference: https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html\n",
    "Learning ConvTranspose2d layers for upsampling\n",
    "\"\"\"\n",
    "# Generator Model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, embedding_dim, image_channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.noise_fc = nn.Sequential(\n",
    "            nn.Linear(noise_dim, 256 * 4 * 4),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.embed_fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 256 * 4 * 4),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, image_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, noise, embed):\n",
    "        noise_features = self.noise_fc(noise).view(noise.size(0), 256, 4, 4)\n",
    "        embed_features = self.embed_fc(embed).view(embed.size(0), 256, 4, 4)\n",
    "        x = noise_features + embed_features\n",
    "        x = self.conv_blocks(x)\n",
    "        return x\n",
    "\n",
    "\"\"\"\n",
    "Reference: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "Learning Conv2d layers for downsampling\n",
    "\"\"\"\n",
    "# Discriminator Model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, embedding_dim, image_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.Conv2d(image_channels + 1, 64, kernel_size=4, stride=2, padding=1),  # added +1\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.1),  # Dropout to weaken discriminator\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.1),  # Dropout to weaken discriminator\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.1),  # Dropout to weaken discriminator\n",
    "        )\n",
    "        self.fc = nn.Linear(256 * 4 * 4, 1)\n",
    "        # Project embedding into a spatial format\n",
    "        self.embed_fc = nn.Linear(embedding_dim, 4 * 4)  # Map embeddings to 4x4 spatial size\n",
    "        \n",
    "    def forward(self, img, embed):\n",
    "        batch_size = img.size(0)\n",
    "        # Convert embedding into spatial form\n",
    "        embed_features = self.embed_fc(embed).view(batch_size, 1, 4, 4)\n",
    "        # Resize embedding map to match image dimensions\n",
    "        embed_features = torch.nn.functional.interpolate(embed_features, size=(img.shape[2], img.shape[3]))\n",
    "        # Concatenate embeddings as an extra channel\n",
    "        x = torch.cat((img, embed_features), dim=1)\n",
    "        x = self.conv_blocks(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        return torch.sigmoid(self.fc(x))\n",
    "\n",
    "# Model Initialization\n",
    "noise_dim = 100\n",
    "embedding_dim = len(df['combined_embedding'][0])\n",
    "generator = Generator(noise_dim, embedding_dim).to(device)\n",
    "discriminator = Discriminator(embedding_dim).to(device)\n",
    "gamma = 10.0  # R1 regularization coefficient\n",
    "\n",
    "patience = 5  # Number of epochs to wait before early stopping\n",
    "lr_patience = 1  # Number of epochs to wait before reducing learning rate\n",
    "lr_factor = 0.5  # Factor to reduce learning rate by\n",
    "min_lr = 1e-6  # Minimum learning rate threshold\n",
    "\n",
    "best_fid_score = float('inf')\n",
    "epochs_since_improvement = 0\n",
    "epochs_since_lr_reduce = 0\n",
    "\n",
    "# Loss and Optimizers\n",
    "criterion = nn.BCELoss()\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "\n",
    "# Prepare InceptionV3 model for FID calculation using the new weights API.\n",
    "inception_model = models.inception_v3(weights=Inception_V3_Weights.IMAGENET1K_V1,\n",
    "                                        transform_input=False,\n",
    "                                        aux_logits=True).to(device)\n",
    "# Replace the final fully connected layer with an identity so that we get features\n",
    "inception_model.fc = nn.Identity()\n",
    "inception_model.eval()\n",
    "\n",
    "def get_inception_features(images, model):\n",
    "    \"\"\"\n",
    "    Resizes images to 299x299, normalizes them with Inception's mean and std,\n",
    "    and returns the features from the model.\n",
    "    \"\"\"\n",
    "    # Resize to InceptionV3 expected input size\n",
    "    images = torch.nn.functional.interpolate(images, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "    # If images are from generator (range [-1, 1]), convert them to [0, 1]\n",
    "    images = (images + 1) / 2\n",
    "    # Normalize with ImageNet statistics\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    images = torch.stack([normalize(img) for img in images])\n",
    "    with torch.no_grad():\n",
    "        features = model(images.to(device))\n",
    "        # If model returns a tuple (due to aux logits), take the first element.\n",
    "        if isinstance(features, tuple):\n",
    "            features = features[0]\n",
    "        features = features.detach().cpu().numpy()\n",
    "    return features\n",
    "\n",
    "def compute_fid(real_images, generated_images, model):\n",
    "    \"\"\"\n",
    "    Computes the Frechet Inception Distance (FID) between two sets of images.\n",
    "    \"\"\"\n",
    "    # Get inception features for real and generated images\n",
    "    real_features = get_inception_features(real_images, model)\n",
    "    fake_features = get_inception_features(generated_images, model)\n",
    "    \n",
    "    # Compute mean and covariance statistics\n",
    "    mu_real = np.mean(real_features, axis=0)\n",
    "    sigma_real = np.cov(real_features, rowvar=False)\n",
    "    mu_fake = np.mean(fake_features, axis=0)\n",
    "    sigma_fake = np.cov(fake_features, rowvar=False)\n",
    "    \n",
    "    # Compute squared difference between means\n",
    "    diff = mu_real - mu_fake\n",
    "    diff_squared = diff.dot(diff)\n",
    "    \n",
    "    # Compute sqrt of product of covariance matrices\n",
    "    covmean, _ = sqrtm(sigma_real.dot(sigma_fake), disp=False)\n",
    "    # If the product is almost singular, sqrtm may return complex numbers\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    \n",
    "    fid = diff_squared + np.trace(sigma_real + sigma_fake - 2 * covmean)\n",
    "    return fid\n",
    "\n",
    "num_epochs = 5000\n",
    "save_interval = int(num_epochs / 50)\n",
    "d_losses, g_losses, d_fake_losses, d_real_losses = [], [], [], []\n",
    "fid_scores = []\n",
    "fid_epochs = []\n",
    "all_fid_scores = []\n",
    "all_generated_images = []\n",
    "all_real_images = []\n",
    "all_epochs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    for i, (combined_embeddings, real_images) in enumerate(train_loader):\n",
    "        combined_embeddings = combined_embeddings.to(device)\n",
    "        real_images = real_images.to(device)\n",
    "        # Ensure real images require gradients for R1 penalty computation.\n",
    "        real_images.requires_grad_()\n",
    "        \n",
    "        # Train Discriminator\n",
    "        noise = torch.randn(real_images.size(0), noise_dim).to(device)\n",
    "        fake_images = generator(noise, combined_embeddings)\n",
    "        \n",
    "        real_labels = torch.full((real_images.size(0), 1), 0.95).to(device)\n",
    "        fake_labels = torch.full((real_images.size(0), 1), 0.05).to(device)\n",
    "        \n",
    "        # Forward pass on real images.\n",
    "        real_outputs = discriminator(real_images, combined_embeddings)\n",
    "        d_loss_real = criterion(real_outputs, real_labels)\n",
    "        \n",
    "        # Compute R1 regularization: gradient penalty on real images.\n",
    "        grad_real = torch.autograd.grad(\n",
    "            outputs=real_outputs.sum(), \n",
    "            inputs=real_images, \n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True\n",
    "        )[0]\n",
    "        grad_penalty = grad_real.view(grad_real.size(0), -1).pow(2).sum(1).mean()\n",
    "        d_loss_real = d_loss_real + (gamma / 2) * grad_penalty\n",
    "        \n",
    "        # Forward pass on fake images.\n",
    "        fake_outputs = discriminator(fake_images.detach(), combined_embeddings)\n",
    "        d_loss_fake = criterion(fake_outputs, fake_labels)\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        \n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # Train Generator\n",
    "        fake_outputs = discriminator(fake_images, combined_embeddings)\n",
    "        g_loss = criterion(fake_outputs, real_labels)\n",
    "        \n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        # Turn off gradients for real_images after update.\n",
    "        real_images.requires_grad_(False)\n",
    "    \n",
    "    # Store loss values\n",
    "    d_losses.append(d_loss.item())\n",
    "    g_losses.append(g_loss.item())\n",
    "    d_real_losses.append(d_loss_real.item())\n",
    "    d_fake_losses.append(d_loss_fake.item())\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] | D Loss: {d_loss:.4f} | G Loss: {g_loss.item():.4f}\")\n",
    "    \n",
    "    # Save evaluation every save_interval\n",
    "    if (epoch + 1) % save_interval == 0 or (epoch + 1) == num_epochs:\n",
    "        os.makedirs(train_output_dir, exist_ok=True)\n",
    "        os.makedirs(val_output_dir, exist_ok=True)\n",
    "\n",
    "        generator.eval()\n",
    "\n",
    "        # --- Evaluate on Training Set ---\n",
    "        train_batch = next(iter(train_loader))\n",
    "        train_embeddings, train_images = train_batch\n",
    "        train_embeddings = train_embeddings.to(device)\n",
    "        noise_train = torch.randn(train_images.size(0), noise_dim).to(device)\n",
    "        with torch.no_grad():\n",
    "            generated_train = generator(noise_train, train_embeddings).cpu()\n",
    "        grid_train = make_grid(generated_train, nrow=8, normalize=True)\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(np.transpose(grid_train.numpy(), (1, 2, 0)))\n",
    "        plt.title(f\"Train Set Generated Images at Epoch {epoch+1}\")\n",
    "        plt.axis(\"off\")\n",
    "        train_image_path = os.path.join(train_output_dir, f\"generated_train_epoch_{epoch+1}.png\")\n",
    "        plt.savefig(train_image_path)\n",
    "        plt.close()\n",
    "\n",
    "        # --- Plot Loss Curves ---\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, len(g_losses) + 1), g_losses, label=\"G Loss\")\n",
    "        plt.plot(range(1, len(d_losses) + 1), d_losses, label=\"D Loss\")\n",
    "        plt.xlabel(\"Epoch (save interval count)\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(f\"Training Losses up to Epoch {epoch+1}\")\n",
    "        plt.legend()\n",
    "        loss_plot_path = os.path.join(train_output_dir, f\"loss_plot_epoch_{epoch+1}.png\")\n",
    "        plt.savefig(loss_plot_path)\n",
    "        plt.close()\n",
    "\n",
    "        # --- Evaluate on Validation Set ---\n",
    "        val_batch = next(iter(val_loader))\n",
    "        val_embeddings, val_images = val_batch\n",
    "        val_embeddings = val_embeddings.to(device)\n",
    "        noise_val = torch.randn(val_images.size(0), noise_dim).to(device)\n",
    "        with torch.no_grad():\n",
    "            generated_val = generator(noise_val, val_embeddings).cpu()\n",
    "        grid_val = make_grid(generated_val, nrow=8, normalize=True)\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(np.transpose(grid_val.numpy(), (1, 2, 0)))\n",
    "        plt.title(f\"Validation Set Generated Images at Epoch {epoch+1}\")\n",
    "        plt.axis(\"off\")\n",
    "        val_image_path = os.path.join(val_output_dir, f\"generated_val_epoch_{epoch+1}.png\")\n",
    "        plt.savefig(val_image_path)\n",
    "        plt.close()\n",
    "\n",
    "        # --- Compute FID Score on Validation Set ---\n",
    "        # Note: using the current batch from the validation set for demonstration.\n",
    "        fid_score = compute_fid(val_images.cpu(), generated_val, inception_model)\n",
    "        fid_scores.append(fid_score)\n",
    "        fid_epochs.append(epoch+1)\n",
    "        print(f\"Epoch {epoch+1} FID Score: {fid_score:.4f}\")\n",
    "\n",
    "        all_fid_scores.append(fid_score)\n",
    "        all_generated_images.append(generated_val.cpu())\n",
    "        all_real_images.append(val_images.cpu())\n",
    "        all_epochs.append(epoch + 1)\n",
    "        \n",
    "        # Early Stopping and LR Reduction Logic\n",
    "        if fid_score < best_fid_score and epoch > 2000:\n",
    "            best_fid_score = fid_score\n",
    "            epochs_since_improvement = 0\n",
    "            epochs_since_lr_reduce = 0\n",
    "    \n",
    "            # Save the best models\n",
    "            torch.save(generator.state_dict(), os.path.join(val_models_dir, \"cgan_emoji_generator.pth\"))\n",
    "            torch.save(discriminator.state_dict(), os.path.join(val_models_dir, \"cgan_emoji_discriminator.pth\"))\n",
    "            print(f\"Saved improved model at Epoch {epoch+1} with FID {fid_score:.4f}\")\n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "            epochs_since_lr_reduce += 1\n",
    "            \n",
    "        # Reduce learning rate if no improvement for 'lr_patience' epochs\n",
    "        if epochs_since_lr_reduce >= lr_patience and epoch > 2000:\n",
    "            new_g_lr = max(g_optimizer.param_groups[0]['lr'] * lr_factor, min_lr)\n",
    "            new_d_lr = max(d_optimizer.param_groups[0]['lr'] * lr_factor, min_lr)\n",
    "    \n",
    "            for param_group in g_optimizer.param_groups:\n",
    "                param_group['lr'] = new_g_lr\n",
    "            for param_group in d_optimizer.param_groups:\n",
    "                param_group['lr'] = new_d_lr\n",
    "    \n",
    "            print(f\"Reducing learning rates to Generator: {new_g_lr}, Discriminator: {new_d_lr}\")\n",
    "    \n",
    "            epochs_since_lr_reduce = 0\n",
    "    \n",
    "        # Early stopping if no improvement for 'patience' epochs\n",
    "        if epochs_since_improvement >= patience and epoch > 2000:\n",
    "            print(f\"No improvement in FID for {patience} intervals. Stopping early at Epoch {epoch+1}.\")\n",
    "            break\n",
    "\n",
    "        generator.train()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f83b77",
   "metadata": {},
   "source": [
    "#Plot the FID scores over epochs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(fid_epochs, fid_scores, marker='o', label=\"FID Score\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"FID Score\")\n",
    "plt.title(\"FID Score Over Training\")\n",
    "plt.legend()\n",
    "fid_plot_path = os.path.join(val_output_dir, \"fid_score_plot.png\")\n",
    "plt.savefig(fid_plot_path)\n",
    "plt.close()\n",
    "print(\"Training complete. Best model saved with FID:\", best_fid_score)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cc4811cdf368249e",
   "metadata": {},
   "source": [
    "## Generating emojis from prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5263523f386324e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:04:11.457929Z",
     "start_time": "2025-03-20T20:04:07.334403Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "Reference: ChatGPT-4.5\n",
    "Prompt: I want to generate emojis using prompts. Input 5 different prompts, and save a plot that displays the prompts and their corresponding emoji generated using my generator. I'm using CLIP embedding for my texts.\n",
    "\"\"\"\n",
    "\n",
    "# Define your prompts\n",
    "prompts = [\n",
    "    \"Crying face\",\n",
    "    \"A brown man running in the left direction\",\n",
    "    \"An angry red face\",\n",
    "    \"A face wearing sunglasses depicting a sense of coolness\",\n",
    "    \"A man and a woman in love\"\n",
    "]\n",
    "\n",
    "generated_images = []\n",
    "\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "\n",
    "# Load CLIP's tokenizer and text model.\n",
    "clip_tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model = clip_model.to(device)\n",
    "clip_model.eval()\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    \"\"\"Mean pool the token embeddings.\"\"\"\n",
    "    token_embeddings = model_output.last_hidden_state  # (batch_size, sequence_length, hidden_dim)\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, dim=1) / torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
    "\n",
    "def embed_text(text):\n",
    "    if pd.isna(text) or text.strip() == \"\":\n",
    "        # Adjust the zero vector size to match CLIP's output dimension (512 for clip-vit-base-patch32)\n",
    "        return np.zeros(512, dtype=np.float32)\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    inputs = clip_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=77)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    \n",
    "    # Disable gradients for inference\n",
    "    with torch.no_grad():\n",
    "        output = clip_model(**inputs)\n",
    "    \n",
    "    # Pool the token embeddings (mean pooling)\n",
    "    pooled_embedding = mean_pooling(output, inputs[\"attention_mask\"])\n",
    "    \n",
    "    # Optionally, you might want to L2 normalize the pooled embedding:\n",
    "    pooled_embedding = torch.nn.functional.normalize(pooled_embedding, p=2, dim=-1)\n",
    "    \n",
    "    return pooled_embedding.squeeze().cpu().numpy().astype(np.float32)\n",
    "\n",
    "\n",
    "# Generate an emoji for each prompt using the Hugging Face CLIP resources\n",
    "for prompt in prompts:\n",
    "    # Tokenize the prompt using the CLIPTokenizer and get the embedding as a numpy array\n",
    "    text_embedding = embed_text(prompt)\n",
    "    # Convert the numpy array back to a torch tensor and add the batch dimension\n",
    "    text_embedding = torch.tensor(text_embedding).to(device).unsqueeze(0)\n",
    "    \n",
    "    # Generate a random noise vector\n",
    "    noise = torch.randn(1, noise_dim).to(device)\n",
    "    \n",
    "    # Generate the emoji image using your generator\n",
    "    with torch.no_grad():\n",
    "        gen_image = generator(noise, text_embedding)\n",
    "        gen_image = gen_image.cpu()  # move to CPU for plotting\n",
    "    generated_images.append(gen_image)\n",
    "\n",
    "# Function to convert a tensor image to a numpy image (assumes [1, C, H, W] in range [-1, 1])\n",
    "def tensor_to_image(tensor):\n",
    "    image = tensor.squeeze(0)          # remove batch dimension\n",
    "    image = (image + 1) / 2            # scale to [0, 1]\n",
    "    image = image.permute(1, 2, 0).numpy()  # convert to HWC format\n",
    "    return np.clip(image, 0, 1)\n",
    "\n",
    "# Plot the generated emojis along with their corresponding prompts\n",
    "fig, axs = plt.subplots(1, len(prompts), figsize=(20, 4))\n",
    "for i, (prompt, gen_img) in enumerate(zip(prompts, generated_images)):\n",
    "    img_np = tensor_to_image(gen_img)\n",
    "    axs[i].imshow(img_np)\n",
    "    axs[i].set_title(prompt, fontsize=10)\n",
    "    axs[i].axis(\"off\")\n",
    "plt.suptitle(\"Emojis Generated from Text Prompts\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "# Save the figure to disk\n",
    "save_path = os.path.join(val_output_dir, \"emojis_from_prompts.png\")\n",
    "plt.savefig(save_path)\n",
    "print(f\"Saved generated emojis plot at: {save_path}\")\n",
    "plt.close()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6d370ba8f354d8",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
