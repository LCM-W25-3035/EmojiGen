{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "356a07f55578fdf1",
   "metadata": {},
   "source": [
    "## DDPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db28cd2c73f33e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from torchvision import transforms\n",
    "from diffusers import UNet2DConditionModel, DDPMScheduler\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Device Configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Using NVIDIA GPU (CUDA)')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print('Using Mac GPU (MPS)')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using CPU')\n",
    "    \n",
    "# ------------------------------\n",
    "# Configs\n",
    "# ------------------------------\n",
    "image_size = 256\n",
    "clip_embedding_dim = 512\n",
    "batch_size = 64\n",
    "num_epochs = 1000\n",
    "learning_rate = 1e-4\n",
    "save_interval = int(num_epochs / 50)\n",
    "save_dir = \"../saved_models\"\n",
    "eval_dir = \"../evaluation/emoji_diffusion\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "os.makedirs(eval_dir, exist_ok=True)\n",
    "fid_device = torch.device(\"cpu\") if device.type == \"mps\" else device\n",
    "fid_metric = FrechetInceptionDistance(feature=2048).to(fid_device)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_parquet('../data/processed_emoji_dataset.parquet')\n",
    "df[\"combined_embedding\"] = df[\"combined_embedding\"].apply(lambda x: np.array(x, dtype=np.float32))\n",
    "\n",
    "# Train and validation split\n",
    "# train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "class EmojiDiffusionDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_tensor = torch.load(self.df.iloc[idx]['image_path']).float()\n",
    "        embedding = torch.tensor(self.df.iloc[idx]['combined_embedding'], dtype=torch.float32)\n",
    "        embedding = embedding / embedding.norm()  # normalize CLIP embedding\n",
    "        skintone = int(self.df.iloc[idx]['skintone'])\n",
    "        prompt = self.df.iloc[idx]['prompt']\n",
    "        return {\n",
    "            \"pixel_values\": image_tensor,\n",
    "            \"embedding\": embedding,\n",
    "            \"skintone\": skintone,\n",
    "            \"prompt\": prompt,\n",
    "        }\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = EmojiDiffusionDataset(df)\n",
    "# Splitting data to training and testing sets\n",
    "train_samples = int(round(len(dataset)*0.90))\n",
    "train_set, val_set = random_split(dataset, [train_samples, len(dataset) - train_samples])\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=64, shuffle=False)\n",
    "\n",
    "# ------------------------------\n",
    "# Model Setup\n",
    "# ------------------------------\n",
    "model = UNet2DConditionModel(\n",
    "    sample_size=image_size,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(64, 128, 128, 256),\n",
    "    down_block_types=(\"DownBlock2D\", \"DownBlock2D\", \"AttnDownBlock2D\", \"DownBlock2D\"),\n",
    "    up_block_types=(\"UpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\"),\n",
    "    cross_attention_dim=528\n",
    ").to(device)\n",
    "\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Skintone embedding\n",
    "num_skintone_classes = 6  # 0â€“5 inclusive\n",
    "skintone_embedding_dim = 16  # small vector size (you can tune this)\n",
    "\n",
    "skintone_embedding_layer = nn.Embedding(num_skintone_classes, skintone_embedding_dim).to(device)\n",
    "\n",
    "# LR Scheduler and Early Stopping\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "early_stopping_patience = 7\n",
    "best_fid = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "best_model_path = os.path.join(save_dir, \"emoji_diffusion.pth\")\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Sampling Function\n",
    "# ------------------------------\n",
    "@torch.no_grad()\n",
    "def generate_emoji(embedding, skintone_ids, model, scheduler, skintone_embedding_layer, num_steps=1000):\n",
    "    model.eval()\n",
    "    scheduler.set_timesteps(num_steps)\n",
    "\n",
    "    bsz = embedding.size(0)\n",
    "\n",
    "    # Normalize CLIP embeddings if not already normalized\n",
    "    embedding = embedding / embedding.norm(dim=1, keepdim=True)\n",
    "\n",
    "    # Get skintone embedding and concatenate\n",
    "    skintone_embed = skintone_embedding_layer(skintone_ids)\n",
    "    final_embedding = torch.cat([embedding, skintone_embed], dim=1).unsqueeze(1)  # (B, 1, 528)\n",
    "\n",
    "    # Start from random noise\n",
    "    image = torch.randn((bsz, 3, image_size, image_size)).to(device)\n",
    "\n",
    "    for t in scheduler.timesteps:\n",
    "        noise_pred = model(image, t, encoder_hidden_states=final_embedding).sample\n",
    "        image = scheduler.step(noise_pred, t, image).prev_sample\n",
    "\n",
    "    # Normalize to [0, 1]\n",
    "    image = (image.clamp(-1, 1) + 1) / 2\n",
    "    return image\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Training Loop\n",
    "# ------------------------------\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "eval_epochs = []\n",
    "\n",
    "model.train()\n",
    "noise_scheduler.set_timesteps(1000)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
    "        clean_images = batch[\"pixel_values\"].to(device)\n",
    "        embeddings = batch[\"embedding\"].to(device)\n",
    "        skintone_ids = batch[\"skintone\"].to(device)\n",
    "        skintone_embeds = skintone_embedding_layer(skintone_ids)\n",
    "        \n",
    "        # Combine CLIP and skintone\n",
    "        final_embedding = torch.cat([embeddings, skintone_embeds], dim=1)\n",
    "        final_embedding = final_embedding.unsqueeze(1)\n",
    "        \n",
    "        # Fix for CLIP embedding shape\n",
    "        # embeddings = embeddings.unsqueeze(1)  # Shape: (batch_size, 1, 512)\n",
    "\n",
    "        noise = torch.randn_like(clean_images).to(device)\n",
    "        timesteps = torch.randint(0, 1000, (clean_images.shape[0],), device=device).long()\n",
    "        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "        noise_pred = model(noisy_images, timesteps, encoder_hidden_states=final_embedding).sample\n",
    "        loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch} | Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Evaluation and Saving\n",
    "    # ------------------------------\n",
    "    if epoch == 1 or epoch % save_interval == 0 or epoch == num_epochs:\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        eval_epochs.append(epoch)\n",
    "        with torch.no_grad():\n",
    "            for val_batch in val_loader:\n",
    "                val_clean = val_batch[\"pixel_values\"].to(device)\n",
    "                val_embed = val_batch[\"embedding\"].to(device)\n",
    "                val_skintone_ids = val_batch[\"skintone\"].to(device)\n",
    "                val_skintone_embed = skintone_embedding_layer(val_skintone_ids)\n",
    "                val_embed = torch.cat([val_embed, val_skintone_embed], dim=1).unsqueeze(1)\n",
    "                val_noise = torch.randn_like(val_clean)\n",
    "                val_t = torch.randint(0, 1000, (val_clean.shape[0],), device=device).long()\n",
    "                val_noisy = noise_scheduler.add_noise(val_clean, val_noise, val_t)\n",
    "\n",
    "                val_pred = model(val_noisy, val_t, encoder_hidden_states=val_embed).sample\n",
    "                val_loss += nn.functional.mse_loss(val_pred, val_noise).item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        print(f\"Epoch {epoch} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Generate samples for FID\n",
    "        def get_first_full_batch(loader, min_batch_size=64):\n",
    "            for batch in loader:\n",
    "                if batch[\"pixel_values\"].size(0) >= min_batch_size:\n",
    "                    return batch\n",
    "            raise ValueError(\"No batch with enough samples found.\")\n",
    "        val_batch = get_first_full_batch(val_loader)  # For FID + sample gen only\n",
    "        clip_embed = val_batch[\"embedding\"][:64].to(device)\n",
    "        skintone_ids = val_batch[\"skintone\"][:64].to(device)\n",
    "        skintone_embed = skintone_embedding_layer(skintone_ids)\n",
    "        final_embed = torch.cat([clip_embed, skintone_embed], dim=1)\n",
    "        \n",
    "        val_generated = generate_emoji(\n",
    "            embedding=clip_embed,\n",
    "            skintone_ids=skintone_ids,\n",
    "            model=model,\n",
    "            scheduler=noise_scheduler,\n",
    "            skintone_embedding_layer=skintone_embedding_layer\n",
    "        )\n",
    "\n",
    "        real_images = val_batch[\"pixel_values\"][:64].to(device)\n",
    "        real_images_uint8 = (real_images * 255).clamp(0, 255).to(torch.uint8).to(fid_device)\n",
    "        val_generated_uint8 = (val_generated * 255).clamp(0, 255).to(torch.uint8).to(fid_device)\n",
    "\n",
    "        fid_metric.reset()\n",
    "        fid_metric.update(real_images_uint8, real=True)\n",
    "        fid_metric.update(val_generated_uint8, real=False)\n",
    "        fid_score = fid_metric.compute().item()\n",
    "        print(f\"Epoch {epoch} | FID Score: {fid_score:.4f}\")\n",
    "\n",
    "        # Save FID score\n",
    "        with open(os.path.join(eval_dir, \"fid_scores.txt\"), \"a\") as f:\n",
    "            f.write(f\"Epoch {epoch}: FID = {fid_score:.4f}\\n\")\n",
    "\n",
    "        # Save generated images\n",
    "        save_image(val_generated, os.path.join(eval_dir, f\"val_epoch_{epoch}.png\"), nrow=8)\n",
    "\n",
    "        # Save loss graph\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(eval_epochs, train_losses[-len(eval_epochs):], label='Train Loss')\n",
    "        plt.plot(eval_epochs, val_losses, label='Val Loss')\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training & Validation Loss\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.savefig(os.path.join(eval_dir, f\"loss_epoch_{epoch}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # Adjust learning rate\n",
    "        scheduler.step(fid_score)\n",
    "        current_lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "        print(f\"Current Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if fid_score < best_fid:\n",
    "            print(f\"New best FID: {fid_score:.4f} (previous: {best_fid:.4f}) â€” saving model.\")\n",
    "            best_fid = fid_score\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"No improvement in FID. {epochs_without_improvement} epochs without improvement.\")\n",
    "\n",
    "        # Early stopping\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "\n",
    "fid_epochs = []\n",
    "fid_values = []\n",
    "with open(os.path.join(eval_dir, \"fid_scores.txt\")) as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(\": FID = \")\n",
    "        fid_epochs.append(int(parts[0].split(\" \")[1]))\n",
    "        fid_values.append(float(parts[1]))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(fid_epochs, fid_values, label='FID Score')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"FID\")\n",
    "plt.title(\"FID Score Over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join(eval_dir, \"fid_progression.png\"))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ecbe5d0729d736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Prepare inverse transform for displaying\n",
    "to_pil = T.ToPILImage()\n",
    "\n",
    "# Prepare figure\n",
    "num_samples = min(len(val_set), 64)\n",
    "val_loader_vis = DataLoader(val_set, batch_size=num_samples, shuffle=False)\n",
    "val_batch = next(iter(val_loader_vis))\n",
    "val_embeddings = val_batch[\"embedding\"].to(device)\n",
    "val_skintone_ids = val_batch[\"skintone\"].to(device)\n",
    "val_skintone_embed = skintone_embedding_layer(val_skintone_ids)\n",
    "val_final_embed = torch.cat([val_embeddings, val_skintone_embed], dim=1)\n",
    "val_images = val_batch[\"pixel_values\"].to(device)\n",
    "\n",
    "# Generate emojis\n",
    "# generated_images = generate_emoji(val_embeddings, model, noise_scheduler)\n",
    "generated_images = generate_emoji(\n",
    "            val_embeddings,\n",
    "            skintone_ids=val_skintone_ids,\n",
    "            model=model,\n",
    "            scheduler=noise_scheduler,\n",
    "            skintone_embedding_layer=skintone_embedding_layer\n",
    "        )\n",
    "\n",
    "# Convert and plot\n",
    "fig, axes = plt.subplots(nrows=num_samples, ncols=3, figsize=(9, num_samples * 1.5))\n",
    "fig.suptitle(\"Prompt | Real Emoji | Generated Emoji\", fontsize=14)\n",
    "\n",
    "for i in range(num_samples):\n",
    "    prompt_text = val_set[i][\"prompt\"] if \"prompt\" in df.columns else \"Prompt not available\"\n",
    "    \n",
    "    real_img = val_images[i].cpu().clamp(0, 1)\n",
    "    gen_img = generated_images[i].cpu().clamp(0, 1)\n",
    "\n",
    "    axes[i, 0].text(0.5, 0.5, prompt_text, ha='center', va='center', wrap=True)\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    axes[i, 1].imshow(to_pil(real_img))\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "    axes[i, 2].imshow(to_pil(gen_img))\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.98)\n",
    "plt.savefig(os.path.join(eval_dir, \"emoji_comparison_grid.png\"))\n",
    "print(\"Generated emoji comparison chart and saved as 'emoji_comparison_grid.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399830a63239851f",
   "metadata": {},
   "source": [
    "## Q-LoRA (Best Configurations - Increase Epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3af6b7dc45fe43b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:39:55.510006Z",
     "start_time": "2025-04-03T02:42:57.074412Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bikinghimire/Projects/AirbnbDynamicPricing/.venv/lib/python3.12/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4763c1a342d14160bb974f636ff79725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,188,736 || all params: 862,709,700 || trainable%: 0.3696\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454de7d2722548dab094bcc03aa0cb39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/16 - Training:   0%|          | 0/3021 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/16] - Average Training Loss: 0.028467\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b53dea61752421aab7a8a4ca465a39d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/16 - Validation Loss:   0%|          | 0/303 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/16] - Average Validation Loss: 0.027153\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c5b0fbe65524ec6a4195336414e0e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch1_grinning_face.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ddc5543b4d422bbdcdfc72b67786bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/16 - Training:   0%|          | 0/3021 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/16] - Average Training Loss: 0.025771\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1762f2aa448b4a15ad85296f316656b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/16 - Validation Loss:   0%|          | 0/303 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/16] - Average Validation Loss: 0.024775\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d1dc076f804befa36ffdf5d6b8cb81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch2_grinning_face.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1adfbabb66f641a98e80a461e6901fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/16 - Training:   0%|          | 0/3021 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/16] - Average Training Loss: 0.025953\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "849d734895db41b38f69fe68c17cd76d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/16 - Validation Loss:   0%|          | 0/303 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/16] - Average Validation Loss: 0.022969\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493a54a3c19641e683e0b7dd279b1248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch3_grinning_face.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63796c99f241464a99f0f8410387af29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/16 - Training:   0%|          | 0/3021 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/16] - Average Training Loss: 0.023891\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c61f0a2215e04166a8b21b7f9a6491af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/16 - Validation Loss:   0%|          | 0/303 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/16] - Average Validation Loss: 0.023697\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0881adc5ff744a081d86182b364669a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch4_grinning_face.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a396fc96e5490ca94eb6577248187e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/16 - Training:   0%|          | 0/3021 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/16] - Average Training Loss: 0.024424\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff0388a19774ee4a6524f86783586ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/16 - Validation Loss:   0%|          | 0/303 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/16] - Average Validation Loss: 0.023954\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89f9a4045ed4f9dbbcf0605d11590f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch5_grinning_face.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffb0d1e4d19e4689ae86dbb684d9943c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/16 - Training:   0%|          | 0/3021 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/16] - Average Training Loss: 0.023163\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad74c33292b4848aacf5b911a7d9580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/16 - Validation Loss:   0%|          | 0/303 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/16] - Average Validation Loss: 0.025324\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31420b3ed790467db76858082b093362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch6_grinning_face.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee194a5bc5bb423d98d5a9c9ca8ef8c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/16 - Training:   0%|          | 0/3021 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/16] - Average Training Loss: 0.023144\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d067c2fc19ae4f37abb66c20230b689a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/16 - Validation Loss:   0%|          | 0/303 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/16] - Average Validation Loss: 0.022478\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e7e07430df7421487873669f004e56f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch7_grinning_face.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ab49bf00044ec4baac64e42b3b0222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/16 - Training:   0%|          | 0/3021 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/16] - Average Training Loss: 0.022431\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc7563270a0a4d838df5085b28ce57f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/16 - Validation Loss:   0%|          | 0/303 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/16] - Average Validation Loss: 0.021195\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d1352dff08045ccac090fa35c27b2f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch8_grinning_face.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d799ec0859c84444b2f7389392b60d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/16 - Training:   0%|          | 0/3021 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/16] - Average Training Loss: 0.021856\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfbe675e0cb143b79be1c43e0109b605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/16 - Validation Loss:   0%|          | 0/303 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/16] - Average Validation Loss: 0.020440\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "221233bdf93440209bb937ab537b5d4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch9_grinning_face.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "881325117b0548a9a15c756fc46b4dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/16 - Training:   0%|          | 0/3021 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/16] - Average Training Loss: 0.021894\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658f57fc207d444b8a5deedc305f7394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/16 - Validation Loss:   0%|          | 0/303 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/16] - Average Validation Loss: 0.025230\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60740e1013b640a197297fb3ae759539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch10_grinning_face.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1747c0c1c9945b6ad5e5c60c021e214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/16 - Training:   0%|          | 0/3021 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/16] - Average Training Loss: 0.021362\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1227fbb90884cf79a94e708ca31d483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/16 - Validation Loss:   0%|          | 0/303 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/16] - Average Validation Loss: 0.022813\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7873327582344304ab644013700b37fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch11_grinning_face.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b94a43014194519beacb1bce1c839ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/16 - Training:   0%|          | 0/3021 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/16] - Average Training Loss: 0.020627\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8b0ccc16be45a88a9d2600e01f8e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/16 - Validation Loss:   0%|          | 0/303 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/16] - Average Validation Loss: 0.024037\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f31cbd9ff5354101b9ac6a0b0671fcb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch12_grinning_face.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcc91e4d49e747429a91ce7a0971621e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/16 - Training:   0%|          | 0/3021 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/16] - Average Training Loss: 0.020688\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04bf0fb4b3c84b58ae2850eb09d3ba54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/16 - Validation Loss:   0%|          | 0/303 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/16] - Average Validation Loss: 0.018867\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c02d7de60c4bfa9b2803d081a9c2a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch13_grinning_face.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f94a4ae3e1294d0398f47fb18b0cf3e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/16 - Training:   0%|          | 0/3021 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/16] - Average Training Loss: 0.020024\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f7d4e589cec4bd998d6eb34d79df7c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/16 - Validation Loss:   0%|          | 0/303 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/16] - Average Validation Loss: 0.022288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a2f41419b04d4f8ad09c35d7bebdd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch14_grinning_face.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "400bea1a11844464b47f6e858239062c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/16 - Training:   0%|          | 0/3021 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/16] - Average Training Loss: 0.020497\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d92f8a7c4f747878f1dff26d1bfa2aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/16 - Validation Loss:   0%|          | 0/303 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/16] - Average Validation Loss: 0.020047\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "777e5d9d47114f7eb1d06fb0608a7e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch15_grinning_face.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92b6e8a42720442a865f0d678adeff07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/16 - Training:   0%|          | 0/3021 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/16] - Average Training Loss: 0.020149\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9fb6fa14fc445ddbf5d57f2ffcbf973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/16 - Validation Loss:   0%|          | 0/303 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/16] - Average Validation Loss: 0.020814\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c12331136d324155b4e57d2c78b9addc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch16_grinning_face.png\n",
      "Training and validation loss graph saved to ../evaluation/lora/training_validation_loss.png\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt  # For plotting the loss graph\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps')\n",
    "\n",
    "# Data Preparation: Only return image and prompt\n",
    "class EmojiDiffusionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_tensor = torch.load(self.df.iloc[idx]['image_path']).float()\n",
    "        prompt = self.df.iloc[idx]['prompt']\n",
    "        return {\"pixel_values\": image_tensor, \"prompt\": prompt}\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_parquet('../data/processed_emoji_dataset.parquet')\n",
    "dataset = EmojiDiffusionDataset(df)\n",
    "train_samples = int(len(dataset) * 0.9)\n",
    "train_set, val_set = random_split(dataset, [train_samples, len(dataset) - train_samples])\n",
    "# Using the full dataset for training; you can switch to train_set if preferred.\n",
    "train_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=1)\n",
    "\n",
    "# Load Model\n",
    "model_id = \"sd-legacy/stable-diffusion-v1-5\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float32  # Important for compatibility with MPS\n",
    ").to(\"mps\")\n",
    "pipe.enable_attention_slicing()  # Memory-efficient optimization\n",
    "\n",
    "# Disable safety checker by using a dummy function that returns the images as-is.\n",
    "if pipe.safety_checker is not None:\n",
    "    pipe.safety_checker = lambda images, clip_input, **kwargs: (images, [False] * len(images))\n",
    "\n",
    "# Q-LoRA configuration for UNet\n",
    "lora_config_unet = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"to_q\", \"to_k\", \"to_v\", \"to_out.0\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "pipe.unet = get_peft_model(pipe.unet, lora_config_unet)\n",
    "pipe.unet.print_trainable_parameters()\n",
    "\n",
    "# Accelerator setup without mixed precision (required for MPS)\n",
    "accelerator = Accelerator(mixed_precision='no', gradient_accumulation_steps=1)\n",
    "optimizer = torch.optim.AdamW(pipe.unet.parameters(), lr=1e-4)\n",
    "pipe.unet, optimizer, train_loader = accelerator.prepare(pipe.unet, optimizer, train_loader)\n",
    "\n",
    "# Lists to store loss values per epoch\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "# Directory for saving generated images and loss graph\n",
    "gen_output_dir = \"../evaluation/lora\"\n",
    "os.makedirs(gen_output_dir, exist_ok=True)\n",
    "\n",
    "# Increase epochs to 20\n",
    "num_epochs = 16\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    pipe.unet.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # ------------------------\n",
    "    # Training Loop\n",
    "    # ------------------------\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get pixel values and prompt text from the batch\n",
    "        pixel_values = batch[\"pixel_values\"].to(\"mps\")\n",
    "        prompts = batch[\"prompt\"]\n",
    "\n",
    "        # Tokenize the prompts and move tensors to the proper device\n",
    "        text_inputs = pipe.tokenizer(\n",
    "            prompts,\n",
    "            padding=\"max_length\",\n",
    "            max_length=pipe.tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        text_inputs = {k: v.to(\"mps\") for k, v in text_inputs.items()}\n",
    "\n",
    "        # Get conditioning embeddings using the built-in text encoder\n",
    "        with torch.no_grad():\n",
    "            encoder_hidden_states = pipe.text_encoder(**text_inputs).last_hidden_state\n",
    "\n",
    "        # Encode images to latent space\n",
    "        with torch.no_grad():\n",
    "            latents = pipe.vae.encode(pixel_values).latent_dist.sample()\n",
    "            latents = latents * pipe.vae.config.scaling_factor\n",
    "\n",
    "        # Generate random noise and timesteps\n",
    "        noise = torch.randn_like(latents).to(\"mps\")\n",
    "        timesteps = torch.randint(0, pipe.scheduler.config.num_train_timesteps, (latents.size(0),), device=\"mps\").long()\n",
    "        noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "        # Predict noise using the UNet with encoder_hidden_states from the text encoder\n",
    "        noise_pred = pipe.unet(noisy_latents, timesteps, encoder_hidden_states=encoder_hidden_states).sample\n",
    "\n",
    "        # Compute loss and update\n",
    "        loss = torch.nn.functional.mse_loss(noise_pred, noise)\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    training_losses.append(avg_train_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Average Training Loss: {avg_train_loss:.6f}\")\n",
    "\n",
    "    # ------------------------\n",
    "    # Validation Loop (Loss Computation)\n",
    "    # ------------------------\n",
    "    pipe.unet.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation Loss\"):\n",
    "            pixel_values = batch[\"pixel_values\"].to(\"mps\")\n",
    "            prompts = batch[\"prompt\"]\n",
    "\n",
    "            text_inputs = pipe.tokenizer(\n",
    "                prompts,\n",
    "                padding=\"max_length\",\n",
    "                max_length=pipe.tokenizer.model_max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            text_inputs = {k: v.to(\"mps\") for k, v in text_inputs.items()}\n",
    "\n",
    "            encoder_hidden_states = pipe.text_encoder(**text_inputs).last_hidden_state\n",
    "\n",
    "            latents = pipe.vae.encode(pixel_values).latent_dist.sample()\n",
    "            latents = latents * pipe.vae.config.scaling_factor\n",
    "\n",
    "            noise = torch.randn_like(latents).to(\"mps\")\n",
    "            timesteps = torch.randint(0, pipe.scheduler.config.num_train_timesteps, (latents.size(0),), device=\"mps\").long()\n",
    "            noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            noise_pred = pipe.unet(noisy_latents, timesteps, encoder_hidden_states=encoder_hidden_states).sample\n",
    "\n",
    "            loss = torch.nn.functional.mse_loss(noise_pred, noise)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    validation_losses.append(avg_val_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Average Validation Loss: {avg_val_loss:.6f}\")\n",
    "\n",
    "    # ------------------------\n",
    "    # Generate an Emoji with prompt \"grinning face\"\n",
    "    # ------------------------\n",
    "    pipe.unet.eval()\n",
    "    with torch.no_grad():\n",
    "        generated = pipe(prompt=\"grinning face\", num_inference_steps=50, guidance_scale=7.5, width=256, height=256)\n",
    "        image = generated.images[0]\n",
    "        filename = os.path.join(gen_output_dir, f\"epoch{epoch+1}_grinning_face.png\")\n",
    "        image.save(filename)\n",
    "        print(f\"Generated emoji saved to {filename}\")\n",
    "\n",
    "    # ------------------------\n",
    "    # Save Checkpoint at End of Epoch\n",
    "    # ------------------------\n",
    "    checkpoint_dir = f\"../evaluation/emoji_diffusion_qlora/checkpoint_epoch_{epoch+1}\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    pipe.unet.save_pretrained(checkpoint_dir)\n",
    "\n",
    "# Save final model\n",
    "final_model_dir = \"../evaluation/emoji_diffusion_qlora/final_model\"\n",
    "os.makedirs(final_model_dir, exist_ok=True)\n",
    "pipe.unet.save_pretrained(final_model_dir)\n",
    "\n",
    "# ------------------------\n",
    "# Plot and Save Training vs Validation Loss Graph\n",
    "# ------------------------\n",
    "epochs = range(1, num_epochs + 1)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(epochs, training_losses, label='Training Loss', marker='o')\n",
    "plt.plot(epochs, validation_losses, label='Validation Loss', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "loss_plot_path = os.path.join(gen_output_dir, \"training_validation_loss.png\")\n",
    "plt.savefig(loss_plot_path)\n",
    "plt.close()\n",
    "print(f\"Training and validation loss graph saved to {loss_plot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ee5e45414ef32b",
   "metadata": {},
   "source": [
    "## Latest with LoRA on text encoder as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c37aa43f9a610db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T10:13:00.665432Z",
     "start_time": "2025-03-31T18:42:40.267001Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3cd754906014f0daa37228feb8fd8ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,594,368 || all params: 861,115,332 || trainable%: 0.1852\n",
      "trainable params: 442,368 || all params: 123,502,848 || trainable%: 0.3582\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb42e80aa61f459ebf036fb7a3e37d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/20 - Training:   0%|          | 0/1511 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] - Average Training Loss: 0.030181\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a6589a388d488ea95b9b8b86c3fece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/20 - Validation Loss:   0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] - Average Validation Loss: 0.026257\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c0f2b132c974a7fa7c58747d4494ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch1_grinning_face.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c2d0814e05a490c999be5b1507a9723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/20 - Training:   0%|          | 0/1511 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20] - Average Training Loss: 0.027553\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af87bfabf93c4a98b8cc48d079ad5469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/20 - Validation Loss:   0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20] - Average Validation Loss: 0.029146\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f5624fe0b78446a9a22b110b82987ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch2_grinning_face.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9308b358b944942a780b828d769dda3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/20 - Training:   0%|          | 0/1511 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20] - Average Training Loss: 0.027370\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c26e8a5e37a4c659d06a69c37fb0359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/20 - Validation Loss:   0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20] - Average Validation Loss: 0.024582\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f50d60baf37a484a91025ba27e1532ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch3_grinning_face.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf2126a445584394a150f588e6156959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/20 - Training:   0%|          | 0/1511 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20] - Average Training Loss: 0.025244\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc9bb3f9cc734970aa090b968e0b79d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/20 - Validation Loss:   0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20] - Average Validation Loss: 0.024046\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98de51c817b54c00a5ba83d04df04c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch4_grinning_face.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d613156b71f4e8fadd24d49031cf862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/20 - Training:   0%|          | 0/1511 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20] - Average Training Loss: 0.025508\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1856fcb846c143d799bf466b2c1cd479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/20 - Validation Loss:   0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20] - Average Validation Loss: 0.028340\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73077cc577cf4a1d80be05f084561d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch5_grinning_face.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c1d3f77536b4920919cdad8239f6ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/20 - Training:   0%|          | 0/1511 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20] - Average Training Loss: 0.024870\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d4e11e569146e49ffc616d5583bd6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/20 - Validation Loss:   0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20] - Average Validation Loss: 0.026163\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dfcf9371f8649f08e6ca901446bcd38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch6_grinning_face.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c221a328fa40a8a275c59628d7f170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/20 - Training:   0%|          | 0/1511 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20] - Average Training Loss: 0.025472\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a74f5a8d0c9a409ba45a7a70ca26d43b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/20 - Validation Loss:   0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20] - Average Validation Loss: 0.027161\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22bcb28024a44296a00329d4e8bdecc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch7_grinning_face.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e1c90566c9a4d929712c76a812360f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/20 - Training:   0%|          | 0/1511 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20] - Average Training Loss: 0.024099\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e204100faece4e0b8eb6762874c3265b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/20 - Validation Loss:   0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20] - Average Validation Loss: 0.022939\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ab8cd736d046259230e3e106474b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch8_grinning_face.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a299841c14204572961aaa431db56ada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/20 - Training:   0%|          | 0/1511 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20] - Average Training Loss: 0.024680\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a2700e6788f47549f19c51c0b6dd968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/20 - Validation Loss:   0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20] - Average Validation Loss: 0.025604\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b8f581152943878483169ecfbec18b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch9_grinning_face.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c4ada26d8c94381993df9aaa8afc0aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/20 - Training:   0%|          | 0/1511 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20] - Average Training Loss: 0.023915\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "620e17e7838345f982eaa9e7b8313ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/20 - Validation Loss:   0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20] - Average Validation Loss: 0.025966\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "098beffb30a84b30befdaf2434c960c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch10_grinning_face.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f639298d9eb45e4b2a9cd936b290214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/20 - Training:   0%|          | 0/1511 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20] - Average Training Loss: 0.023389\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f638574008e45c5977b344781d90568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/20 - Validation Loss:   0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20] - Average Validation Loss: 0.025517\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8308d7b340e6499780900e8617278dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch11_grinning_face.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c46425307a41c384af835a0e59ae52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/20 - Training:   0%|          | 0/1511 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20] - Average Training Loss: 0.024187\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1999db39d44acca8446d96424196dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/20 - Validation Loss:   0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20] - Average Validation Loss: 0.025474\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dbdde10945a48b988979d6a2898af10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch12_grinning_face.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1eacc26e588471b8300e8556c2d7e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/20 - Training:   0%|          | 0/1511 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20] - Average Training Loss: 0.023374\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc3217964d6489da533effea5609db7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/20 - Validation Loss:   0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20] - Average Validation Loss: 0.023072\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e61d661036e2407fb9a56b5593061499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch13_grinning_face.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f5dca1085dc47ab94999dadc466443f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/20 - Training:   0%|          | 0/1511 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20] - Average Training Loss: 0.023259\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "834d5d9c139e4942b3f8c61602764ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/20 - Validation Loss:   0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20] - Average Validation Loss: 0.020529\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b21be7951d7240c79b70a037b52bb031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch14_grinning_face.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e563c51001274803bc92a039054556c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/20 - Training:   0%|          | 0/1511 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20] - Average Training Loss: 0.022325\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98233aed19574697a6137918f23a15c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/20 - Validation Loss:   0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20] - Average Validation Loss: 0.021598\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c869189ca942faa9f5bd0011aff72e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch15_grinning_face.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6956e8fd1f4e4ab9becf3b9bdb5bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/20 - Training:   0%|          | 0/1511 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20] - Average Training Loss: 0.022773\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acedfe2c6d9047bd8c69153a3de52bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/20 - Validation Loss:   0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20] - Average Validation Loss: 0.023074\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb9e300936b145c2b4a47a9767937460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch16_grinning_face.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c723646a254c07bfd7d5239df47835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/20 - Training:   0%|          | 0/1511 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20] - Average Training Loss: 0.023216\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e917150445d4f92980998bc07191076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/20 - Validation Loss:   0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20] - Average Validation Loss: 0.023264\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f570e3c8e44c41b08e014d32ee8479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch17_grinning_face.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb742e370234e1986eb7a8165dfdfbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/20 - Training:   0%|          | 0/1511 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20] - Average Training Loss: 0.022304\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ce81e9e1904804a7f2a184c7848eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/20 - Validation Loss:   0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20] - Average Validation Loss: 0.021595\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab2ef3bdc434737b5e65fa69b99e6a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch18_grinning_face.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "405d2e6e6fc94b6d895dbe88127fdaf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/20 - Training:   0%|          | 0/1511 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20] - Average Training Loss: 0.021824\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74ed0a527cf4c5db0812a79b73d53a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/20 - Validation Loss:   0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20] - Average Validation Loss: 0.022110\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83b2164e46a4c62aeebfabba4679025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch19_grinning_face.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63e73b8570349fd8f5845103d14c350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/20 - Training:   0%|          | 0/1511 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20] - Average Training Loss: 0.021106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "facad5c618f44e629398faeb0de86894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/20 - Validation Loss:   0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20] - Average Validation Loss: 0.019410\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646a717796d54aa098f76c87cffa7aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated emoji saved to ../evaluation/lora/epoch20_grinning_face.png\n",
      "Training and validation loss graph saved to ../evaluation/lora/training_validation_loss.png\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import DataLoader, random_split\n",
    "# from diffusers import StableDiffusionPipeline\n",
    "# from peft import LoraConfig, get_peft_model\n",
    "# from accelerate import Accelerator\n",
    "# from tqdm.auto import tqdm\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# import matplotlib.pyplot as plt  # For plotting the loss graph\n",
    "# \n",
    "# # Device configuration\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'mps')\n",
    "# \n",
    "# # Data Preparation: Only return image and prompt\n",
    "# class EmojiDiffusionDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, df):\n",
    "#         self.df = df\n",
    "# \n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "# \n",
    "#     def __getitem__(self, idx):\n",
    "#         image_tensor = torch.load(self.df.iloc[idx]['image_path']).float()\n",
    "#         prompt = self.df.iloc[idx]['prompt']\n",
    "#         return {\"pixel_values\": image_tensor, \"prompt\": prompt}\n",
    "# \n",
    "# # Load Dataset\n",
    "# df = pd.read_parquet('../data/processed_emoji_dataset.parquet')\n",
    "# dataset = EmojiDiffusionDataset(df)\n",
    "# train_samples = int(len(dataset) * 0.9)\n",
    "# train_set, val_set = random_split(dataset, [train_samples, len(dataset) - train_samples])\n",
    "# # Using the full dataset for training; you can switch to train_set if preferred.\n",
    "# train_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "# val_loader = DataLoader(val_set, batch_size=2)\n",
    "# \n",
    "# # Load Model\n",
    "# model_id = \"sd-legacy/stable-diffusion-v1-5\"\n",
    "# pipe = StableDiffusionPipeline.from_pretrained(\n",
    "#     model_id,\n",
    "#     torch_dtype=torch.float32  # Important for compatibility with MPS\n",
    "# ).to(\"mps\")\n",
    "# pipe.enable_attention_slicing()  # Memory-efficient optimization\n",
    "# \n",
    "# # Disable safety checker by using a dummy function that returns the images as-is.\n",
    "# if pipe.safety_checker is not None:\n",
    "#     pipe.safety_checker = lambda images, clip_input, **kwargs: (images, [False] * len(images))\n",
    "# \n",
    "# # Q-LoRA configuration for UNet\n",
    "# lora_config_unet = LoraConfig(\n",
    "#     r=8,\n",
    "#     lora_alpha=16,\n",
    "#     target_modules=[\"to_q\", \"to_k\", \"to_v\", \"to_out.0\"],\n",
    "#     lora_dropout=0.1,\n",
    "#     bias=\"none\"\n",
    "# )\n",
    "# pipe.unet = get_peft_model(pipe.unet, lora_config_unet)\n",
    "# pipe.unet.print_trainable_parameters()\n",
    "# \n",
    "# # Q-LoRA configuration for Text Encoder\n",
    "# lora_config_text = LoraConfig(\n",
    "#     r=8,\n",
    "#     lora_alpha=16,\n",
    "#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "#     lora_dropout=0.1,\n",
    "#     bias=\"none\"\n",
    "# )\n",
    "# pipe.text_encoder = get_peft_model(pipe.text_encoder, lora_config_text)\n",
    "# pipe.text_encoder.print_trainable_parameters()\n",
    "# \n",
    "# # Accelerator setup without mixed precision (required for MPS)\n",
    "# accelerator = Accelerator(mixed_precision='no', gradient_accumulation_steps=4)\n",
    "# optimizer = torch.optim.AdamW(pipe.unet.parameters(), lr=5e-5)\n",
    "# pipe.unet, optimizer, train_loader = accelerator.prepare(pipe.unet, optimizer, train_loader)\n",
    "# \n",
    "# # Lists to store loss values per epoch\n",
    "# training_losses = []\n",
    "# validation_losses = []\n",
    "# \n",
    "# # Directory for saving generated images and loss graph\n",
    "# gen_output_dir = \"../evaluation/lora\"\n",
    "# os.makedirs(gen_output_dir, exist_ok=True)\n",
    "# \n",
    "# # Increase epochs to 20\n",
    "# num_epochs = 20\n",
    "# \n",
    "# for epoch in range(num_epochs):\n",
    "#     pipe.unet.train()\n",
    "#     total_train_loss = 0\n",
    "# \n",
    "#     # ------------------------\n",
    "#     # Training Loop\n",
    "#     # ------------------------\n",
    "#     for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "#         optimizer.zero_grad()\n",
    "# \n",
    "#         # Get pixel values and prompt text from the batch\n",
    "#         pixel_values = batch[\"pixel_values\"].to(\"mps\")\n",
    "#         prompts = batch[\"prompt\"]\n",
    "# \n",
    "#         # Tokenize the prompts and move tensors to the proper device\n",
    "#         text_inputs = pipe.tokenizer(\n",
    "#             prompts,\n",
    "#             padding=\"max_length\",\n",
    "#             max_length=pipe.tokenizer.model_max_length,\n",
    "#             return_tensors=\"pt\"\n",
    "#         )\n",
    "#         text_inputs = {k: v.to(\"mps\") for k, v in text_inputs.items()}\n",
    "# \n",
    "#         # Get conditioning embeddings using the built-in text encoder\n",
    "#         with torch.no_grad():\n",
    "#             encoder_hidden_states = pipe.text_encoder(**text_inputs).last_hidden_state\n",
    "# \n",
    "#         # Encode images to latent space\n",
    "#         with torch.no_grad():\n",
    "#             latents = pipe.vae.encode(pixel_values).latent_dist.sample()\n",
    "#             latents = latents * pipe.vae.config.scaling_factor\n",
    "# \n",
    "#         # Generate random noise and timesteps\n",
    "#         noise = torch.randn_like(latents).to(\"mps\")\n",
    "#         timesteps = torch.randint(0, pipe.scheduler.config.num_train_timesteps, (latents.size(0),), device=\"mps\").long()\n",
    "#         noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n",
    "# \n",
    "#         # Predict noise using the UNet with encoder_hidden_states from the text encoder\n",
    "#         noise_pred = pipe.unet(noisy_latents, timesteps, encoder_hidden_states=encoder_hidden_states).sample\n",
    "# \n",
    "#         # Compute loss and update\n",
    "#         loss = torch.nn.functional.mse_loss(noise_pred, noise)\n",
    "#         accelerator.backward(loss)\n",
    "#         optimizer.step()\n",
    "# \n",
    "#         total_train_loss += loss.item()\n",
    "# \n",
    "#     avg_train_loss = total_train_loss / len(train_loader)\n",
    "#     training_losses.append(avg_train_loss)\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}] - Average Training Loss: {avg_train_loss:.6f}\")\n",
    "# \n",
    "#     # ------------------------\n",
    "#     # Validation Loop (Loss Computation)\n",
    "#     # ------------------------\n",
    "#     pipe.unet.eval()\n",
    "#     total_val_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation Loss\"):\n",
    "#             pixel_values = batch[\"pixel_values\"].to(\"mps\")\n",
    "#             prompts = batch[\"prompt\"]\n",
    "# \n",
    "#             text_inputs = pipe.tokenizer(\n",
    "#                 prompts,\n",
    "#                 padding=\"max_length\",\n",
    "#                 max_length=pipe.tokenizer.model_max_length,\n",
    "#                 return_tensors=\"pt\"\n",
    "#             )\n",
    "#             text_inputs = {k: v.to(\"mps\") for k, v in text_inputs.items()}\n",
    "# \n",
    "#             encoder_hidden_states = pipe.text_encoder(**text_inputs).last_hidden_state\n",
    "# \n",
    "#             latents = pipe.vae.encode(pixel_values).latent_dist.sample()\n",
    "#             latents = latents * pipe.vae.config.scaling_factor\n",
    "# \n",
    "#             noise = torch.randn_like(latents).to(\"mps\")\n",
    "#             timesteps = torch.randint(0, pipe.scheduler.config.num_train_timesteps, (latents.size(0),), device=\"mps\").long()\n",
    "#             noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n",
    "# \n",
    "#             noise_pred = pipe.unet(noisy_latents, timesteps, encoder_hidden_states=encoder_hidden_states).sample\n",
    "# \n",
    "#             loss = torch.nn.functional.mse_loss(noise_pred, noise)\n",
    "#             total_val_loss += loss.item()\n",
    "# \n",
    "#     avg_val_loss = total_val_loss / len(val_loader)\n",
    "#     validation_losses.append(avg_val_loss)\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}] - Average Validation Loss: {avg_val_loss:.6f}\")\n",
    "# \n",
    "#     # ------------------------\n",
    "#     # Generate an Emoji with prompt \"grinning face\"\n",
    "#     # ------------------------\n",
    "#     pipe.unet.eval()\n",
    "#     with torch.no_grad():\n",
    "#         generated = pipe(prompt=\"grinning face\", num_inference_steps=50, guidance_scale=7.5, width=256, height=256)\n",
    "#         image = generated.images[0]\n",
    "#         filename = os.path.join(gen_output_dir, f\"epoch{epoch+1}_grinning_face.png\")\n",
    "#         image.save(filename)\n",
    "#         print(f\"Generated emoji saved to {filename}\")\n",
    "# \n",
    "#     # ------------------------\n",
    "#     # Save Checkpoint at End of Epoch\n",
    "#     # ------------------------\n",
    "#     checkpoint_dir = f\"../evaluation/emoji_diffusion_qlora/checkpoint_epoch_{epoch+1}\"\n",
    "#     os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "#     pipe.unet.save_pretrained(checkpoint_dir)\n",
    "# \n",
    "# # Save final model\n",
    "# final_model_dir = \"../evaluation/emoji_diffusion_qlora/final_model\"\n",
    "# os.makedirs(final_model_dir, exist_ok=True)\n",
    "# pipe.unet.save_pretrained(final_model_dir)\n",
    "# \n",
    "# # ------------------------\n",
    "# # Plot and Save Training vs Validation Loss Graph\n",
    "# # ------------------------\n",
    "# epochs = range(1, num_epochs + 1)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(epochs, training_losses, label='Training Loss', marker='o')\n",
    "# plt.plot(epochs, validation_losses, label='Validation Loss', marker='o')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Training vs Validation Loss')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# loss_plot_path = os.path.join(gen_output_dir, \"training_validation_loss.png\")\n",
    "# plt.savefig(loss_plot_path)\n",
    "# plt.close()\n",
    "# print(f\"Training and validation loss graph saved to {loss_plot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80508199b430af4",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649b10f7d47af2bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T11:14:18.084429Z",
     "start_time": "2025-04-03T11:13:00.088061Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ece1968c6de40ec974abee93cf0b3ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f5e205e150e40518d35a755036afa15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference result saved to cat_face.png\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from peft import PeftModel, LoraConfig\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Device configuration (use 'cuda' if available, otherwise 'mps' or 'cpu')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps')\n",
    "\n",
    "# Load the base pipeline\n",
    "model_id = \"sd-legacy/stable-diffusion-v1-5\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float32\n",
    ").to(device)\n",
    "pipe.enable_attention_slicing()  # Memory-efficient optimization\n",
    "\n",
    "# Disable the safety checker to bypass NSFW filtering\n",
    "if pipe.safety_checker is not None:\n",
    "    pipe.safety_checker = lambda images, clip_input, **kwargs: (images, [False] * len(images))\n",
    "\n",
    "# Q-LoRA configuration (must match your training configuration)\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"to_q\", \"to_k\", \"to_v\", \"to_out.0\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# Directory for the final trained LoRA weights\n",
    "final_model_dir = \"../evaluation/emoji_diffusion_qlora/final_model\"\n",
    "\n",
    "# Load the LoRA weights onto the UNet using PeftModel.from_pretrained\n",
    "pipe.unet = PeftModel.from_pretrained(pipe.unet, final_model_dir)\n",
    "\n",
    "# Inference parameters\n",
    "prompt = \"monkey with banana\"  # Change to your desired prompt\n",
    "num_inference_steps = 50\n",
    "guidance_scale = 7.5\n",
    "\n",
    "# Generate the image\n",
    "with torch.no_grad():\n",
    "    generated = pipe(prompt=prompt, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale, height=256, width=256)\n",
    "    image = generated.images[0]\n",
    "\n",
    "# Save the generated image\n",
    "output_path = \"cat_face.png\"\n",
    "os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) != \"\" else \".\", exist_ok=True)\n",
    "image.save(output_path)\n",
    "print(f\"Inference result saved to {output_path}\")\n",
    "\n",
    "# Optionally, display the image\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b7093e711cdb5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
