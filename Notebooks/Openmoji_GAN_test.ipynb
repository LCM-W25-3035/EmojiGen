{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# Device Configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Using NVIDIA GPU (CUDA)')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print('Using Mac GPU (MPS)')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using CPU')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_parquet('../data/processed_openmoji_dataset.parquet')\n",
    "df[\"combined_embedding\"] = df[\"combined_embedding\"].apply(lambda x: np.array(x, dtype=np.float32))\n",
    "\n",
    "# Custom Dataset Class\n",
    "class EmojiDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        embedding = torch.tensor(self.dataframe.iloc[idx]['combined_embedding']).float()\n",
    "        image_tensor = torch.load(self.dataframe.iloc[idx]['image_path']).float()\n",
    "        return embedding, image_tensor\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = EmojiDataset(df)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Reference: https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html\n",
    "Learning ConvTranspose2d layers for upsampling\n",
    "\"\"\"\n",
    "# Generator Model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, embedding_dim, image_channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Compute the starting spatial size dynamically\n",
    "        self.init_size = 32 // 4  # If final image is 32x32, init_size should be 8x8\n",
    "        \n",
    "        self.noise_fc = nn.Sequential(\n",
    "            nn.Linear(noise_dim, 256 * self.init_size * self.init_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.embed_fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 256 * self.init_size * self.init_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, image_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, noise, embed):\n",
    "        noise_features = self.noise_fc(noise).view(noise.size(0), 256, self.init_size, self.init_size)\n",
    "        embed_features = self.embed_fc(embed).view(embed.size(0), 256, self.init_size, self.init_size)\n",
    "        \n",
    "        # x = torch.cat((noise_features, embed_features), dim=1)\n",
    "        x = noise_features + embed_features\n",
    "        # x = self.fc(x).view(x.size(0), 256, 4, 4)\n",
    "        x = self.conv_blocks(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Reference: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "Learning Conv2d layers for downsampling\n",
    "\"\"\"\n",
    "# Discriminator Model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, embedding_dim, image_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.embed_fc = nn.Linear(embedding_dim, 32 * 32)  # Directly match image size\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.Conv2d(image_channels + 1, 64, kernel_size=4, stride=2, padding=1), # added +1\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),  # Dropout to weaken discriminator\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),  # Dropout to weaken discriminator\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),  # Dropout to weaken discriminator\n",
    "        )\n",
    "        # self.fc = nn.Linear(256 * 4 * 4 + embedding_dim, 1)\n",
    "        self.fc = nn.Linear(256 * 8 * 8, 1)\n",
    "        # Project embedding into a spatial format\n",
    "        # self.embed_fc = nn.Linear(embedding_dim, 8 * 8)  # Map embeddings to 4x4 spatial size\n",
    "        \n",
    "    def forward(self, img, embed):\n",
    "        # Convert embedding into spatial form\n",
    "        embed_features = self.embed_fc(embed).view(img.size(0), 1, img.shape[2], img.shape[3])\n",
    "        # Concatenate embeddings as an extra channel\n",
    "        x = torch.cat((img, embed_features), dim=1)  # (batch, image_channels + 1, H, W)\n",
    "        x = self.conv_blocks(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        return torch.sigmoid(self.fc(x))\n",
    "\n",
    "# Model Initialization\n",
    "noise_dim = 100\n",
    "embedding_dim = len(df['combined_embedding'][0])\n",
    "generator = Generator(noise_dim, embedding_dim).to(device)\n",
    "discriminator = Discriminator(embedding_dim).to(device)\n",
    "\n",
    "# Loss and Optimizers\n",
    "criterion = nn.BCELoss()\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.00005, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "num_epochs = 5000\n",
    "d_losses, g_losses, d_fake_conf, d_real_conf = [], [], [], []\n",
    "\n",
    "# Initialize fixed noise once before training loop\n",
    "fixed_noise = torch.zeros(40, noise_dim).to(device)  # This remains constant throughout training\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True)\n",
    "    \n",
    "    for combined_embeddings, real_images in progress_bar:\n",
    "        combined_embeddings = combined_embeddings.to(device)\n",
    "        real_images = real_images.to(device)\n",
    "        \n",
    "        # Train Discriminator\n",
    "        noise = torch.randn(real_images.size(0), noise_dim).to(device)\n",
    "        fake_images = generator(noise, combined_embeddings)\n",
    "        \n",
    "        real_labels = torch.full((real_images.size(0), 1), 0.9).to(device)\n",
    "        fake_labels = torch.full((real_images.size(0), 1), 0.1).to(device)\n",
    "        \n",
    "        real_outputs = discriminator(real_images, combined_embeddings)\n",
    "        fake_outputs = discriminator(fake_images.detach(), combined_embeddings)\n",
    "        \n",
    "        # Discriminator confidence\n",
    "        real_confidence = real_outputs.mean().item()\n",
    "        fake_confidence = fake_outputs.mean().item()\n",
    "        \n",
    "        d_loss_real = criterion(real_outputs, real_labels)\n",
    "        d_loss_fake = criterion(fake_outputs, fake_labels)\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        \n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # Train Generator\n",
    "        fake_outputs = discriminator(fake_images, combined_embeddings)\n",
    "        g_loss = criterion(fake_outputs, real_labels)\n",
    "        \n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "    \n",
    "    # Store loss values\n",
    "    d_losses.append(d_loss.item())\n",
    "    g_losses.append(g_loss.item())\n",
    "    d_fake_conf.append(fake_confidence)\n",
    "    d_real_conf.append(real_confidence)\n",
    "\n",
    "    # Using tqdm.write() for clean logging\n",
    "    tqdm.write(f\"Epoch [{epoch+1}/{num_epochs}] | D Real Confidence: {real_confidence:.4f} | D Fake Confidence: {fake_confidence:.4f} | G Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "    # Visualization every 25 epochs\n",
    "    if (epoch + 1) % 25 == 0:\n",
    "        with torch.no_grad():\n",
    "            random_noise = torch.randn(40, noise_dim).to(device)\n",
    "            \n",
    "            sample_embeddings, real_images = next(iter(dataloader))\n",
    "            real_count = min(40, real_images.shape[0])  # Ensure we don't slice out of bounds\n",
    "\n",
    "            real_images = real_images[:real_count].cpu()\n",
    "            sample_embeddings = sample_embeddings[:real_count].to(device)\n",
    "\n",
    "            generated_fixed = generator(fixed_noise[:real_count], sample_embeddings).cpu()\n",
    "            generated_random = generator(random_noise[:real_count], sample_embeddings).cpu()\n",
    "        \n",
    "        \n",
    "        print(f\"real_images.shape: {real_images.shape}\")\n",
    "        print(f\"generated_fixed.shape: {generated_fixed.shape}\")\n",
    "        print(f\"generated_random.shape: {generated_random.shape}\")\n",
    "        # Concatenate images for visualization: Real | Fixed | Random\n",
    "        all_images = torch.cat((real_images, generated_fixed, generated_random), dim=0)\n",
    "        \n",
    "        # Display Grid (Real on top, Fixed in middle, Random at bottom)\n",
    "        grid = make_grid(all_images, nrow=10, normalize=True)\n",
    "\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(np.transpose(grid.numpy(), (1, 2, 0)))\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Real (Top) | Fixed Noise (Middle) | Random Noise (Bottom) - Epoch {epoch+1}')\n",
    "        plt.show()\n",
    "\n",
    "        # Plot Loss Chart\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(range(1, len(d_real_conf) + 1), d_real_conf, label=\"D Real Confidence\")\n",
    "        plt.plot(range(1, len(d_fake_conf) + 1), d_fake_conf, label=\"D Fake Confidence\")\n",
    "        plt.plot(range(1, len(g_losses) + 1), g_losses, label=\"G Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Training Loss Over Time\")\n",
    "        plt.show()"
   ],
   "id": "eeca5e52293a57bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# Device Configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Using NVIDIA GPU (CUDA)')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print('Using Mac GPU (MPS)')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using CPU')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_parquet('../data/processed_openmoji_dataset.parquet')\n",
    "df[\"combined_embedding\"] = df[\"combined_embedding\"].apply(lambda x: np.array(x, dtype=np.float32))\n",
    "\n",
    "# Custom Dataset Class\n",
    "class EmojiDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        embedding = torch.tensor(self.dataframe.iloc[idx]['combined_embedding']).float()\n",
    "        image_tensor = torch.load(self.dataframe.iloc[idx]['image_path']).float()\n",
    "        return embedding, image_tensor\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = EmojiDataset(df)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Reference: https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html\n",
    "Learning ConvTranspose2d layers for upsampling\n",
    "\"\"\"\n",
    "# Generator Model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, embedding_dim, image_channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.noise_fc = nn.Sequential(\n",
    "            nn.Linear(noise_dim, 256 * 4 * 4),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.embed_fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 256 * 4 * 4),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, image_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, noise, embed):\n",
    "        noise_features = self.noise_fc(noise).view(noise.size(0), 256, 4, 4)\n",
    "        embed_features = self.embed_fc(embed).view(embed.size(0), 256, 4, 4)\n",
    "        \n",
    "        # x = torch.cat((noise_features, embed_features), dim=1)\n",
    "        x = noise_features + embed_features\n",
    "        # x = self.fc(x).view(x.size(0), 256, 4, 4)\n",
    "        x = self.conv_blocks(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Reference: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "Learning Conv2d layers for downsampling\n",
    "\"\"\"\n",
    "# Discriminator Model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, embedding_dim, image_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.Conv2d(image_channels + 1, 64, kernel_size=4, stride=2, padding=1), # added +1\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),  # Dropout to weaken discriminator\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),  # Dropout to weaken discriminator\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),  # Dropout to weaken discriminator\n",
    "        )\n",
    "        # self.fc = nn.Linear(256 * 4 * 4 + embedding_dim, 1)\n",
    "        self.fc = nn.Linear(256 * 4 * 4, 1)\n",
    "        # Project embedding into a spatial format\n",
    "        self.embed_fc = nn.Linear(embedding_dim, 4 * 4)  # Map embeddings to 4x4 spatial size\n",
    "        \n",
    "    def forward(self, img, embed):\n",
    "        batch_size = img.size(0)\n",
    "        # Convert embedding into spatial form\n",
    "        embed_features = self.embed_fc(embed).view(batch_size, 1, 4, 4)  # Shape: (batch, 1, 4, 4)\n",
    "        # Resize embedding map to match image dimensions (expand)\n",
    "        embed_features = torch.nn.functional.interpolate(embed_features, size=(img.shape[2], img.shape[3]))\n",
    "        # Concatenate embeddings as an extra channel\n",
    "        x = torch.cat((img, embed_features), dim=1)  # (batch, image_channels + 1, H, W)\n",
    "        x = self.conv_blocks(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        return torch.sigmoid(self.fc(x))\n",
    "        \n",
    "        # x = self.conv_blocks(img)\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        # x = torch.cat((x, embed), dim=1)\n",
    "        # return torch.sigmoid(self.fc(x))\n",
    "\n",
    "# Model Initialization\n",
    "noise_dim = 100\n",
    "embedding_dim = len(df['combined_embedding'][0])\n",
    "generator = Generator(noise_dim, embedding_dim).to(device)\n",
    "discriminator = Discriminator(embedding_dim).to(device)\n",
    "\n",
    "# Loss and Optimizers\n",
    "criterion = nn.BCELoss()\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0005, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "num_epochs = 1000\n",
    "d_losses, g_losses, d_fake_conf, d_real_conf = [], [], [], []\n",
    "\n",
    "# Initialize fixed noise once before training loop\n",
    "fixed_noise = torch.zeros(40, noise_dim).to(device)  # This remains constant throughout training\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True)\n",
    "    \n",
    "    for combined_embeddings, real_images in progress_bar:\n",
    "        combined_embeddings = combined_embeddings.to(device)\n",
    "        real_images = real_images.to(device)\n",
    "        \n",
    "        # Train Discriminator\n",
    "        noise = torch.randn(real_images.size(0), noise_dim).to(device)\n",
    "        fake_images = generator(noise, combined_embeddings)\n",
    "        \n",
    "        real_labels = torch.full((real_images.size(0), 1), 0.9).to(device)\n",
    "        fake_labels = torch.full((real_images.size(0), 1), 0.1).to(device)\n",
    "        \n",
    "        real_outputs = discriminator(real_images, combined_embeddings)\n",
    "        fake_outputs = discriminator(fake_images.detach(), combined_embeddings)\n",
    "        \n",
    "        # Discriminator confidence\n",
    "        real_confidence = real_outputs.mean().item()\n",
    "        fake_confidence = fake_outputs.mean().item()\n",
    "        \n",
    "        d_loss_real = criterion(real_outputs, real_labels)\n",
    "        d_loss_fake = criterion(fake_outputs, fake_labels)\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        \n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # Train Generator\n",
    "        fake_outputs = discriminator(fake_images, combined_embeddings)\n",
    "        g_loss = criterion(fake_outputs, real_labels)\n",
    "        \n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "    \n",
    "    # Store loss values\n",
    "    d_losses.append(d_loss.item())\n",
    "    g_losses.append(g_loss.item())\n",
    "    d_fake_conf.append(fake_confidence)\n",
    "    d_real_conf.append(real_confidence)\n",
    "\n",
    "    # Using tqdm.write() for clean logging\n",
    "    tqdm.write(f\"Epoch [{epoch+1}/{num_epochs}] | D Real Confidence: {real_confidence:.4f} | D Fake Confidence: {fake_confidence:.4f} | G Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "    # Visualization every 25 epochs\n",
    "    if (epoch + 1) % 25 == 0:\n",
    "        with torch.no_grad():\n",
    "            random_noise = torch.randn(40, noise_dim).to(device)\n",
    "            \n",
    "            sample_embeddings, real_images = next(iter(dataloader))\n",
    "            real_count = min(40, real_images.shape[0])  # Ensure we don't slice out of bounds\n",
    "\n",
    "            real_images = real_images[:real_count].cpu()\n",
    "            sample_embeddings = sample_embeddings[:real_count].to(device)\n",
    "\n",
    "            generated_fixed = generator(fixed_noise[:real_count], sample_embeddings).cpu()\n",
    "            generated_random = generator(random_noise[:real_count], sample_embeddings).cpu()\n",
    "\n",
    "        # Concatenate images for visualization: Real | Fixed | Random\n",
    "        all_images = torch.cat((real_images, generated_fixed, generated_random), dim=0)\n",
    "        \n",
    "        # Display Grid (Real on top, Fixed in middle, Random at bottom)\n",
    "        grid = make_grid(all_images, nrow=10, normalize=True)\n",
    "\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(np.transpose(grid.numpy(), (1, 2, 0)))\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Real (Top) | Fixed Noise (Middle) | Random Noise (Bottom) - Epoch {epoch+1}')\n",
    "        plt.show()\n",
    "\n",
    "        # Plot Loss Chart\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(range(1, len(d_real_conf) + 1), d_real_conf, label=\"D Real Confidence\")\n",
    "        plt.plot(range(1, len(d_fake_conf) + 1), d_fake_conf, label=\"D Fake Confidence\")\n",
    "        plt.plot(range(1, len(g_losses) + 1), g_losses, label=\"G Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Training Loss Over Time\")\n",
    "        plt.show()"
   ],
   "id": "eb4182680bf8ba32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Device Configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Using NVIDIA GPU (CUDA)')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print('Using Mac GPU (MPS)')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using CPU')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_parquet('../data/processed_openmoji_dataset.parquet')\n",
    "df[\"combined_embedding\"] = df[\"combined_embedding\"].apply(lambda x: np.array(x, dtype=np.float32))\n",
    "\n",
    "class EmojiDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embedding = torch.tensor(self.dataframe.iloc[idx]['combined_embedding']).float()\n",
    "        image_tensor = torch.load(self.dataframe.iloc[idx]['image_path']).float()\n",
    "        return embedding, image_tensor\n",
    "\n",
    "dataset = EmojiDataset(df)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, embedding_dim, image_channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(noise_dim + embedding_dim, 512, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, image_channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, noise, embed):\n",
    "        x = torch.cat((noise, embed), dim=1).unsqueeze(2).unsqueeze(3)\n",
    "        return self.main(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, embedding_dim, image_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.embed_layer = nn.Linear(embedding_dim, 32 * 32)  # Convert embedding to a 32x32 map\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(image_channels + 1, 128, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "            # nn.BatchNorm2d(512),\n",
    "            # nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1, 8, 1, 0, bias=False), # Adjusted 4 to 8\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, img, embed):\n",
    "        embed = self.embed_layer(embed)  # Convert embedding to (batch, 1024)\n",
    "        embed = embed.view(embed.shape[0], 1, 32, 32)  # Reshape to (batch, 1, 32, 32)\n",
    "        \n",
    "        x = torch.cat((img, embed), dim=1)  # Concatenate along channel dimension\n",
    "        return self.main(x).view(-1, 1)\n",
    "\n",
    "noise_dim = 100\n",
    "embedding_dim = len(df['combined_embedding'][0])\n",
    "generator = Generator(noise_dim, embedding_dim).to(device)\n",
    "discriminator = Discriminator(embedding_dim).to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=5e-5, betas=(0.5, 0.999)) # Lower LR for D\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "\n",
    "d_losses, g_losses, d_fake_conf, d_real_conf = [], [], [], []\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True)\n",
    "    for combined_embeddings, real_images in progress_bar:\n",
    "        combined_embeddings = combined_embeddings.to(device)\n",
    "        real_images = real_images.to(device)\n",
    "        batch_size = real_images.size(0)\n",
    "        \n",
    "        real_labels = torch.full((batch_size, 1), 0.9, dtype=torch.float, device=device)\n",
    "        fake_labels = torch.full((batch_size, 1), 0.1, dtype=torch.float, device=device)\n",
    "        \n",
    "        # Train Discriminator\n",
    "        noise = torch.randn(batch_size, noise_dim, device=device)\n",
    "        fake_images = generator(noise, combined_embeddings)\n",
    "        \n",
    "        real_outputs = discriminator(real_images, combined_embeddings)\n",
    "        fake_outputs = discriminator(fake_images.detach(), combined_embeddings)\n",
    "        \n",
    "        # Discriminator confidence\n",
    "        real_confidence = real_outputs.mean().item()\n",
    "        fake_confidence = fake_outputs.mean().item()\n",
    "        \n",
    "        d_loss = criterion(real_outputs, real_labels) + criterion(fake_outputs, fake_labels)\n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # Train Generator\n",
    "        fake_outputs = discriminator(fake_images, combined_embeddings)\n",
    "        g_loss = criterion(fake_outputs, real_labels)\n",
    "        \n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "    d_losses.append(d_loss.item())\n",
    "    g_losses.append(g_loss.item())\n",
    "    d_fake_conf.append(fake_confidence)\n",
    "    d_real_conf.append(real_confidence)\n",
    "    tqdm.write(f\"Epoch [{epoch+1}/{num_epochs}] | D Real Confidence: {real_confidence:.4f} | D Fake Confidence: {fake_confidence:.4f} | G Loss: {g_loss.item():.4f}\")\n",
    "        \n",
    "    if (epoch + 1) % 25 == 0:\n",
    "        with torch.no_grad():\n",
    "            noise = torch.randn(40, noise_dim).to(device)\n",
    "            \n",
    "            # Get real embeddings and real images from the dataset\n",
    "            sample_embeddings, _ = next(iter(dataloader))  \n",
    "            sample_embeddings = sample_embeddings[:40].to(device)  # Take the first 64 embeddings\n",
    "            real_images = real_images[:40].cpu()  # Take the first 64 real images\n",
    "            \n",
    "            # Generate emojis\n",
    "            generated_images = generator(noise, sample_embeddings).cpu()\n",
    "        \n",
    "        # Concatenate real and generated images\n",
    "        all_images = torch.cat((real_images, generated_images), dim=0)  # Stack along batch dimension\n",
    "        \n",
    "        # Display Grid (Real on top, Generated below)\n",
    "        grid = make_grid(all_images, nrow=8, normalize=True)\n",
    "        \n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(np.transpose(grid.numpy(), (1, 2, 0)))\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Real (Top) vs. Generated (Bottom) Emojis - Epoch {epoch+1}')\n",
    "        plt.show()\n",
    "\n",
    "        # Plot Loss Chart\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(range(1, len(d_real_conf) + 1), d_real_conf, label=\"D Real Confidence\")\n",
    "        plt.plot(range(1, len(d_fake_conf) + 1), d_fake_conf, label=\"D Fake Confidence\")\n",
    "        plt.plot(range(1, len(g_losses) + 1), g_losses, label=\"G Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Training Loss Over Time\")\n",
    "        plt.show()\n"
   ],
   "id": "9f7747244f142e73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Embedding Consistency Test (Does the generator map embeddings to stable outputs?)",
   "id": "52d8ed4416ad6c56"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T04:25:44.773995Z",
     "start_time": "2025-03-01T04:25:44.773472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    sample_embeddings, real_images = next(iter(dataloader))  \n",
    "    sample_embeddings = sample_embeddings[:40].to(device)  \n",
    "    real_images = real_images[:40].cpu()  \n",
    "\n",
    "    # Generate emojis WITHOUT noise\n",
    "    noise = torch.zeros((40, noise_dim)).to(device)  # Zero noise for consistency\n",
    "    generated_images = generator(noise, sample_embeddings).cpu()\n",
    "\n",
    "# Visualize\n",
    "grid = make_grid(torch.cat((real_images, generated_images), dim=0), nrow=8, normalize=True)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(np.transpose(grid.numpy(), (1, 2, 0)))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real (Top) vs. Generated (Bottom) - No Noise Test\")\n",
    "plt.show()"
   ],
   "id": "60eb1efd21dbff73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Noise Sensitivity Test (Does noise introduce meaningful variations?)",
   "id": "65b6e21ff31c72d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    sample_embeddings, real_images = next(iter(dataloader))  \n",
    "    sample_embeddings = sample_embeddings[:40].to(device)  \n",
    "\n",
    "    # Generate with different noise inputs\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(8, 12))  # 3 different noise tests\n",
    "    for i in range(3):  \n",
    "        noise = torch.randn(40, noise_dim).to(device)  # Different noise each time\n",
    "        generated_images = generator(noise, sample_embeddings).cpu()\n",
    "\n",
    "        grid = make_grid(generated_images, nrow=8, normalize=True)\n",
    "        axes[i].imshow(np.transpose(grid.numpy(), (1, 2, 0)))\n",
    "        axes[i].axis(\"off\")\n",
    "        axes[i].set_title(f\"Generated Emojis - Noise Sample {i+1}\")\n",
    "\n",
    "    plt.show()"
   ],
   "id": "a2e3ca288fff1472",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Latent Space Interpolation Test (Are embeddings smoothly mapped to outputs?)",
   "id": "f96ed6a532703dc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    # Pick two embeddings\n",
    "    sample_embeddings, _ = next(iter(dataloader))  \n",
    "    e1, e2 = sample_embeddings[:1].to(device), sample_embeddings[1:2].to(device)  # (1, embedding_dim)\n",
    "\n",
    "    # Interpolate embeddings (10 steps)\n",
    "    alpha_vals = torch.linspace(0, 1, steps=10).to(device)  # 10 interpolation points\n",
    "    interpolated_embeddings = torch.stack([torch.lerp(e1, e2, alpha) for alpha in alpha_vals]).squeeze(1)\n",
    "\n",
    "    # Generate images for interpolated embeddings\n",
    "    noise = torch.zeros((10, noise_dim)).to(device)  # No noise for consistency\n",
    "    generated_images = generator(noise, interpolated_embeddings).cpu()\n",
    "\n",
    "# Visualize\n",
    "grid = make_grid(generated_images, nrow=10, normalize=True)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(np.transpose(grid.numpy(), (1, 2, 0)))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Latent Space Interpolation - Smooth Transition?\")\n",
    "plt.show()"
   ],
   "id": "df471331f6bfc00b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-28T17:51:09.914441Z",
     "start_time": "2025-02-28T17:51:09.911596Z"
    }
   },
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# Use GPU if available\n",
    "\"\"\"\n",
    "Reference: https://pytorch.org/get-started/locally/\n",
    "\"\"\"\n",
    "\n",
    "# Device Configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Using NVIDIA GPU (CUDA)')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print('Using Mac GPU (MPS)')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using CPU')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Mac GPU (MPS)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T17:51:10.201537Z",
     "start_time": "2025-02-28T17:51:10.173678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the dataset\n",
    "df = pd.read_parquet('../data/processed_openmoji_dataset.parquet')\n",
    "\n",
    "# Convert combined_embedding to float32 numpy array\n",
    "df[\"combined_embedding\"] = df[\"combined_embedding\"].apply(lambda x: np.array(x, dtype=np.float32))\n",
    "\n",
    "# Custom Dataset Class\n",
    "class EmojiDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embedding = torch.tensor(self.dataframe.iloc[idx]['combined_embedding']).float()\n",
    "        image_tensor = torch.load(self.dataframe.iloc[idx]['image_path']).float()\n",
    "        \n",
    "        return embedding, image_tensor\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = EmojiDataset(df)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ],
   "id": "5e02bc271e15d0ad",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T17:51:10.207387Z",
     "start_time": "2025-02-28T17:51:10.204666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, embedding_dim, image_channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(noise_dim + embedding_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, image_channels * 32 * 32),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, embed):\n",
    "        x = torch.cat((noise, embed), dim=1)\n",
    "        x = self.model(x)\n",
    "        return x.view(x.size(0), 3, 32, 32)"
   ],
   "id": "8b04ff579ea1afc5",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T17:51:10.210629Z",
     "start_time": "2025-02-28T17:51:10.208217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, embedding_dim, image_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(image_channels * 32 * 32 + embedding_dim, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img, embed):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        x = torch.cat((img_flat, embed), dim=1)\n",
    "        return self.model(x)"
   ],
   "id": "551a8a98663e3908",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T17:51:10.384440Z",
     "start_time": "2025-02-28T17:51:10.215327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "noise_dim = 100  # Dimension of noise vector\n",
    "embedding_dim = len(df['combined_embedding'][0])  # Length of SBERT embedding\n",
    "\n",
    "# Initialize Generator and Discriminator\n",
    "generator = Generator(noise_dim, embedding_dim).to(device)\n",
    "discriminator = Discriminator(embedding_dim).to(device)\n",
    "\n",
    "# Loss and Optimizers\n",
    "criterion = nn.BCELoss()\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.5, 0.999))"
   ],
   "id": "f3e72f08c9bf1d1b",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_epochs = 500\n",
    "d_losses, g_losses, d_fake_losses, d_real_losses = [], [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True)\n",
    "    \n",
    "    for combined_embeddings, real_images in progress_bar:\n",
    "        combined_embeddings = combined_embeddings.to(device)\n",
    "        real_images = real_images.to(device)\n",
    "        \n",
    "        # Train Discriminator\n",
    "        noise = torch.randn(real_images.size(0), noise_dim).to(device)\n",
    "        fake_images = generator(noise, combined_embeddings)\n",
    "        \n",
    "        real_labels = torch.ones(real_images.size(0), 1).to(device)\n",
    "        fake_labels = torch.zeros(real_images.size(0), 1).to(device)\n",
    "        \n",
    "        real_outputs = discriminator(real_images, combined_embeddings)\n",
    "        fake_outputs = discriminator(fake_images.detach(), combined_embeddings)\n",
    "        \n",
    "        # Discriminator confidence\n",
    "        real_confidence = real_outputs.mean().item()\n",
    "        fake_confidence = fake_outputs.mean().item()\n",
    "        \n",
    "        d_loss_real = criterion(real_outputs, real_labels)\n",
    "        d_loss_fake = criterion(fake_outputs, fake_labels)\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        \n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # Train Generator\n",
    "        fake_outputs = discriminator(fake_images, combined_embeddings)\n",
    "        g_loss = criterion(fake_outputs, real_labels)\n",
    "        \n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "    \n",
    "    # Store loss values\n",
    "    d_losses.append(d_loss.item())\n",
    "    g_losses.append(g_loss.item())\n",
    "    d_fake_losses.append(fake_confidence)\n",
    "    d_real_losses.append(real_confidence)\n",
    "\n",
    "    # Using tqdm.write() for clean logging\n",
    "    tqdm.write(f\"Epoch [{epoch+1}/{num_epochs}] | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "    # Display 64 generated emojis and loss chart every 25 epochs\n",
    "    if (epoch + 1) % 25 == 0:\n",
    "        with torch.no_grad():\n",
    "            noise = torch.randn(40, noise_dim).to(device)\n",
    "            \n",
    "            # Get real embeddings and real images from the dataset\n",
    "            sample_embeddings, _ = next(iter(dataloader))  \n",
    "            sample_embeddings = sample_embeddings[:40].to(device)  # Take the first 64 embeddings\n",
    "            real_images = real_images[:40].cpu()  # Take the first 64 real images\n",
    "            \n",
    "            # Generate emojis\n",
    "            generated_images = generator(noise, sample_embeddings).cpu()\n",
    "        \n",
    "        # Concatenate real and generated images\n",
    "        all_images = torch.cat((real_images, generated_images), dim=0)  # Stack along batch dimension\n",
    "        \n",
    "        # Display Grid (Real on top, Generated below)\n",
    "        grid = make_grid(all_images, nrow=8, normalize=True)\n",
    "        \n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(np.transpose(grid.numpy(), (1, 2, 0)))\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Real (Top) vs. Generated (Bottom) Emojis - Epoch {epoch+1}')\n",
    "        plt.show()\n",
    "\n",
    "        # Plot Loss Chart\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(range(1, len(d_real_losses) + 1), d_real_losses, label=\"D Loss Real\")\n",
    "        plt.plot(range(1, len(d_fake_losses) + 1), d_fake_losses, label=\"D Loss Fake\")\n",
    "        plt.plot(range(1, len(g_losses) + 1), g_losses, label=\"G Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Training Loss Over Time\")\n",
    "        plt.show()"
   ],
   "id": "6d5b50299e06071",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T19:30:23.022693Z",
     "start_time": "2025-02-28T19:30:22.000586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loading the same SBERT model\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Sample prompt text\n",
    "sample_text = \"happy face with heart eyes\"\n",
    "sample_embedding = sbert_model.encode([sample_text], convert_to_tensor=True)\n",
    "sample_embedding = sample_embedding.float().to(device)\n",
    "\n",
    "def generate_emoji(generator, embedding, num_samples=1):\n",
    "    generator.eval()  # Set to evaluation mode\n",
    "    noise = torch.randn(num_samples, noise_dim).to(device)  # Random noise for generation\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_images = generator(noise, embedding)\n",
    "    generated_images = generated_images.cpu() * 0.5 + 0.5  # Denormalize\n",
    "\n",
    "    return generated_images\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_generated_emoji(generated_images):\n",
    "    num_samples = generated_images.size(0)\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(5 * num_samples, 5))\n",
    "    if num_samples == 1:\n",
    "        axes = [axes]  # Ensure axes is always iterable\n",
    "\n",
    "    for i, img in enumerate(generated_images):\n",
    "        axes[i].imshow(img.permute(1, 2, 0).numpy())  # Convert from Tensor format\n",
    "        axes[i].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Generate and visualize the emoji\n",
    "generated_emoji = generate_emoji(generator, sample_embedding)\n",
    "visualize_generated_emoji(generated_emoji)"
   ],
   "id": "da33a504910092c1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4fff9777c67381d7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
