{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from diffusers import UNet2DConditionModel, AutoencoderKL\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import torch\n",
    "from diffusers import UNet2DConditionModel\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Prompt(GPT 4o): Fine tunning Stable Difussion model using Lora .\n",
    "#  Enable cuDNN optimization\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\")  # Optimize matmul precision\n",
    "\n",
    "#  Ensure PyTorch uses GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "#  Free GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "#  Dataset Class\n",
    "class EmojiDataset(Dataset):\n",
    "    def __init__(self, parquet_file):\n",
    "        self.data = pd.read_parquet(parquet_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.data.iloc[idx][\"image_path\"]\n",
    "        image_tensor = torch.load(image_path).float() / 127.5 - 1  # Normalize to [-1,1]\n",
    "        text_embedding = torch.tensor(self.data.iloc[idx][\"combined_embedding\"], dtype=torch.float32)\n",
    "        return image_tensor, text_embedding\n",
    "#  Load Dataset\n",
    "parquet_file = \"../data/processed_emoji_dataset.parquet\"\n",
    "dataset = EmojiDataset(parquet_file)\n",
    "batch_size = 4  # Reduce batch size to free memory\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "#  Load Stable Diffusion VAE and UNet to GPU\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/stable-diffusion-2\", subfolder=\"vae\").to(device, dtype=torch.float16)\n",
    "unet = UNet2DConditionModel.from_pretrained(\"stabilityai/stable-diffusion-2\", subfolder=\"unet\").to(device, dtype=torch.float16)\n",
    "print(unet.config)  # Check image size\n",
    "#  Apply LoRA to UNet\n",
    "lora_config = LoraConfig(\n",
    "    r=4,  # LoRA rank\n",
    "    lora_alpha=16,  # Scaling factor\n",
    "    target_modules=[\"to_q\", \"to_k\", \"to_v\"],  # Apply LoRA to attention layers\n",
    "    lora_dropout=0.05,  # Dropout for regularization\n",
    "    bias=\"none\"\n",
    "    )\n",
    "\n",
    "unet = get_peft_model(unet, lora_config)\n",
    "unet.print_trainable_parameters()  #  Print trainable parameters (should be very small)\n",
    "# Enable memory optimization\n",
    "unet.enable_gradient_checkpointing()\n",
    "\n",
    "\n",
    "# Embedding Projector (CLIP 512 → UNet 1024)\n",
    "class EmbeddingProjector(nn.Module):\n",
    "    def __init__(self, input_dim=512, output_dim=1024):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "embedding_projector = EmbeddingProjector().to(device, dtype=torch.float16)\n",
    "# Freeze VAE\n",
    "for param in vae.parameters():\n",
    "    param.requires_grad = False  \n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, unet.parameters()), lr=1e-4)\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 20\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for images, embeddings in progress_bar:\n",
    "        images = images.to(device, dtype=torch.float16, non_blocking=True)\n",
    "        embeddings = embeddings.to(device, dtype=torch.float16, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Project CLIP embeddings\n",
    "        with torch.no_grad():\n",
    "            projected_embeddings = embedding_projector(embeddings).unsqueeze(1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            latents = torch.utils.checkpoint.checkpoint(\n",
    "                lambda x: vae.encode(x).latent_dist.sample() * 0.18215, images, use_reentrant=False\n",
    "            )\n",
    "        # Convert latents to bfloat16 to save memory\n",
    "        latents = latents.to(torch.bfloat16)\n",
    "\n",
    "        # Generate noise\n",
    "        noise = torch.randn_like(latents, dtype=torch.bfloat16)\n",
    "        timesteps = torch.randint(0, 1000, (latents.shape[0],), device=device).long()\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.amp.autocast(\"cuda\"):\n",
    "            noise_pred = unet(latents, timesteps, encoder_hidden_states=projected_embeddings).sample\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "\n",
    "        # Backpropagation\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)  # Avoid NaN issues\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "    losses.append(avg_epoch_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "# Save Model\n",
    "torch.save({\n",
    "    \"unet\": unet.state_dict(),\n",
    "    \"embedding_projector\": embedding_projector.state_dict()\n",
    "}, \"emoji_generator.pth\")\n",
    "print(\"Model saved successfully!\")\n",
    "\n",
    "# Save LoRA Weights\n",
    "unet.save_pretrained(\"lora_emoji_unet\")\n",
    "torch.save(embedding_projector.state_dict(), \"embedding_projector.pth\")\n",
    "print(\"LoRA adapters saved successfully!\")\n",
    "\n",
    "# Plot Loss Curve\n",
    "plt.plot(range(1, num_epochs + 1), losses, marker=\"o\", linestyle=\"-\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Free GPU memory before inference\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "# Load Trained LoRA Model\n",
    "unet = UNet2DConditionModel.from_pretrained(\"stabilityai/stable-diffusion-2\", subfolder=\"unet\").to(device)\n",
    "unet = PeftModel.from_pretrained(unet, \"lora_emoji_unet\").to(device).to(torch.float16)\n",
    "embedding_projector.load_state_dict(torch.load(\"embedding_projector.pth\"))\n",
    "unet.eval()\n",
    "embedding_projector.eval()\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Assuming the necessary models are already loaded\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "text_encoder = text_encoder.to(device)\n",
    "\n",
    "# Define your text prompt\n",
    "text_description = \"Green\"  # This is where we specify \"dog\"\n",
    "\n",
    "# Tokenize and encode the text prompt\n",
    "tokens = tokenizer(text_description, return_tensors=\"pt\").to(device)\n",
    "text_embedding = text_encoder(**tokens).last_hidden_state.mean(dim=1)  # Aggregate token embeddings\n",
    "text_embedding = text_embedding.to(torch.float16)  # Ensure it's float16 for compatibility with UNet\n",
    "\n",
    "# Project the embedding to match UNet’s expected format\n",
    "projected_embedding = embedding_projector(text_embedding).unsqueeze(0).to(torch.float16)  # Ensure float16\n",
    "\n",
    "# Generate noise in latent space (fixed size 96x96)\n",
    "latents = torch.randn(1, 4, 96, 96).to(device).to(torch.float16)  # Ensure latents are in float16\n",
    "timesteps = torch.tensor([500], device=device).long()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Generate noise in latent space (smaller size)\n",
    "latents = torch.randn(1, 4, 64, 64, device=device, dtype=torch.float16)\n",
    "timesteps = torch.tensor([500], device=device).long()\n",
    "\n",
    "# Generate Emoji\n",
    "with torch.no_grad():\n",
    "    denoised_latents = unet(latents, timesteps, encoder_hidden_states=projected_embedding).sample\n",
    "\n",
    "# Move to CPU and Decode\n",
    "denoised_latents = denoised_latents / 0.18215\n",
    "\n",
    "# vae.to(\"cpu\")\n",
    "# with torch.no_grad():\n",
    "#     decoded_image = vae.decode(denoised_latents.to(\"cpu\")).sample.to(\"cpu\")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    decoded_image = vae.decode(denoised_latents).sample\n",
    "# Post-process Image\n",
    "decoded_image = (decoded_image.clamp(-1, 1) + 1) / 2\n",
    "decoded_image = decoded_image.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "decoded_image = (decoded_image * 255).astype(np.uint8)\n",
    "emoji_image = Image.fromarray(decoded_image)\n",
    "\n",
    "# Display the image\n",
    "emoji_image.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
