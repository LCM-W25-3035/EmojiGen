{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from diffusers import UNet2DConditionModel, AutoencoderKL\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import torch\n",
    "from diffusers import UNet2DConditionModel\n",
    "from diffusers.loaders import AttnProcsLayers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt(GPT 4o): Fine tunning Stable Difussion model using Lora ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Enable cuDNN optimization\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\")  # Optimize matmul precision\n",
    "\n",
    "#  Ensure PyTorch uses GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "#  Free GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Dataset Class\n",
    "class EmojiDataset(Dataset):\n",
    "    def __init__(self, parquet_file):\n",
    "        self.data = pd.read_parquet(parquet_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.data.iloc[idx][\"image_path\"]\n",
    "        image_tensor = torch.load(image_path).float() / 127.5 - 1  # Normalize to [-1,1]\n",
    "        text_embedding = torch.tensor(self.data.iloc[idx][\"combined_embedding\"], dtype=torch.float32)\n",
    "        return image_tensor, text_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load Dataset\n",
    "parquet_file = \"../data/processed_emoji_dataset.parquet\"\n",
    "dataset = EmojiDataset(parquet_file)\n",
    "batch_size = 8  # Reduce batch size to free memory\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load Stable Diffusion VAE and UNet to GPU\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/stable-diffusion-2\", subfolder=\"vae\").to(device, dtype=torch.float16)\n",
    "unet = UNet2DConditionModel.from_pretrained(\"stabilityai/stable-diffusion-2\", subfolder=\"unet\").to(device, dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unet.config)  # Check image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Apply LoRA to UNet\n",
    "lora_config = LoraConfig(\n",
    "    r=2,  # LoRA rank\n",
    "    lora_alpha=16,  # Scaling factor\n",
    "    target_modules=[\"to_q\", \"to_k\", \"to_v\"],  # Apply LoRA to attention layers\n",
    "    lora_dropout=0.05,  # Dropout for regularization\n",
    "    bias=\"none\"\n",
    "    )\n",
    "\n",
    "unet = get_peft_model(unet, lora_config)\n",
    "unet.print_trainable_parameters()  #  Print trainable parameters (should be very small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable memory optimization\n",
    "unet.enable_gradient_checkpointing()\n",
    "\n",
    "\n",
    "# Embedding Projector (CLIP 512 → UNet 1024)\n",
    "class EmbeddingProjector(nn.Module):\n",
    "    def __init__(self, input_dim=512, output_dim=1024):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "embedding_projector = EmbeddingProjector().to(device, dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze VAE\n",
    "for param in vae.parameters():\n",
    "    param.requires_grad = False  \n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, unet.parameters()), lr=1e-4)\n",
    "scaler = torch.amp.GradScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "num_epochs = 2\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for images, embeddings in progress_bar:\n",
    "        images = images.to(device, dtype=torch.float16, non_blocking=True)\n",
    "        embeddings = embeddings.to(device, dtype=torch.float16, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Project CLIP embeddings\n",
    "        with torch.no_grad():\n",
    "            projected_embeddings = embedding_projector(embeddings).unsqueeze(1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            latents = torch.utils.checkpoint.checkpoint(\n",
    "                lambda x: vae.encode(x).latent_dist.sample() * 0.18215, images, use_reentrant=False\n",
    "            )\n",
    "        # Convert latents to bfloat16 to save memory\n",
    "        latents = latents.to(torch.bfloat16)\n",
    "\n",
    "        # Generate noise\n",
    "        noise = torch.randn_like(latents, dtype=torch.bfloat16)\n",
    "        timesteps = torch.randint(0, 1000, (latents.shape[0],), device=device).long()\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.amp.autocast(\"cuda\"):\n",
    "            noise_pred = unet(latents, timesteps, encoder_hidden_states=projected_embeddings).sample\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "\n",
    "        # Backpropagation\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)  # Avoid NaN issues\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "    losses.append(avg_epoch_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "torch.save({\n",
    "    \"unet\": unet.state_dict(),\n",
    "    \"embedding_projector\": embedding_projector.state_dict()\n",
    "}, \"emoji_generator.pth\")\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA Weights\n",
    "unet.save_pretrained(\"lora_emoji_unet\")\n",
    "torch.save(embedding_projector.state_dict(), \"embedding_projector.pth\")\n",
    "print(\"LoRA adapters saved successfully!\")\n",
    "\n",
    "# Plot Loss Curve\n",
    "plt.plot(range(1, num_epochs + 1), losses, marker=\"o\", linestyle=\"-\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free GPU memory before inference\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Trained LoRA Model\n",
    "unet = UNet2DConditionModel.from_pretrained(\"stabilityai/stable-diffusion-2\", subfolder=\"unet\").to(device)\n",
    "unet = PeftModel.from_pretrained(unet, \"lora_emoji_unet\").to(device)\n",
    "embedding_projector.load_state_dict(torch.load(\"embedding_projector.pth\"))\n",
    "unet.eval()\n",
    "embedding_projector.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Emoji from Text Prompt\n",
    "text_description = \"dog\"\n",
    "embedding_np = dataset.data.loc[dataset.data[\"text\"] == text_description, \"combined_embedding\"].values[0]\n",
    "text_embedding = torch.tensor(embedding_np, dtype=torch.float32).to(device)\n",
    "projected_embedding = embedding_projector(text_embedding).unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate noise in latent space (fixed size 96x96)\n",
    "latents = torch.randn(1, 4, 96, 96).to(device)\n",
    "timesteps = torch.tensor([500], device=device).long()\n",
    "\n",
    "# Generate Emoji\n",
    "with torch.no_grad():\n",
    "    denoised_latents = unet(latents, timesteps, encoder_hidden_states=projected_embedding).sample\n",
    "\n",
    "# Scale and Decode Image\n",
    "denoised_latents = denoised_latents / 0.18215\n",
    "with torch.no_grad():\n",
    "    decoded_image = vae.decode(denoised_latents).sample\n",
    "\n",
    "# Post-process Image\n",
    "decoded_image = (decoded_image.clamp(-1, 1) + 1) / 2\n",
    "decoded_image = decoded_image.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "decoded_image = (decoded_image * 255).astype(np.uint8)\n",
    "emoji_image = Image.fromarray(decoded_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Generated Emoji\n",
    "plt.imshow(emoji_image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from diffusers import UNet2DConditionModel, AutoencoderKL\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable cuDNN optimization\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\")  # Optimize matmul precision\n",
    "\n",
    "# Free GPU memory\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dataset class using precomputed CLIP embeddings\n",
    "class EmojiDataset(Dataset):\n",
    "    def __init__(self, parquet_file):\n",
    "        self.data = pd.read_parquet(parquet_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.data.iloc[idx][\"image_path\"]\n",
    "        image_tensor = torch.load(image_path).float() / 127.5 - 1  # Normalize to [-1,1]\n",
    "        text_embedding = torch.tensor(self.data.iloc[idx][\"combined_embedding\"], dtype=torch.float32)\n",
    "        return image_tensor, text_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "parquet_file = \"../data/processed_emoji_dataset.parquet\"\n",
    "dataset = EmojiDataset(parquet_file)\n",
    "batch_size = 4  # Reduce batch size to free memory\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Load Stable Diffusion VAE and UNet\n",
    "device = \"cuda\"\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/stable-diffusion-2\", subfolder=\"vae\").to(device)\n",
    "unet = UNet2DConditionModel.from_pretrained(\"stabilityai/stable-diffusion-2\", subfolder=\"unet\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Enable memory optimization\n",
    "unet.enable_gradient_checkpointing()\n",
    "\n",
    "# Try `torch.compile` for speed boost (PyTorch 2.0+)\n",
    "try:\n",
    "    unet = torch.compile(unet)\n",
    "except:\n",
    "    print(\"torch.compile is not available. Continuing without it.\")\n",
    "\n",
    "# Embedding Projector\n",
    "class EmbeddingProjector(nn.Module):\n",
    "    def __init__(self, input_dim=512, output_dim=1024):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_projector = EmbeddingProjector().to(device)\n",
    "\n",
    "# Freeze all UNet layers except the last few\n",
    "for param in unet.parameters():\n",
    "    param.requires_grad = False  \n",
    "for layer in list(unet.children())[-1:]:  \n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True  \n",
    "\n",
    "# Freeze VAE\n",
    "for param in vae.parameters():\n",
    "    param.requires_grad = False  \n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, unet.parameters()), lr=1e-4)\n",
    "scaler = torch.amp.GradScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training Loop\n",
    "num_epochs = 20\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for images, embeddings in progress_bar:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        embeddings = embeddings.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Project CLIP embeddings\n",
    "        with torch.no_grad():\n",
    "            projected_embeddings = embedding_projector(embeddings).unsqueeze(1)\n",
    "\n",
    "        # Encode images into latent space with checkpointing\n",
    "        with torch.no_grad():\n",
    "            latents = torch.utils.checkpoint.checkpoint(lambda x: vae.encode(x).latent_dist.sample() * 0.18215, images)\n",
    "        \n",
    "        # Convert latents to bfloat16 to save memory\n",
    "        latents = latents.to(torch.bfloat16)\n",
    "\n",
    "        # Generate noise\n",
    "        noise = torch.randn_like(latents, dtype=torch.bfloat16)\n",
    "        timesteps = torch.randint(0, 1000, (latents.shape[0],), device=device).long()\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.amp.autocast(\"cuda\"):\n",
    "            noise_pred = unet(latents, timesteps, encoder_hidden_states=projected_embeddings).sample\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "\n",
    "        # Backpropagation\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)  # Avoid NaN issues\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "    losses.append(avg_epoch_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "torch.save({\n",
    "    \"unet\": unet.state_dict(),\n",
    "    \"embedding_projector\": embedding_projector.state_dict()\n",
    "}, \"emoji_generator.pth\")\n",
    "print(\"Model saved successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss Curve\n",
    "plt.plot(range(1, num_epochs + 1), losses, marker=\"o\", linestyle=\"-\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from diffusers import UNet2DConditionModel, AutoencoderKL\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class using precomputed CLIP embeddings\n",
    "class EmojiDataset(Dataset):\n",
    "    def __init__(self, parquet_file, transform=None):\n",
    "        self.data = pd.read_parquet(parquet_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.data.iloc[idx][\"image_path\"]\n",
    "        image_tensor = torch.load(image_path)  # Shape: (3, 64, 64)\n",
    "        image_tensor = image_tensor.float() / 255.0 * 2 - 1  # Normalize to [-1,1]\n",
    "\n",
    "        text_embedding = torch.tensor(self.data.iloc[idx][\"combined_embedding\"], dtype=torch.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image_tensor)\n",
    "\n",
    "        return image_tensor, text_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "parquet_file = \"../data/processed_emoji_dataset.parquet\"\n",
    "dataset = EmojiDataset(parquet_file)\n",
    "batch_size = 8  # Adjust batch size to avoid GPU memory overflow\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Stable Diffusion VAE and UNet\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/stable-diffusion-2\", subfolder=\"vae\").to(\"cuda\")\n",
    "unet = UNet2DConditionModel.from_pretrained(\"stabilityai/stable-diffusion-2\", subfolder=\"unet\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable memory optimization\n",
    "unet.enable_gradient_checkpointing()\n",
    "\n",
    "# Embedding Projector (CLIP 768 → UNet 768, if needed)\n",
    "class EmbeddingProjector(nn.Module):\n",
    "    def __init__(self, input_dim=512, output_dim=1024):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "embedding_projector = EmbeddingProjector().to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all UNet layers except the last few for fine-tuning\n",
    "for param in unet.parameters():\n",
    "    param.requires_grad = False  \n",
    "num_layers_to_train = 1  # Adjust as needed\n",
    "for module in list(unet.children())[-num_layers_to_train:]:  \n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = True  \n",
    "\n",
    "# Freeze VAE\n",
    "for param in vae.parameters():\n",
    "    param.requires_grad = False  \n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, unet.parameters()), lr=1e-4)\n",
    "scaler = torch.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "num_epochs = 20\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for images, embeddings in progress_bar:\n",
    "        images, embeddings = images.to(\"cuda\"), embeddings.to(\"cuda\")\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Project CLIP embeddings\n",
    "        with torch.no_grad():\n",
    "            projected_embeddings = embedding_projector(embeddings).unsqueeze(1)\n",
    "\n",
    "        # Encode images into latent space\n",
    "        with torch.no_grad():\n",
    "            latents = vae.encode(images).latent_dist.sample()\n",
    "            latents = latents * 0.18215  # Scaling factor\n",
    "\n",
    "        # Generate noise\n",
    "        noise = torch.randn_like(latents)\n",
    "        timesteps = torch.randint(0, 1000, (latents.shape[0],), device=\"cuda\").long()\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.amp.autocast(\"cuda\"):\n",
    "            noise_pred = unet(latents, timesteps, encoder_hidden_states=projected_embeddings).sample\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "\n",
    "        # Backpropagation\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "    losses.append(avg_epoch_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "torch.save({\n",
    "    \"unet\": unet.state_dict(),\n",
    "    \"embedding_projector\": embedding_projector.state_dict()\n",
    "}, \"emoji_generator.pth\")\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss Curve\n",
    "plt.plot(range(1, num_epochs + 1), losses, marker=\"o\", linestyle=\"-\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "checkpoint = torch.load(\"emoji_generator.pth\")\n",
    "unet.load_state_dict(checkpoint[\"unet\"])\n",
    "embedding_projector.load_state_dict(checkpoint[\"embedding_projector\"])\n",
    "unet.eval()\n",
    "embedding_projector.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a text prompt\n",
    "text_description = \"dog\"\n",
    "embedding_np = dataset.data.loc[dataset.data[\"text\"] == text_description, \"combined_embedding\"].values[0]\n",
    "text_embedding = torch.tensor(embedding_np, dtype=torch.float32).to(\"cuda\")\n",
    "projected_embedding = embedding_projector(text_embedding).unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate noise in latent space (fix size to 96x96)\n",
    "latents = torch.randn(1, 4, 96, 96).to(\"cuda\")\n",
    "timesteps = torch.tensor([500], device=\"cuda\").long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate emoji\n",
    "with torch.no_grad():\n",
    "    denoised_latents = unet(latents, timesteps, encoder_hidden_states=projected_embedding).sample\n",
    "\n",
    "# Fix scaling factor before decoding\n",
    "denoised_latents = denoised_latents / 0.18215  \n",
    "\n",
    "with torch.no_grad():\n",
    "    decoded_image = vae.decode(denoised_latents).sample\n",
    "\n",
    "# Post-process image\n",
    "decoded_image = (decoded_image.clamp(-1, 1) + 1) / 2\n",
    "decoded_image = decoded_image.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "decoded_image = (decoded_image * 255).astype(np.uint8)\n",
    "emoji_image = Image.fromarray(decoded_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Image\n",
    "plt.imshow(emoji_image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
