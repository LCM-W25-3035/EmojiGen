{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from diffusers import UNet2DConditionModel, AutoencoderKL\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "from peft import LoraConfig, PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt(GPT 4o): Fine tunning Stable Difussion model using Lora ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enable cuDNN optimization for faster training\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# Ensure PyTorch uses GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Free GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "class EmojiDataset(Dataset):\n",
    "    def __init__(self, parquet_file):\n",
    "        self.data = pd.read_parquet(parquet_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.data.iloc[idx][\"image_path\"]\n",
    "        image_tensor = torch.load(image_path).float() / 127.5 - 1  # Normalize to [-1,1]\n",
    "        text_embedding = torch.tensor(self.data.iloc[idx][\"combined_embedding\"], dtype=torch.float32)\n",
    "        return image_tensor, text_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenDict({'sample_size': 64, 'in_channels': 4, 'out_channels': 4, 'center_input_sample': False, 'flip_sin_to_cos': True, 'freq_shift': 0, 'down_block_types': ['CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'DownBlock2D'], 'mid_block_type': 'UNetMidBlock2DCrossAttn', 'up_block_types': ['UpBlock2D', 'CrossAttnUpBlock2D', 'CrossAttnUpBlock2D', 'CrossAttnUpBlock2D'], 'only_cross_attention': False, 'block_out_channels': [320, 640, 1280, 1280], 'layers_per_block': 2, 'downsample_padding': 1, 'mid_block_scale_factor': 1, 'dropout': 0.0, 'act_fn': 'silu', 'norm_num_groups': 32, 'norm_eps': 1e-05, 'cross_attention_dim': 1024, 'transformer_layers_per_block': 1, 'reverse_transformer_layers_per_block': None, 'encoder_hid_dim': None, 'encoder_hid_dim_type': None, 'attention_head_dim': [5, 10, 20, 20], 'num_attention_heads': None, 'dual_cross_attention': False, 'use_linear_projection': True, 'class_embed_type': None, 'addition_embed_type': None, 'addition_time_embed_dim': None, 'num_class_embeds': None, 'upcast_attention': False, 'resnet_time_scale_shift': 'default', 'resnet_skip_time_act': False, 'resnet_out_scale_factor': 1.0, 'time_embedding_type': 'positional', 'time_embedding_dim': None, 'time_embedding_act_fn': None, 'timestep_post_act': None, 'time_cond_proj_dim': None, 'conv_in_kernel': 3, 'conv_out_kernel': 3, 'projection_class_embeddings_input_dim': None, 'attention_type': 'default', 'class_embeddings_concat': False, 'mid_block_only_cross_attention': None, 'cross_attention_norm': None, 'addition_embed_type_num_heads': 64, '_use_default_values': ['class_embeddings_concat', 'encoder_hid_dim', 'mid_block_only_cross_attention', 'resnet_time_scale_shift', 'class_embed_type', 'encoder_hid_dim_type', 'time_embedding_type', 'num_attention_heads', 'mid_block_type', 'transformer_layers_per_block', 'resnet_out_scale_factor', 'addition_embed_type', 'time_cond_proj_dim', 'num_class_embeds', 'reverse_transformer_layers_per_block', 'time_embedding_act_fn', 'conv_out_kernel', 'addition_embed_type_num_heads', 'time_embedding_dim', 'projection_class_embeddings_input_dim', 'resnet_skip_time_act', 'upcast_attention', 'cross_attention_norm', 'attention_type', 'timestep_post_act', 'only_cross_attention', 'conv_in_kernel', 'addition_time_embed_dim', 'dropout'], '_class_name': 'UNet2DConditionModel', '_diffusers_version': '0.8.0', '_name_or_path': 'stabilityai/stable-diffusion-2-base'})\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "parquet_file = \"../data/processed_emoji_dataset.parquet\"\n",
    "dataset = EmojiDataset(parquet_file)\n",
    "batch_size = 16  \n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Load Stable Diffusion 2 Base Model (Trained on 256x256 images)\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/stable-diffusion-2-base\", subfolder=\"vae\").to(device, dtype=torch.float16)\n",
    "unet = UNet2DConditionModel.from_pretrained(\"stabilityai/stable-diffusion-2-base\", subfolder=\"unet\").to(device, dtype=torch.float16)\n",
    "\n",
    "print(unet.config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 829,952 || all params: 866,740,676 || trainable%: 0.0958\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Apply LoRA to UNet\n",
    "lora_config = LoraConfig(\n",
    "    r=4,  # LoRA rank\n",
    "    lora_alpha=16,  # Scaling factor\n",
    "    target_modules=[\"to_q\", \"to_k\", \"to_v\", \"proj_out\", \"proj_in\"],  \n",
    "    lora_dropout=0.05,  # Dropout for regularization\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "unet = get_peft_model(unet, lora_config)\n",
    "unet.print_trainable_parameters()  # Print trainable parameters (should be very small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable memory optimization\n",
    "unet.enable_gradient_checkpointing()\n",
    "\n",
    "# Embedding Projector (CLIP 512 → UNet 768)\n",
    "class EmbeddingProjector(nn.Module):\n",
    "    def __init__(self, input_dim=512, output_dim=1024):  # Changed to 768\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "embedding_projector = EmbeddingProjector().to(device, dtype=torch.float16)\n",
    "\n",
    "# Freeze VAE\n",
    "for param in vae.parameters():\n",
    "    param.requires_grad = False  \n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, unet.parameters()), lr=0.001)\n",
    "scaler = torch.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  33%|███▎      | 56/170 [26:49<48:57, 25.77s/it, loss=1]      "
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "num_epochs = 5\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for images, embeddings in progress_bar:\n",
    "        images = images.to(device, dtype=torch.float16, non_blocking=True)\n",
    "        embeddings = embeddings.to(device, dtype=torch.float16, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Project CLIP embeddings\n",
    "        with torch.no_grad():\n",
    "            projected_embeddings = embedding_projector(embeddings).unsqueeze(1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            latents = vae.encode(images).latent_dist.sample() * 0.18215\n",
    "\n",
    "        # Convert latents to bfloat16 to save memory\n",
    "        latents = latents.to(torch.float16)\n",
    "\n",
    "        # Generate noise\n",
    "        noise = torch.randn_like(latents, dtype=torch.float16)\n",
    "        timesteps = torch.randint(0, 1000, (latents.shape[0],), device=device).long()\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.amp.autocast(\"cuda\"):\n",
    "            noise_pred = unet(latents, timesteps, encoder_hidden_states=projected_embeddings).sample\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "\n",
    "        # Backpropagation\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)  # Avoid NaN issues\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "    losses.append(avg_epoch_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save Model\n",
    "torch.save({\n",
    "    \"unet\": unet.state_dict(),\n",
    "    \"embedding_projector\": embedding_projector.state_dict()\n",
    "}, \"emoji_generator.pth\")\n",
    "print(\"Model saved successfully!\")\n",
    "\n",
    "# Save LoRA Weights\n",
    "unet.save_pretrained(\"lora_emoji_unet\")\n",
    "torch.save(embedding_projector.state_dict(), \"embedding_projector.pth\")\n",
    "print(\"LoRA adapters saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot Loss Curve\n",
    "plt.plot(range(1, num_epochs + 1), losses, marker=\"o\", linestyle=\"-\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = PeftModel.from_pretrained(unet, \"lora_emoji_unet\").to(device).to(torch.float16)\n",
    "embedding_projector.load_state_dict(torch.load(\"embedding_projector.pth\"))\n",
    "unet.eval()\n",
    "embedding_projector.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming the necessary models are already loaded\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "text_encoder = text_encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your text prompt\n",
    "text_description = \"htdjfgvg iukf iltog\"  # This is where we specify \"dog\"\n",
    "\n",
    "# Tokenize and encode the text prompt\n",
    "tokens = tokenizer(text_description, return_tensors=\"pt\").to(device)\n",
    "text_embedding = text_encoder(**tokens).last_hidden_state.mean(dim=1)  # Aggregate token embeddings\n",
    "text_embedding = text_embedding.to(torch.float16)  # Ensure it's float16 for compatibility with UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project the embedding to match UNet’s expected format\n",
    "projected_embedding = embedding_projector(text_embedding).unsqueeze(0).to(torch.float16)  # Ensure float16\n",
    "\n",
    "# Generate noise in latent space (fixed size 96x96)\n",
    "latents = torch.randn(1, 4, 96, 96).to(device).to(torch.float16)  # Ensure latents are in float16\n",
    "timesteps = torch.tensor([500], device=device).long()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Generate noise in latent space (smaller size)\n",
    "latents = torch.randn(1, 4, 64, 64, device=device, dtype=torch.float16)\n",
    "timesteps = torch.tensor([500], device=device).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Emoji\n",
    "with torch.no_grad():\n",
    "    denoised_latents = unet(latents, timesteps, encoder_hidden_states=projected_embedding).sample\n",
    "\n",
    "# Move to CPU and Decode\n",
    "denoised_latents = denoised_latents / 0.18215\n",
    "\n",
    "with torch.no_grad():\n",
    "    decoded_image = vae.decode(denoised_latents).sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Post-process Image\n",
    "decoded_image = (decoded_image.clamp(-1, 1) + 1) / 2\n",
    "decoded_image = decoded_image.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "decoded_image = (decoded_image * 255).astype(np.uint8)\n",
    "emoji_image = Image.fromarray(decoded_image)\n",
    "\n",
    "# Display the image\n",
    "emoji_image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
