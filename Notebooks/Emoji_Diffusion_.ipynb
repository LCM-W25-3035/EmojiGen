{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from diffusers import UNet2DConditionModel, AutoencoderKL\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Enable cuDNN optimization for faster training\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# Ensure PyTorch uses GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Free GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Dataset Class\n",
    "class EmojiDataset(Dataset):\n",
    "    def __init__(self, parquet_file):\n",
    "        self.data = pd.read_parquet(parquet_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.data.iloc[idx][\"image_path\"]\n",
    "        image_tensor = torch.load(image_path).float()  # No manual normalization\n",
    "        text_embedding = torch.tensor(self.data.iloc[idx][\"combined_embedding\"], dtype=torch.float32)\n",
    "        return image_tensor, text_embedding\n",
    "\n",
    "# Load Dataset\n",
    "parquet_file = \"../data/processed_emoji_dataset.parquet\"\n",
    "dataset = EmojiDataset(parquet_file)\n",
    "batch_size = 32  \n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 207,488 || all params: 866,118,212 || trainable%: 0.0240\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load Stable Diffusion 2 Base Model\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/stable-diffusion-2-base\", subfolder=\"vae\").to(device, dtype=torch.float16)\n",
    "unet = UNet2DConditionModel.from_pretrained(\"stabilityai/stable-diffusion-2-base\", subfolder=\"unet\").to(device, dtype=torch.float16)\n",
    "\n",
    "# Apply LoRA to UNet\n",
    "lora_config = LoraConfig(\n",
    "    r=1,  # LoRA rank\n",
    "    lora_alpha=8,  # Scaling factor\n",
    "    target_modules=[\"to_q\", \"to_k\", \"to_v\", \"proj_out\", \"proj_in\"],  \n",
    "    lora_dropout=0.1,  # Dropout for regularization\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "unet = get_peft_model(unet, lora_config)\n",
    "unet.print_trainable_parameters()\n",
    "\n",
    "# Enable memory optimization\n",
    "unet.enable_gradient_checkpointing()\n",
    "\n",
    "class EmbeddingProjector(nn.Module):\n",
    "    def __init__(self, input_dim=512, output_dim=1024):  # Change output to 1280\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "embedding_projector = EmbeddingProjector().to(device, dtype=torch.float16)\n",
    "\n",
    "# Freeze VAE\n",
    "for param in vae.parameters():\n",
    "    param.requires_grad = False  \n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, unet.parameters()), lr=0.001)\n",
    "scaler = torch.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: 100%|██████████| 85/85 [01:25<00:00,  1.01s/it, loss=1.01] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 1.0192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30: 100%|██████████| 85/85 [01:20<00:00,  1.06it/s, loss=1.02] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30, Loss: 1.0038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30: 100%|██████████| 85/85 [01:13<00:00,  1.16it/s, loss=1.01] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30, Loss: 0.9992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30: 100%|██████████| 85/85 [01:13<00:00,  1.15it/s, loss=0.993]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30, Loss: 1.0045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30: 100%|██████████| 85/85 [01:08<00:00,  1.23it/s, loss=1.01] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30, Loss: 1.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30: 100%|██████████| 85/85 [01:44<00:00,  1.23s/it, loss=0.974]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30, Loss: 0.9950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30: 100%|██████████| 85/85 [01:47<00:00,  1.27s/it, loss=1.01] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30, Loss: 0.9963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30: 100%|██████████| 85/85 [01:34<00:00,  1.11s/it, loss=1.02] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30, Loss: 0.9996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30: 100%|██████████| 85/85 [01:42<00:00,  1.21s/it, loss=0.977]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30, Loss: 0.9990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30: 100%|██████████| 85/85 [01:43<00:00,  1.21s/it, loss=0.987]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30, Loss: 0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30: 100%|██████████| 85/85 [01:58<00:00,  1.40s/it, loss=0.978]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30, Loss: 1.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30: 100%|██████████| 85/85 [03:26<00:00,  2.42s/it, loss=0.994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30, Loss: 0.9972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30: 100%|██████████| 85/85 [02:44<00:00,  1.93s/it, loss=1.01] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30, Loss: 0.9998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30: 100%|██████████| 85/85 [01:17<00:00,  1.10it/s, loss=1.01] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30, Loss: 1.0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30: 100%|██████████| 85/85 [01:30<00:00,  1.06s/it, loss=0.972]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30, Loss: 0.9993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30: 100%|██████████| 85/85 [02:16<00:00,  1.61s/it, loss=0.984]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30, Loss: 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30: 100%|██████████| 85/85 [02:28<00:00,  1.75s/it, loss=1.03] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30, Loss: 0.9990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30: 100%|██████████| 85/85 [03:28<00:00,  2.46s/it, loss=1.01] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30, Loss: 1.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30: 100%|██████████| 85/85 [03:27<00:00,  2.44s/it, loss=1.01] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30, Loss: 1.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30: 100%|██████████| 85/85 [03:53<00:00,  2.75s/it, loss=1.02] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30, Loss: 1.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30: 100%|██████████| 85/85 [03:07<00:00,  2.21s/it, loss=1.01] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30, Loss: 1.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30: 100%|██████████| 85/85 [02:53<00:00,  2.04s/it, loss=0.991]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30, Loss: 0.9972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30: 100%|██████████| 85/85 [01:19<00:00,  1.07it/s, loss=0.979]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30, Loss: 0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30: 100%|██████████| 85/85 [01:24<00:00,  1.00it/s, loss=0.995]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30, Loss: 1.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30: 100%|██████████| 85/85 [01:28<00:00,  1.04s/it, loss=0.999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30, Loss: 1.0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30: 100%|██████████| 85/85 [02:44<00:00,  1.94s/it, loss=1.04] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30, Loss: 1.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30: 100%|██████████| 85/85 [03:25<00:00,  2.42s/it, loss=1.02] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30, Loss: 0.9982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30: 100%|██████████| 85/85 [03:11<00:00,  2.25s/it, loss=0.999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30, Loss: 1.0027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/30: 100%|██████████| 85/85 [03:11<00:00,  2.26s/it, loss=0.996]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30, Loss: 0.9996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/30: 100%|██████████| 85/85 [03:16<00:00,  2.32s/it, loss=1]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30, Loss: 0.9982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 30\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for images, embeddings in progress_bar:\n",
    "        images = images.to(device, dtype=torch.float16, non_blocking=True)\n",
    "        embeddings = embeddings.to(device, dtype=torch.float16, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Project CLIP embeddings\n",
    "        with torch.no_grad():\n",
    "            projected_embeddings = embedding_projector(embeddings).unsqueeze(1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            latents = vae.encode(images).latent_dist.sample() * 0.18215\n",
    "        latents = latents.to(torch.float16)\n",
    "\n",
    "        # Generate noise\n",
    "        noise = torch.randn_like(latents, dtype=torch.float16)\n",
    "        timesteps = torch.randint(0, 1000, (latents.shape[0],), device=device).long()\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.amp.autocast(\"cuda\"):\n",
    "            noise_pred = unet(latents, timesteps, encoder_hidden_states=projected_embeddings).sample\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "\n",
    "        # Backpropagation\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "    losses.append(avg_epoch_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save Model\n",
    "torch.save({\n",
    "    \"unet\": unet.state_dict(),\n",
    "    \"embedding_projector\": embedding_projector.state_dict()\n",
    "}, \"emoji_generator.pth\")\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA adapters saved successfully!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save LoRA Weights\n",
    "unet.save_pretrained(\"lora_emoji_unet\")\n",
    "torch.save(embedding_projector.state_dict(), \"embedding_projector.pth\")\n",
    "print(\"LoRA adapters saved successfully!\")\n",
    "\n",
    "# Plot Loss Curve\n",
    "plt.plot(range(1, num_epochs + 1), losses, marker=\"o\", linestyle=\"-\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Study\\College\\Project\\myvenv\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "c:\\Study\\College\\Project\\myvenv\\Lib\\site-packages\\peft\\peft_model.py:569: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.base_model.model.down_blocks.0.attentions.0.proj_in.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.proj_in.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.proj_out.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.proj_out.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.proj_in.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.proj_in.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.proj_out.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.proj_out.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.proj_in.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.proj_in.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.proj_out.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.proj_out.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.proj_in.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.proj_in.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.proj_out.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.proj_out.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.proj_in.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.proj_in.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.proj_out.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.proj_out.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.proj_in.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.proj_in.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.proj_out.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.proj_out.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.proj_in.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.proj_in.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.proj_out.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.proj_out.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.proj_in.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.proj_in.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.proj_out.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.proj_out.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.proj_in.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.proj_in.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.proj_out.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.proj_out.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.proj_in.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.proj_in.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.proj_out.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.proj_out.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.proj_in.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.proj_in.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.proj_out.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.proj_out.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.proj_in.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.proj_in.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.proj_out.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.proj_out.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.proj_in.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.proj_in.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.proj_out.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.proj_out.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.proj_in.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.proj_in.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.proj_out.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.proj_out.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.proj_in.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.proj_in.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.proj_out.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.proj_out.lora_B.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.proj_in.lora_A.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.proj_in.lora_B.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.proj_out.lora_A.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.proj_out.lora_B.default.weight'].\n",
      "  warnings.warn(warn_message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EmbeddingProjector(\n",
       "  (fc): Linear(in_features=512, out_features=1024, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Model for Inference\n",
    "unet = PeftModel.from_pretrained(unet, \"lora_emoji_unet\").to(device).to(torch.float16)\n",
    "embedding_projector.load_state_dict(torch.load(\"embedding_projector.pth\"))\n",
    "unet.eval()\n",
    "embedding_projector.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP Model\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "\n",
    "# Text Processing\n",
    "text_description = \"Smiling face\"\n",
    "tokens = tokenizer(text_description, return_tensors=\"pt\").to(device)\n",
    "text_embedding = text_encoder(**tokens).last_hidden_state[:, 0, :]  # Use CLS token\n",
    "text_embedding = text_embedding.to(torch.float16)\n",
    "projected_embedding = embedding_projector(text_embedding).unsqueeze(0).to(torch.float16)\n",
    "\n",
    "# Generate Noise\n",
    "latents = torch.randn(1, 4, 64, 64, device=device, dtype=torch.float16)\n",
    "timesteps = torch.tensor([500], device=device).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Generate Emoji\n",
    "with torch.no_grad():\n",
    "    denoised_latents = unet(latents, timesteps, encoder_hidden_states=projected_embedding).sample\n",
    "\n",
    "denoised_latents = denoised_latents / 0.18215\n",
    "\n",
    "with torch.no_grad():\n",
    "    decoded_image = vae.decode(denoised_latents).sample\n",
    "\n",
    "decoded_image = (decoded_image.clamp(-1, 1) + 1) / 2\n",
    "emoji_image = Image.fromarray((decoded_image.squeeze(0).permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8))\n",
    "emoji_image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from diffusers import UNet2DConditionModel, AutoencoderKL\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "from peft import LoraConfig, PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt(GPT 4o): Fine tunning Stable Difussion model using Lora ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enable cuDNN optimization for faster training\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# Ensure PyTorch uses GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Free GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "class EmojiDataset(Dataset):\n",
    "    def __init__(self, parquet_file):\n",
    "        self.data = pd.read_parquet(parquet_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.data.iloc[idx][\"image_path\"]\n",
    "        image_tensor = torch.load(image_path).float() / 127.5 - 1  # Normalize to [-1,1]\n",
    "        text_embedding = torch.tensor(self.data.iloc[idx][\"combined_embedding\"], dtype=torch.float32)\n",
    "        return image_tensor, text_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenDict({'sample_size': 64, 'in_channels': 4, 'out_channels': 4, 'center_input_sample': False, 'flip_sin_to_cos': True, 'freq_shift': 0, 'down_block_types': ['CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'DownBlock2D'], 'mid_block_type': 'UNetMidBlock2DCrossAttn', 'up_block_types': ['UpBlock2D', 'CrossAttnUpBlock2D', 'CrossAttnUpBlock2D', 'CrossAttnUpBlock2D'], 'only_cross_attention': False, 'block_out_channels': [320, 640, 1280, 1280], 'layers_per_block': 2, 'downsample_padding': 1, 'mid_block_scale_factor': 1, 'dropout': 0.0, 'act_fn': 'silu', 'norm_num_groups': 32, 'norm_eps': 1e-05, 'cross_attention_dim': 1024, 'transformer_layers_per_block': 1, 'reverse_transformer_layers_per_block': None, 'encoder_hid_dim': None, 'encoder_hid_dim_type': None, 'attention_head_dim': [5, 10, 20, 20], 'num_attention_heads': None, 'dual_cross_attention': False, 'use_linear_projection': True, 'class_embed_type': None, 'addition_embed_type': None, 'addition_time_embed_dim': None, 'num_class_embeds': None, 'upcast_attention': False, 'resnet_time_scale_shift': 'default', 'resnet_skip_time_act': False, 'resnet_out_scale_factor': 1.0, 'time_embedding_type': 'positional', 'time_embedding_dim': None, 'time_embedding_act_fn': None, 'timestep_post_act': None, 'time_cond_proj_dim': None, 'conv_in_kernel': 3, 'conv_out_kernel': 3, 'projection_class_embeddings_input_dim': None, 'attention_type': 'default', 'class_embeddings_concat': False, 'mid_block_only_cross_attention': None, 'cross_attention_norm': None, 'addition_embed_type_num_heads': 64, '_use_default_values': ['mid_block_only_cross_attention', 'conv_in_kernel', 'time_cond_proj_dim', 'resnet_time_scale_shift', 'addition_embed_type', 'mid_block_type', 'num_attention_heads', 'num_class_embeds', 'timestep_post_act', 'time_embedding_dim', 'conv_out_kernel', 'attention_type', 'reverse_transformer_layers_per_block', 'resnet_out_scale_factor', 'class_embeddings_concat', 'resnet_skip_time_act', 'encoder_hid_dim', 'only_cross_attention', 'encoder_hid_dim_type', 'cross_attention_norm', 'time_embedding_act_fn', 'upcast_attention', 'addition_embed_type_num_heads', 'dropout', 'time_embedding_type', 'transformer_layers_per_block', 'addition_time_embed_dim', 'class_embed_type', 'projection_class_embeddings_input_dim'], '_class_name': 'UNet2DConditionModel', '_diffusers_version': '0.8.0', '_name_or_path': 'stabilityai/stable-diffusion-2-base'})\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "parquet_file = \"../data/processed_emoji_dataset.parquet\"\n",
    "dataset = EmojiDataset(parquet_file)\n",
    "batch_size = 32  \n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Load Stable Diffusion 2 Base Model (Trained on 256x256 images)\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/stable-diffusion-2-base\", subfolder=\"vae\").to(device, dtype=torch.float16)\n",
    "unet = UNet2DConditionModel.from_pretrained(\"stabilityai/stable-diffusion-2-base\", subfolder=\"unet\").to(device, dtype=torch.float16)\n",
    "\n",
    "print(unet.config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 207,488 || all params: 866,118,212 || trainable%: 0.0240\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Apply LoRA to UNet\n",
    "lora_config = LoraConfig(\n",
    "    r=1,  # LoRA rank\n",
    "    lora_alpha=8,  # Scaling factor\n",
    "    target_modules=[\"to_q\", \"to_k\", \"to_v\", \"proj_out\", \"proj_in\"],  \n",
    "    lora_dropout=0.1,  # Dropout for regularization\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "unet = get_peft_model(unet, lora_config)\n",
    "unet.print_trainable_parameters()  # Print trainable parameters (should be very small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable memory optimization\n",
    "unet.enable_gradient_checkpointing()\n",
    "\n",
    "# Embedding Projector (CLIP 512 → UNet 768)\n",
    "class EmbeddingProjector(nn.Module):\n",
    "    def __init__(self, input_dim=512, output_dim=1024):  # Changed to 768\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "embedding_projector = EmbeddingProjector().to(device, dtype=torch.float16)\n",
    "\n",
    "# Freeze VAE\n",
    "for param in vae.parameters():\n",
    "    param.requires_grad = False  \n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, unet.parameters()), lr=0.001)\n",
    "scaler = torch.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/85 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/tensor_images/OpenMojiEmoji\\\\1f468-1f468-1f467-1f467.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(train_dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Study\\College\\Project\\myvenv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Study\\College\\Project\\myvenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Study\\College\\Project\\myvenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Study\\College\\Project\\myvenv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[3], line 11\u001b[0m, in \u001b[0;36mEmojiDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     10\u001b[0m     image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39miloc[idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 11\u001b[0m     image_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m127.5\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Normalize to [-1,1]\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     text_embedding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39miloc[idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombined_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image_tensor, text_embedding\n",
      "File \u001b[1;32mc:\\Study\\College\\Project\\myvenv\\Lib\\site-packages\\torch\\serialization.py:1425\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1423\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1425\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1427\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1428\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1429\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1430\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Study\\College\\Project\\myvenv\\Lib\\site-packages\\torch\\serialization.py:751\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    750\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 751\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    753\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Study\\College\\Project\\myvenv\\Lib\\site-packages\\torch\\serialization.py:732\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 732\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/tensor_images/OpenMojiEmoji\\\\1f468-1f468-1f467-1f467.pt'"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "num_epochs = 5\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for images, embeddings in progress_bar:\n",
    "        images = images.to(device, dtype=torch.float16, non_blocking=True)\n",
    "        embeddings = embeddings.to(device, dtype=torch.float16, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Project CLIP embeddings\n",
    "        with torch.no_grad():\n",
    "            projected_embeddings = embedding_projector(embeddings).unsqueeze(1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            latents = vae.encode(images).latent_dist.sample() * 0.18215\n",
    "\n",
    "        # Convert latents to bfloat16 to save memory\n",
    "        latents = latents.to(torch.float16)\n",
    "\n",
    "        # Generate noise\n",
    "        noise = torch.randn_like(latents, dtype=torch.float16)\n",
    "        timesteps = torch.randint(0, 1000, (latents.shape[0],), device=device).long()\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.amp.autocast(\"cuda\"):\n",
    "            noise_pred = unet(latents, timesteps, encoder_hidden_states=projected_embeddings).sample\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "\n",
    "        # Backpropagation\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)  # Avoid NaN issues\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "    losses.append(avg_epoch_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save Model\n",
    "torch.save({\n",
    "    \"unet\": unet.state_dict(),\n",
    "    \"embedding_projector\": embedding_projector.state_dict()\n",
    "}, \"emoji_generator.pth\")\n",
    "print(\"Model saved successfully!\")\n",
    "\n",
    "# Save LoRA Weights\n",
    "unet.save_pretrained(\"lora_emoji_unet\")\n",
    "torch.save(embedding_projector.state_dict(), \"embedding_projector.pth\")\n",
    "print(\"LoRA adapters saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot Loss Curve\n",
    "plt.plot(range(1, num_epochs + 1), losses, marker=\"o\", linestyle=\"-\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = PeftModel.from_pretrained(unet, \"lora_emoji_unet\").to(device).to(torch.float16)\n",
    "embedding_projector.load_state_dict(torch.load(\"embedding_projector.pth\"))\n",
    "unet.eval()\n",
    "embedding_projector.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming the necessary models are already loaded\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "text_encoder = text_encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your text prompt\n",
    "text_description = \"Smiling face\"  # This is where we specify \"dog\"\n",
    "\n",
    "# Tokenize and encode the text prompt\n",
    "tokens = tokenizer(text_description, return_tensors=\"pt\").to(device)\n",
    "text_embedding = text_encoder(**tokens).last_hidden_state.mean(dim=1)  # Aggregate token embeddings\n",
    "text_embedding = text_embedding.to(torch.float16)  # Ensure it's float16 for compatibility with UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project the embedding to match UNet’s expected format\n",
    "projected_embedding = embedding_projector(text_embedding).unsqueeze(0).to(torch.float16)  # Ensure float16\n",
    "\n",
    "# Generate noise in latent space (fixed size 96x96)\n",
    "latents = torch.randn(1, 4, 96, 96).to(device).to(torch.float16)  # Ensure latents are in float16\n",
    "timesteps = torch.tensor([500], device=device).long()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Generate noise in latent space (smaller size)\n",
    "latents = torch.randn(1, 4, 64, 64, device=device, dtype=torch.float16)\n",
    "timesteps = torch.tensor([500], device=device).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Emoji\n",
    "with torch.no_grad():\n",
    "    denoised_latents = unet(latents, timesteps, encoder_hidden_states=projected_embedding).sample\n",
    "\n",
    "# Move to CPU and Decode\n",
    "denoised_latents = denoised_latents / 0.18215\n",
    "\n",
    "with torch.no_grad():\n",
    "    decoded_image = vae.decode(denoised_latents).sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Post-process Image\n",
    "decoded_image = (decoded_image.clamp(-1, 1) + 1) / 2\n",
    "decoded_image = decoded_image.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "decoded_image = (decoded_image * 255).astype(np.uint8)\n",
    "emoji_image = Image.fromarray(decoded_image)\n",
    "\n",
    "# Display the image\n",
    "emoji_image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
