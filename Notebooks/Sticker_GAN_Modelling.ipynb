{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T14:06:15.510345Z",
     "start_time": "2025-02-13T14:06:14.952155Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NVIDIA GPU (CUDA)\n",
      "Epoch [1/5000] | D Loss: 0.5273 | G Loss: 2.6988\n",
      "Epoch [2/5000] | D Loss: 0.5713 | G Loss: 1.8639\n",
      "Epoch [3/5000] | D Loss: 1.0389 | G Loss: 1.4714\n",
      "Epoch [4/5000] | D Loss: 1.2142 | G Loss: 1.2669\n",
      "Epoch [5/5000] | D Loss: 1.2757 | G Loss: 0.9785\n",
      "Epoch [6/5000] | D Loss: 1.1565 | G Loss: 0.9270\n",
      "Epoch [7/5000] | D Loss: 1.1086 | G Loss: 1.1707\n",
      "Epoch [8/5000] | D Loss: 1.0925 | G Loss: 1.2096\n",
      "Epoch [9/5000] | D Loss: 1.1195 | G Loss: 1.0311\n",
      "Epoch [10/5000] | D Loss: 0.8552 | G Loss: 1.3062\n",
      "Epoch [11/5000] | D Loss: 1.1800 | G Loss: 1.0851\n",
      "Epoch [12/5000] | D Loss: 1.5196 | G Loss: 0.8307\n",
      "Epoch [13/5000] | D Loss: 1.1984 | G Loss: 0.8569\n",
      "Epoch [14/5000] | D Loss: 1.1371 | G Loss: 0.8797\n",
      "Epoch [15/5000] | D Loss: 1.3121 | G Loss: 1.1381\n",
      "Epoch [16/5000] | D Loss: 1.0238 | G Loss: 1.2024\n",
      "Epoch [17/5000] | D Loss: 1.3852 | G Loss: 0.8648\n",
      "Epoch [18/5000] | D Loss: 1.2535 | G Loss: 1.2320\n",
      "Epoch [19/5000] | D Loss: 0.9803 | G Loss: 1.3076\n",
      "Epoch [20/5000] | D Loss: 1.2282 | G Loss: 0.9947\n",
      "Epoch [21/5000] | D Loss: 0.9250 | G Loss: 1.1047\n",
      "Epoch [22/5000] | D Loss: 1.1587 | G Loss: 1.1966\n",
      "Epoch [23/5000] | D Loss: 1.1786 | G Loss: 0.9962\n",
      "Epoch [24/5000] | D Loss: 1.2571 | G Loss: 0.8871\n",
      "Epoch [25/5000] | D Loss: 1.3779 | G Loss: 0.9201\n",
      "Epoch [26/5000] | D Loss: 1.0018 | G Loss: 1.0897\n",
      "Epoch [27/5000] | D Loss: 1.1695 | G Loss: 0.9809\n",
      "Epoch [28/5000] | D Loss: 1.0469 | G Loss: 1.1229\n",
      "Epoch [29/5000] | D Loss: 0.9079 | G Loss: 1.2926\n",
      "Epoch [30/5000] | D Loss: 1.2993 | G Loss: 1.0670\n",
      "Epoch [31/5000] | D Loss: 1.2024 | G Loss: 1.0218\n",
      "Epoch [32/5000] | D Loss: 1.0826 | G Loss: 1.0903\n",
      "Epoch [33/5000] | D Loss: 1.0541 | G Loss: 1.0650\n",
      "Epoch [34/5000] | D Loss: 1.0921 | G Loss: 1.0969\n",
      "Epoch [35/5000] | D Loss: 1.0703 | G Loss: 1.0446\n",
      "Epoch [36/5000] | D Loss: 1.3287 | G Loss: 0.8285\n",
      "Epoch [37/5000] | D Loss: 1.0131 | G Loss: 1.0267\n",
      "Epoch [38/5000] | D Loss: 1.0085 | G Loss: 1.0883\n",
      "Epoch [39/5000] | D Loss: 1.2593 | G Loss: 1.1722\n",
      "Epoch [40/5000] | D Loss: 1.1170 | G Loss: 1.1303\n",
      "Epoch [41/5000] | D Loss: 1.2254 | G Loss: 1.0390\n",
      "Epoch [42/5000] | D Loss: 1.1983 | G Loss: 1.1099\n",
      "Epoch [43/5000] | D Loss: 0.9766 | G Loss: 1.2972\n",
      "Epoch [44/5000] | D Loss: 1.1850 | G Loss: 1.1135\n",
      "Epoch [45/5000] | D Loss: 1.0035 | G Loss: 1.3352\n",
      "Epoch [46/5000] | D Loss: 1.2096 | G Loss: 1.4181\n",
      "Epoch [47/5000] | D Loss: 1.3281 | G Loss: 0.8789\n",
      "Epoch [48/5000] | D Loss: 0.9852 | G Loss: 0.9994\n",
      "Epoch [49/5000] | D Loss: 0.8515 | G Loss: 1.3368\n",
      "Epoch [50/5000] | D Loss: 1.2786 | G Loss: 1.0014\n",
      "Epoch [51/5000] | D Loss: 1.0849 | G Loss: 1.0665\n",
      "Epoch [52/5000] | D Loss: 1.1768 | G Loss: 1.5019\n",
      "Epoch [53/5000] | D Loss: 1.1499 | G Loss: 1.0751\n",
      "Epoch [54/5000] | D Loss: 1.1169 | G Loss: 1.0560\n",
      "Epoch [55/5000] | D Loss: 1.2662 | G Loss: 0.9791\n",
      "Epoch [56/5000] | D Loss: 1.1308 | G Loss: 1.2564\n",
      "Epoch [57/5000] | D Loss: 1.0913 | G Loss: 1.0557\n",
      "Epoch [58/5000] | D Loss: 1.1623 | G Loss: 1.2238\n",
      "Epoch [59/5000] | D Loss: 1.3671 | G Loss: 1.0533\n",
      "Epoch [60/5000] | D Loss: 1.3601 | G Loss: 1.3521\n",
      "Epoch [61/5000] | D Loss: 0.8844 | G Loss: 1.5040\n",
      "Epoch [62/5000] | D Loss: 1.4608 | G Loss: 1.4202\n",
      "Epoch [63/5000] | D Loss: 1.0927 | G Loss: 1.2339\n",
      "Epoch [64/5000] | D Loss: 1.0351 | G Loss: 1.1455\n",
      "Epoch [65/5000] | D Loss: 1.1190 | G Loss: 0.9962\n",
      "Epoch [66/5000] | D Loss: 1.1159 | G Loss: 0.9957\n",
      "Epoch [67/5000] | D Loss: 1.1756 | G Loss: 1.0821\n",
      "Epoch [68/5000] | D Loss: 0.9445 | G Loss: 1.1541\n",
      "Epoch [69/5000] | D Loss: 0.9149 | G Loss: 1.1845\n",
      "Epoch [70/5000] | D Loss: 1.0875 | G Loss: 1.0560\n",
      "Epoch [71/5000] | D Loss: 1.3042 | G Loss: 1.0559\n",
      "Epoch [72/5000] | D Loss: 1.2445 | G Loss: 1.0804\n",
      "Epoch [73/5000] | D Loss: 1.0183 | G Loss: 1.0851\n",
      "Epoch [74/5000] | D Loss: 1.0909 | G Loss: 1.1052\n",
      "Epoch [75/5000] | D Loss: 1.0899 | G Loss: 1.3215\n",
      "Epoch [76/5000] | D Loss: 1.1686 | G Loss: 1.0728\n",
      "Epoch [77/5000] | D Loss: 0.7406 | G Loss: 1.5181\n",
      "Epoch [78/5000] | D Loss: 1.1287 | G Loss: 1.0963\n",
      "Epoch [79/5000] | D Loss: 0.7072 | G Loss: 1.7041\n",
      "Epoch [80/5000] | D Loss: 0.9206 | G Loss: 1.3061\n",
      "Epoch [81/5000] | D Loss: 1.2501 | G Loss: 1.0340\n",
      "Epoch [82/5000] | D Loss: 1.1125 | G Loss: 0.9774\n",
      "Epoch [83/5000] | D Loss: 1.2723 | G Loss: 1.0535\n",
      "Epoch [84/5000] | D Loss: 1.1733 | G Loss: 1.0964\n",
      "Epoch [85/5000] | D Loss: 0.9188 | G Loss: 1.2592\n",
      "Epoch [86/5000] | D Loss: 0.9920 | G Loss: 1.3075\n",
      "Epoch [87/5000] | D Loss: 0.8872 | G Loss: 1.5358\n",
      "Epoch [88/5000] | D Loss: 1.0692 | G Loss: 1.0435\n",
      "Epoch [89/5000] | D Loss: 0.8915 | G Loss: 1.2680\n",
      "Epoch [90/5000] | D Loss: 1.0120 | G Loss: 1.3192\n",
      "Epoch [91/5000] | D Loss: 1.1820 | G Loss: 1.0531\n",
      "Epoch [92/5000] | D Loss: 1.0243 | G Loss: 1.3895\n",
      "Epoch [93/5000] | D Loss: 1.0272 | G Loss: 1.2717\n",
      "Epoch [94/5000] | D Loss: 1.0544 | G Loss: 1.2600\n",
      "Epoch [95/5000] | D Loss: 1.4476 | G Loss: 1.1010\n",
      "Epoch [96/5000] | D Loss: 0.8943 | G Loss: 0.9663\n",
      "Epoch [97/5000] | D Loss: 1.1846 | G Loss: 1.1314\n",
      "Epoch [98/5000] | D Loss: 0.8660 | G Loss: 1.3423\n",
      "Epoch [99/5000] | D Loss: 1.2655 | G Loss: 1.2345\n",
      "Epoch [100/5000] | D Loss: 1.0740 | G Loss: 1.5274\n",
      "Epoch 100 FID Score: 212.3314\n",
      "Epoch [101/5000] | D Loss: 1.1164 | G Loss: 1.1302\n",
      "Epoch [102/5000] | D Loss: 1.1093 | G Loss: 1.2682\n",
      "Epoch [103/5000] | D Loss: 1.0917 | G Loss: 1.1016\n",
      "Epoch [104/5000] | D Loss: 1.1758 | G Loss: 1.0134\n",
      "Epoch [105/5000] | D Loss: 1.2981 | G Loss: 1.0060\n",
      "Epoch [106/5000] | D Loss: 1.1591 | G Loss: 1.2252\n",
      "Epoch [107/5000] | D Loss: 1.0514 | G Loss: 1.4534\n",
      "Epoch [108/5000] | D Loss: 1.1478 | G Loss: 1.0315\n",
      "Epoch [109/5000] | D Loss: 0.8896 | G Loss: 1.3022\n",
      "Epoch [110/5000] | D Loss: 1.0672 | G Loss: 1.2646\n",
      "Epoch [111/5000] | D Loss: 0.9838 | G Loss: 1.1278\n",
      "Epoch [112/5000] | D Loss: 0.9600 | G Loss: 1.5642\n",
      "Epoch [113/5000] | D Loss: 1.0763 | G Loss: 1.3668\n",
      "Epoch [114/5000] | D Loss: 1.0528 | G Loss: 1.2013\n",
      "Epoch [115/5000] | D Loss: 0.7683 | G Loss: 1.7765\n",
      "Epoch [116/5000] | D Loss: 1.0778 | G Loss: 1.2119\n",
      "Epoch [117/5000] | D Loss: 1.1571 | G Loss: 1.3337\n",
      "Epoch [118/5000] | D Loss: 1.2529 | G Loss: 0.8694\n",
      "Epoch [119/5000] | D Loss: 0.9973 | G Loss: 1.4818\n",
      "Epoch [120/5000] | D Loss: 1.0098 | G Loss: 1.4169\n",
      "Epoch [121/5000] | D Loss: 0.8825 | G Loss: 1.9652\n",
      "Epoch [122/5000] | D Loss: 1.0849 | G Loss: 0.8577\n",
      "Epoch [123/5000] | D Loss: 1.0406 | G Loss: 1.0196\n",
      "Epoch [124/5000] | D Loss: 0.9071 | G Loss: 1.2227\n",
      "Epoch [125/5000] | D Loss: 0.9367 | G Loss: 1.6141\n",
      "Epoch [126/5000] | D Loss: 1.1188 | G Loss: 1.1803\n",
      "Epoch [127/5000] | D Loss: 0.9244 | G Loss: 0.9857\n",
      "Epoch [128/5000] | D Loss: 0.9852 | G Loss: 1.1771\n",
      "Epoch [129/5000] | D Loss: 1.0135 | G Loss: 1.5811\n",
      "Epoch [130/5000] | D Loss: 0.9546 | G Loss: 1.5090\n",
      "Epoch [131/5000] | D Loss: 1.4878 | G Loss: 1.0038\n",
      "Epoch [132/5000] | D Loss: 1.1387 | G Loss: 0.8463\n",
      "Epoch [133/5000] | D Loss: 1.1268 | G Loss: 1.0888\n",
      "Epoch [134/5000] | D Loss: 0.9976 | G Loss: 1.2641\n",
      "Epoch [135/5000] | D Loss: 0.9886 | G Loss: 1.4246\n",
      "Epoch [136/5000] | D Loss: 1.1331 | G Loss: 1.1525\n",
      "Epoch [137/5000] | D Loss: 0.8973 | G Loss: 1.6239\n",
      "Epoch [138/5000] | D Loss: 1.3012 | G Loss: 1.1305\n",
      "Epoch [139/5000] | D Loss: 1.0263 | G Loss: 1.0781\n",
      "Epoch [140/5000] | D Loss: 1.0737 | G Loss: 1.0872\n",
      "Epoch [141/5000] | D Loss: 0.8622 | G Loss: 1.3882\n",
      "Epoch [142/5000] | D Loss: 0.8799 | G Loss: 1.4039\n",
      "Epoch [143/5000] | D Loss: 1.0487 | G Loss: 1.2076\n",
      "Epoch [144/5000] | D Loss: 0.6755 | G Loss: 1.5883\n",
      "Epoch [145/5000] | D Loss: 1.1239 | G Loss: 1.5489\n",
      "Epoch [146/5000] | D Loss: 0.9982 | G Loss: 1.2472\n",
      "Epoch [147/5000] | D Loss: 1.0434 | G Loss: 1.1486\n",
      "Epoch [148/5000] | D Loss: 1.0161 | G Loss: 1.2123\n",
      "Epoch [149/5000] | D Loss: 1.1746 | G Loss: 1.4933\n",
      "Epoch [150/5000] | D Loss: 0.8058 | G Loss: 1.7949\n",
      "Epoch [151/5000] | D Loss: 1.2831 | G Loss: 1.5927\n",
      "Epoch [152/5000] | D Loss: 1.5223 | G Loss: 0.9201\n",
      "Epoch [153/5000] | D Loss: 1.2631 | G Loss: 1.1713\n",
      "Epoch [154/5000] | D Loss: 1.0223 | G Loss: 1.0572\n",
      "Epoch [155/5000] | D Loss: 0.9020 | G Loss: 1.7958\n",
      "Epoch [156/5000] | D Loss: 0.9944 | G Loss: 1.5424\n",
      "Epoch [157/5000] | D Loss: 1.0702 | G Loss: 0.9626\n",
      "Epoch [158/5000] | D Loss: 1.0405 | G Loss: 1.2163\n",
      "Epoch [159/5000] | D Loss: 0.9030 | G Loss: 1.5383\n",
      "Epoch [160/5000] | D Loss: 1.0817 | G Loss: 1.1588\n",
      "Epoch [161/5000] | D Loss: 1.0512 | G Loss: 1.7789\n",
      "Epoch [162/5000] | D Loss: 0.8357 | G Loss: 1.8316\n",
      "Epoch [163/5000] | D Loss: 1.0463 | G Loss: 1.7053\n",
      "Epoch [164/5000] | D Loss: 0.7295 | G Loss: 1.9745\n",
      "Epoch [165/5000] | D Loss: 0.7568 | G Loss: 1.3627\n",
      "Epoch [166/5000] | D Loss: 1.3473 | G Loss: 0.9780\n",
      "Epoch [167/5000] | D Loss: 1.0729 | G Loss: 1.8600\n",
      "Epoch [168/5000] | D Loss: 1.0174 | G Loss: 1.4816\n",
      "Epoch [169/5000] | D Loss: 1.2312 | G Loss: 0.9829\n",
      "Epoch [170/5000] | D Loss: 0.9267 | G Loss: 1.3447\n",
      "Epoch [171/5000] | D Loss: 1.0144 | G Loss: 1.4161\n",
      "Epoch [172/5000] | D Loss: 0.8315 | G Loss: 1.8464\n",
      "Epoch [173/5000] | D Loss: 0.7709 | G Loss: 1.7074\n",
      "Epoch [174/5000] | D Loss: 1.9627 | G Loss: 0.7924\n",
      "Epoch [175/5000] | D Loss: 1.2523 | G Loss: 0.9512\n",
      "Epoch [176/5000] | D Loss: 0.8451 | G Loss: 2.0040\n",
      "Epoch [177/5000] | D Loss: 0.9829 | G Loss: 1.5719\n",
      "Epoch [178/5000] | D Loss: 0.9475 | G Loss: 1.2066\n",
      "Epoch [179/5000] | D Loss: 0.7704 | G Loss: 2.2199\n",
      "Epoch [180/5000] | D Loss: 0.7651 | G Loss: 1.7435\n",
      "Epoch [181/5000] | D Loss: 1.0367 | G Loss: 1.2315\n",
      "Epoch [182/5000] | D Loss: 0.9237 | G Loss: 1.5581\n",
      "Epoch [183/5000] | D Loss: 0.6692 | G Loss: 2.5207\n",
      "Epoch [184/5000] | D Loss: 0.9596 | G Loss: 1.9793\n",
      "Epoch [185/5000] | D Loss: 0.8993 | G Loss: 1.8701\n",
      "Epoch [186/5000] | D Loss: 0.7677 | G Loss: 2.2400\n",
      "Epoch [187/5000] | D Loss: 0.7379 | G Loss: 1.7781\n",
      "Epoch [188/5000] | D Loss: 0.9869 | G Loss: 1.7719\n",
      "Epoch [189/5000] | D Loss: 1.2212 | G Loss: 1.2895\n",
      "Epoch [190/5000] | D Loss: 0.8127 | G Loss: 1.7190\n",
      "Epoch [191/5000] | D Loss: 1.0402 | G Loss: 0.8316\n",
      "Epoch [192/5000] | D Loss: 0.6434 | G Loss: 1.6077\n",
      "Epoch [193/5000] | D Loss: 0.9051 | G Loss: 1.6816\n",
      "Epoch [194/5000] | D Loss: 1.1367 | G Loss: 2.4530\n",
      "Epoch [195/5000] | D Loss: 1.3912 | G Loss: 0.8333\n",
      "Epoch [196/5000] | D Loss: 0.7177 | G Loss: 2.1614\n",
      "Epoch [197/5000] | D Loss: 1.0346 | G Loss: 1.7016\n",
      "Epoch [198/5000] | D Loss: 0.9940 | G Loss: 1.8371\n",
      "Epoch [199/5000] | D Loss: 1.0139 | G Loss: 1.0251\n",
      "Epoch [200/5000] | D Loss: 0.8575 | G Loss: 2.4084\n",
      "Epoch 200 FID Score: 205.3187\n",
      "Epoch [201/5000] | D Loss: 1.5106 | G Loss: 1.3847\n",
      "Epoch [202/5000] | D Loss: 0.8296 | G Loss: 1.6100\n",
      "Epoch [203/5000] | D Loss: 0.8510 | G Loss: 1.9294\n",
      "Epoch [204/5000] | D Loss: 0.9922 | G Loss: 1.7159\n",
      "Epoch [205/5000] | D Loss: 0.6260 | G Loss: 2.5387\n",
      "Epoch [206/5000] | D Loss: 0.8449 | G Loss: 2.3447\n",
      "Epoch [207/5000] | D Loss: 1.2092 | G Loss: 1.5742\n",
      "Epoch [208/5000] | D Loss: 0.7537 | G Loss: 1.9755\n",
      "Epoch [209/5000] | D Loss: 0.7434 | G Loss: 2.0418\n",
      "Epoch [210/5000] | D Loss: 1.5286 | G Loss: 0.7582\n",
      "Epoch [211/5000] | D Loss: 0.8827 | G Loss: 1.5287\n",
      "Epoch [212/5000] | D Loss: 1.0674 | G Loss: 1.6680\n",
      "Epoch [213/5000] | D Loss: 0.8389 | G Loss: 1.6910\n",
      "Epoch [214/5000] | D Loss: 0.8968 | G Loss: 1.5089\n",
      "Epoch [215/5000] | D Loss: 0.8178 | G Loss: 1.5748\n",
      "Epoch [216/5000] | D Loss: 0.8306 | G Loss: 1.9052\n",
      "Epoch [217/5000] | D Loss: 1.4949 | G Loss: 0.9491\n",
      "Epoch [218/5000] | D Loss: 0.8500 | G Loss: 2.1021\n",
      "Epoch [219/5000] | D Loss: 1.0333 | G Loss: 1.7924\n",
      "Epoch [220/5000] | D Loss: 0.7643 | G Loss: 1.4945\n",
      "Epoch [221/5000] | D Loss: 0.8149 | G Loss: 2.4167\n",
      "Epoch [222/5000] | D Loss: 0.7874 | G Loss: 2.3169\n",
      "Epoch [223/5000] | D Loss: 1.7306 | G Loss: 0.4196\n",
      "Epoch [224/5000] | D Loss: 0.7373 | G Loss: 1.9626\n",
      "Epoch [225/5000] | D Loss: 0.8226 | G Loss: 0.9136\n",
      "Epoch [226/5000] | D Loss: 0.6925 | G Loss: 2.2900\n",
      "Epoch [227/5000] | D Loss: 0.8459 | G Loss: 2.3068\n",
      "Epoch [228/5000] | D Loss: 0.7377 | G Loss: 1.7939\n",
      "Epoch [229/5000] | D Loss: 1.0110 | G Loss: 2.0393\n",
      "Epoch [230/5000] | D Loss: 1.2440 | G Loss: 0.9284\n",
      "Epoch [231/5000] | D Loss: 1.8474 | G Loss: 0.6670\n",
      "Epoch [232/5000] | D Loss: 1.6972 | G Loss: 1.3146\n",
      "Epoch [233/5000] | D Loss: 0.6529 | G Loss: 2.5924\n",
      "Epoch [234/5000] | D Loss: 1.1021 | G Loss: 1.7636\n",
      "Epoch [235/5000] | D Loss: 1.2572 | G Loss: 1.1870\n",
      "Epoch [236/5000] | D Loss: 2.2847 | G Loss: 0.5974\n",
      "Epoch [237/5000] | D Loss: 0.8306 | G Loss: 2.2560\n",
      "Epoch [238/5000] | D Loss: 1.1747 | G Loss: 0.9027\n",
      "Epoch [239/5000] | D Loss: 1.0657 | G Loss: 2.2324\n",
      "Epoch [240/5000] | D Loss: 1.1078 | G Loss: 1.5275\n",
      "Epoch [241/5000] | D Loss: 1.7818 | G Loss: 0.7622\n",
      "Epoch [242/5000] | D Loss: 0.8852 | G Loss: 2.3993\n",
      "Epoch [243/5000] | D Loss: 0.8196 | G Loss: 1.9746\n",
      "Epoch [244/5000] | D Loss: 0.7437 | G Loss: 2.0260\n",
      "Epoch [245/5000] | D Loss: 0.8172 | G Loss: 2.2488\n",
      "Epoch [246/5000] | D Loss: 0.9621 | G Loss: 2.0620\n",
      "Epoch [247/5000] | D Loss: 1.0275 | G Loss: 1.6730\n",
      "Epoch [248/5000] | D Loss: 0.8335 | G Loss: 1.6688\n",
      "Epoch [249/5000] | D Loss: 1.5977 | G Loss: 1.2406\n",
      "Epoch [250/5000] | D Loss: 0.6175 | G Loss: 2.4607\n",
      "Epoch [251/5000] | D Loss: 0.8514 | G Loss: 1.3687\n",
      "Epoch [252/5000] | D Loss: 0.7680 | G Loss: 2.6082\n",
      "Epoch [253/5000] | D Loss: 0.6847 | G Loss: 2.1918\n",
      "Epoch [254/5000] | D Loss: 1.1172 | G Loss: 1.8253\n",
      "Epoch [255/5000] | D Loss: 0.8378 | G Loss: 2.6809\n",
      "Epoch [256/5000] | D Loss: 0.6308 | G Loss: 1.9325\n",
      "Epoch [257/5000] | D Loss: 0.6719 | G Loss: 2.1530\n",
      "Epoch [258/5000] | D Loss: 1.6919 | G Loss: 0.7773\n",
      "Epoch [259/5000] | D Loss: 1.0286 | G Loss: 1.4585\n",
      "Epoch [260/5000] | D Loss: 1.0895 | G Loss: 1.6194\n",
      "Epoch [261/5000] | D Loss: 0.6567 | G Loss: 2.6476\n",
      "Epoch [262/5000] | D Loss: 0.7280 | G Loss: 1.7075\n",
      "Epoch [263/5000] | D Loss: 0.6383 | G Loss: 2.5853\n",
      "Epoch [264/5000] | D Loss: 0.6991 | G Loss: 2.5531\n",
      "Epoch [265/5000] | D Loss: 0.9744 | G Loss: 1.9598\n",
      "Epoch [266/5000] | D Loss: 1.1178 | G Loss: 2.0787\n",
      "Epoch [267/5000] | D Loss: 0.7433 | G Loss: 2.5990\n",
      "Epoch [268/5000] | D Loss: 0.7157 | G Loss: 1.7523\n",
      "Epoch [269/5000] | D Loss: 0.6380 | G Loss: 1.9501\n",
      "Epoch [270/5000] | D Loss: 0.7262 | G Loss: 2.0012\n",
      "Epoch [271/5000] | D Loss: 0.6560 | G Loss: 2.7023\n",
      "Epoch [272/5000] | D Loss: 1.0642 | G Loss: 1.5583\n",
      "Epoch [273/5000] | D Loss: 0.6479 | G Loss: 2.6223\n",
      "Epoch [274/5000] | D Loss: 1.4568 | G Loss: 0.9484\n",
      "Epoch [275/5000] | D Loss: 1.2666 | G Loss: 1.9021\n",
      "Epoch [276/5000] | D Loss: 0.7259 | G Loss: 2.0126\n",
      "Epoch [277/5000] | D Loss: 0.8903 | G Loss: 1.9712\n",
      "Epoch [278/5000] | D Loss: 0.7759 | G Loss: 2.6592\n",
      "Epoch [279/5000] | D Loss: 0.6930 | G Loss: 2.8725\n",
      "Epoch [280/5000] | D Loss: 0.6562 | G Loss: 2.7291\n",
      "Epoch [281/5000] | D Loss: 0.7474 | G Loss: 1.8121\n",
      "Epoch [282/5000] | D Loss: 0.7552 | G Loss: 1.9496\n",
      "Epoch [283/5000] | D Loss: 0.6383 | G Loss: 2.9748\n",
      "Epoch [284/5000] | D Loss: 1.2224 | G Loss: 2.0035\n",
      "Epoch [285/5000] | D Loss: 0.9672 | G Loss: 2.5416\n",
      "Epoch [286/5000] | D Loss: 1.4625 | G Loss: 0.5035\n",
      "Epoch [287/5000] | D Loss: 1.4544 | G Loss: 1.5283\n",
      "Epoch [288/5000] | D Loss: 1.0600 | G Loss: 1.4979\n",
      "Epoch [289/5000] | D Loss: 0.7010 | G Loss: 2.1976\n",
      "Epoch [290/5000] | D Loss: 0.7130 | G Loss: 2.3807\n",
      "Epoch [291/5000] | D Loss: 0.7872 | G Loss: 2.3640\n",
      "Epoch [292/5000] | D Loss: 0.7566 | G Loss: 2.1216\n",
      "Epoch [293/5000] | D Loss: 0.7555 | G Loss: 2.5183\n",
      "Epoch [294/5000] | D Loss: 0.7359 | G Loss: 2.4876\n",
      "Epoch [295/5000] | D Loss: 0.7299 | G Loss: 2.7685\n",
      "Epoch [296/5000] | D Loss: 0.7920 | G Loss: 2.5034\n",
      "Epoch [297/5000] | D Loss: 0.7637 | G Loss: 2.3849\n",
      "Epoch [298/5000] | D Loss: 0.6480 | G Loss: 2.1563\n",
      "Epoch [299/5000] | D Loss: 0.7412 | G Loss: 2.4920\n",
      "Epoch [300/5000] | D Loss: 0.9028 | G Loss: 1.5563\n",
      "Epoch 300 FID Score: 194.2384\n",
      "Epoch [301/5000] | D Loss: 0.6703 | G Loss: 2.0670\n",
      "Epoch [302/5000] | D Loss: 0.7972 | G Loss: 1.7919\n",
      "Epoch [303/5000] | D Loss: 0.7514 | G Loss: 2.1761\n",
      "Epoch [304/5000] | D Loss: 0.9208 | G Loss: 1.8276\n",
      "Epoch [305/5000] | D Loss: 0.9245 | G Loss: 1.3830\n",
      "Epoch [306/5000] | D Loss: 0.7573 | G Loss: 2.0513\n",
      "Epoch [307/5000] | D Loss: 0.6249 | G Loss: 3.6998\n",
      "Epoch [308/5000] | D Loss: 0.6653 | G Loss: 1.8646\n",
      "Epoch [309/5000] | D Loss: 0.9511 | G Loss: 2.1054\n",
      "Epoch [310/5000] | D Loss: 0.8498 | G Loss: 2.0518\n",
      "Epoch [311/5000] | D Loss: 0.7086 | G Loss: 2.5578\n",
      "Epoch [312/5000] | D Loss: 0.7687 | G Loss: 2.6877\n",
      "Epoch [313/5000] | D Loss: 0.7731 | G Loss: 2.0760\n",
      "Epoch [314/5000] | D Loss: 0.7928 | G Loss: 2.3706\n",
      "Epoch [315/5000] | D Loss: 0.7971 | G Loss: 2.2948\n",
      "Epoch [316/5000] | D Loss: 0.9369 | G Loss: 2.0881\n",
      "Epoch [317/5000] | D Loss: 0.6622 | G Loss: 3.2721\n",
      "Epoch [318/5000] | D Loss: 0.7206 | G Loss: 1.4510\n",
      "Epoch [319/5000] | D Loss: 1.1989 | G Loss: 1.1688\n",
      "Epoch [320/5000] | D Loss: 0.6352 | G Loss: 2.7224\n",
      "Epoch [321/5000] | D Loss: 0.7355 | G Loss: 2.0463\n",
      "Epoch [322/5000] | D Loss: 0.7275 | G Loss: 2.5930\n",
      "Epoch [323/5000] | D Loss: 0.8782 | G Loss: 2.9632\n",
      "Epoch [324/5000] | D Loss: 0.7308 | G Loss: 2.4688\n",
      "Epoch [325/5000] | D Loss: 0.6961 | G Loss: 2.5211\n",
      "Epoch [326/5000] | D Loss: 0.5779 | G Loss: 2.1658\n",
      "Epoch [327/5000] | D Loss: 0.6876 | G Loss: 3.0068\n",
      "Epoch [328/5000] | D Loss: 0.6672 | G Loss: 1.8164\n",
      "Epoch [329/5000] | D Loss: 0.8506 | G Loss: 2.9135\n",
      "Epoch [330/5000] | D Loss: 1.0293 | G Loss: 1.4651\n",
      "Epoch [331/5000] | D Loss: 0.7596 | G Loss: 2.6767\n",
      "Epoch [332/5000] | D Loss: 0.6172 | G Loss: 3.1274\n",
      "Epoch [333/5000] | D Loss: 0.6646 | G Loss: 3.0962\n",
      "Epoch [334/5000] | D Loss: 0.8262 | G Loss: 1.2666\n",
      "Epoch [335/5000] | D Loss: 0.7256 | G Loss: 1.8525\n",
      "Epoch [336/5000] | D Loss: 0.6486 | G Loss: 2.4213\n",
      "Epoch [337/5000] | D Loss: 0.6691 | G Loss: 2.8055\n",
      "Epoch [338/5000] | D Loss: 1.0292 | G Loss: 1.2497\n",
      "Epoch [339/5000] | D Loss: 0.8429 | G Loss: 2.2217\n",
      "Epoch [340/5000] | D Loss: 0.7111 | G Loss: 1.9766\n",
      "Epoch [341/5000] | D Loss: 0.8437 | G Loss: 2.4560\n",
      "Epoch [342/5000] | D Loss: 0.9381 | G Loss: 2.6259\n",
      "Epoch [343/5000] | D Loss: 0.6917 | G Loss: 2.6813\n",
      "Epoch [344/5000] | D Loss: 0.7610 | G Loss: 2.4831\n",
      "Epoch [345/5000] | D Loss: 1.1834 | G Loss: 1.6847\n",
      "Epoch [346/5000] | D Loss: 1.7031 | G Loss: 0.6715\n",
      "Epoch [347/5000] | D Loss: 1.2345 | G Loss: 2.8320\n",
      "Epoch [348/5000] | D Loss: 0.8023 | G Loss: 3.2264\n",
      "Epoch [349/5000] | D Loss: 0.6844 | G Loss: 2.4148\n",
      "Epoch [350/5000] | D Loss: 0.7096 | G Loss: 2.3324\n",
      "Epoch [351/5000] | D Loss: 0.9472 | G Loss: 2.1270\n",
      "Epoch [352/5000] | D Loss: 0.9222 | G Loss: 2.0336\n",
      "Epoch [353/5000] | D Loss: 0.8167 | G Loss: 2.0728\n",
      "Epoch [354/5000] | D Loss: 0.7364 | G Loss: 2.4093\n",
      "Epoch [355/5000] | D Loss: 0.7137 | G Loss: 2.8924\n",
      "Epoch [356/5000] | D Loss: 0.8487 | G Loss: 2.5913\n",
      "Epoch [357/5000] | D Loss: 0.7283 | G Loss: 2.5102\n",
      "Epoch [358/5000] | D Loss: 0.7616 | G Loss: 2.9404\n",
      "Epoch [359/5000] | D Loss: 0.9481 | G Loss: 2.5010\n",
      "Epoch [360/5000] | D Loss: 0.9562 | G Loss: 1.5280\n",
      "Epoch [361/5000] | D Loss: 0.8852 | G Loss: 2.0488\n",
      "Epoch [362/5000] | D Loss: 1.1348 | G Loss: 2.4097\n",
      "Epoch [363/5000] | D Loss: 1.2582 | G Loss: 2.1243\n",
      "Epoch [364/5000] | D Loss: 0.9760 | G Loss: 2.6722\n",
      "Epoch [365/5000] | D Loss: 0.6637 | G Loss: 2.6204\n",
      "Epoch [366/5000] | D Loss: 0.7056 | G Loss: 3.0935\n",
      "Epoch [367/5000] | D Loss: 0.7329 | G Loss: 2.7602\n",
      "Epoch [368/5000] | D Loss: 0.7945 | G Loss: 1.7765\n",
      "Epoch [369/5000] | D Loss: 1.6612 | G Loss: 0.9808\n",
      "Epoch [370/5000] | D Loss: 0.7619 | G Loss: 2.0172\n",
      "Epoch [371/5000] | D Loss: 0.6910 | G Loss: 2.3386\n",
      "Epoch [372/5000] | D Loss: 0.7198 | G Loss: 2.1785\n",
      "Epoch [373/5000] | D Loss: 0.6912 | G Loss: 2.2611\n",
      "Epoch [374/5000] | D Loss: 0.6657 | G Loss: 3.1322\n",
      "Epoch [375/5000] | D Loss: 0.8240 | G Loss: 1.4429\n",
      "Epoch [376/5000] | D Loss: 1.5800 | G Loss: 1.7875\n",
      "Epoch [377/5000] | D Loss: 1.1849 | G Loss: 1.8252\n",
      "Epoch [378/5000] | D Loss: 0.8143 | G Loss: 1.8731\n",
      "Epoch [379/5000] | D Loss: 2.6326 | G Loss: 0.2890\n",
      "Epoch [380/5000] | D Loss: 0.7526 | G Loss: 2.6192\n",
      "Epoch [381/5000] | D Loss: 1.1240 | G Loss: 1.8947\n",
      "Epoch [382/5000] | D Loss: 0.6969 | G Loss: 2.5557\n",
      "Epoch [383/5000] | D Loss: 0.8163 | G Loss: 2.1093\n",
      "Epoch [384/5000] | D Loss: 0.7159 | G Loss: 2.8899\n",
      "Epoch [385/5000] | D Loss: 0.6471 | G Loss: 2.7475\n",
      "Epoch [386/5000] | D Loss: 0.6588 | G Loss: 2.3004\n",
      "Epoch [387/5000] | D Loss: 0.7918 | G Loss: 1.9444\n",
      "Epoch [388/5000] | D Loss: 0.7351 | G Loss: 2.0009\n",
      "Epoch [389/5000] | D Loss: 0.8595 | G Loss: 2.2712\n",
      "Epoch [390/5000] | D Loss: 0.9750 | G Loss: 2.0383\n",
      "Epoch [391/5000] | D Loss: 0.6799 | G Loss: 2.2987\n",
      "Epoch [392/5000] | D Loss: 0.6323 | G Loss: 2.8303\n",
      "Epoch [393/5000] | D Loss: 0.6835 | G Loss: 2.3028\n",
      "Epoch [394/5000] | D Loss: 0.7997 | G Loss: 2.1246\n",
      "Epoch [395/5000] | D Loss: 0.8870 | G Loss: 1.8665\n",
      "Epoch [396/5000] | D Loss: 0.6614 | G Loss: 2.4605\n",
      "Epoch [397/5000] | D Loss: 1.1515 | G Loss: 1.5062\n",
      "Epoch [398/5000] | D Loss: 1.1519 | G Loss: 2.4312\n",
      "Epoch [399/5000] | D Loss: 0.7280 | G Loss: 2.6847\n",
      "Epoch [400/5000] | D Loss: 0.9807 | G Loss: 2.1228\n",
      "Epoch 400 FID Score: 188.4872\n",
      "Epoch [401/5000] | D Loss: 0.6046 | G Loss: 2.5809\n",
      "Epoch [402/5000] | D Loss: 1.1288 | G Loss: 1.5789\n",
      "Epoch [403/5000] | D Loss: 0.8818 | G Loss: 1.8203\n",
      "Epoch [404/5000] | D Loss: 0.6782 | G Loss: 3.3431\n",
      "Epoch [405/5000] | D Loss: 0.7470 | G Loss: 2.7523\n",
      "Epoch [406/5000] | D Loss: 1.7915 | G Loss: 1.2380\n",
      "Epoch [407/5000] | D Loss: 0.6943 | G Loss: 2.6324\n",
      "Epoch [408/5000] | D Loss: 0.8606 | G Loss: 2.3758\n",
      "Epoch [409/5000] | D Loss: 0.5934 | G Loss: 3.7703\n",
      "Epoch [410/5000] | D Loss: 0.5869 | G Loss: 2.5845\n",
      "Epoch [411/5000] | D Loss: 0.9778 | G Loss: 1.2512\n",
      "Epoch [412/5000] | D Loss: 5.3971 | G Loss: 0.6712\n",
      "Epoch [413/5000] | D Loss: 0.6957 | G Loss: 2.4769\n",
      "Epoch [414/5000] | D Loss: 0.6690 | G Loss: 2.3967\n",
      "Epoch [415/5000] | D Loss: 0.8201 | G Loss: 1.7127\n",
      "Epoch [416/5000] | D Loss: 0.6168 | G Loss: 2.8145\n",
      "Epoch [417/5000] | D Loss: 0.9004 | G Loss: 2.6526\n",
      "Epoch [418/5000] | D Loss: 0.7837 | G Loss: 1.6559\n",
      "Epoch [419/5000] | D Loss: 0.7811 | G Loss: 2.1559\n",
      "Epoch [420/5000] | D Loss: 1.2239 | G Loss: 1.7244\n",
      "Epoch [421/5000] | D Loss: 0.7330 | G Loss: 2.4817\n",
      "Epoch [422/5000] | D Loss: 0.6916 | G Loss: 1.9948\n",
      "Epoch [423/5000] | D Loss: 0.6516 | G Loss: 2.9612\n",
      "Epoch [424/5000] | D Loss: 0.7985 | G Loss: 1.8392\n",
      "Epoch [425/5000] | D Loss: 0.9741 | G Loss: 1.9615\n",
      "Epoch [426/5000] | D Loss: 0.6606 | G Loss: 3.1879\n",
      "Epoch [427/5000] | D Loss: 1.0101 | G Loss: 1.5912\n",
      "Epoch [428/5000] | D Loss: 1.1357 | G Loss: 2.2330\n",
      "Epoch [429/5000] | D Loss: 0.6629 | G Loss: 3.3676\n",
      "Epoch [430/5000] | D Loss: 0.7999 | G Loss: 1.8093\n",
      "Epoch [431/5000] | D Loss: 1.5411 | G Loss: 1.4937\n",
      "Epoch [432/5000] | D Loss: 0.7936 | G Loss: 2.5634\n",
      "Epoch [433/5000] | D Loss: 0.6350 | G Loss: 2.3017\n",
      "Epoch [434/5000] | D Loss: 0.7002 | G Loss: 2.6922\n",
      "Epoch [435/5000] | D Loss: 1.8645 | G Loss: 1.0307\n",
      "Epoch [436/5000] | D Loss: 0.6645 | G Loss: 2.5525\n",
      "Epoch [437/5000] | D Loss: 0.9314 | G Loss: 1.5593\n",
      "Epoch [438/5000] | D Loss: 0.6986 | G Loss: 2.7868\n",
      "Epoch [439/5000] | D Loss: 1.2415 | G Loss: 0.9338\n",
      "Epoch [440/5000] | D Loss: 0.6401 | G Loss: 2.9885\n",
      "Epoch [441/5000] | D Loss: 0.8931 | G Loss: 1.9858\n",
      "Epoch [442/5000] | D Loss: 0.7904 | G Loss: 2.2555\n",
      "Epoch [443/5000] | D Loss: 0.6926 | G Loss: 3.7915\n",
      "Epoch [444/5000] | D Loss: 0.8309 | G Loss: 2.0368\n",
      "Epoch [445/5000] | D Loss: 0.6873 | G Loss: 2.5876\n",
      "Epoch [446/5000] | D Loss: 0.6127 | G Loss: 3.6777\n",
      "Epoch [447/5000] | D Loss: 0.6722 | G Loss: 2.9401\n",
      "Epoch [448/5000] | D Loss: 1.1922 | G Loss: 2.5264\n",
      "Epoch [449/5000] | D Loss: 0.8734 | G Loss: 1.4172\n",
      "Epoch [450/5000] | D Loss: 0.6545 | G Loss: 2.6511\n",
      "Epoch [451/5000] | D Loss: 0.7675 | G Loss: 2.5127\n",
      "Epoch [452/5000] | D Loss: 1.5383 | G Loss: 1.1731\n",
      "Epoch [453/5000] | D Loss: 0.7234 | G Loss: 2.5515\n",
      "Epoch [454/5000] | D Loss: 0.5925 | G Loss: 2.1217\n",
      "Epoch [455/5000] | D Loss: 0.6981 | G Loss: 2.6404\n",
      "Epoch [456/5000] | D Loss: 0.7535 | G Loss: 2.1959\n",
      "Epoch [457/5000] | D Loss: 1.1888 | G Loss: 1.7079\n",
      "Epoch [458/5000] | D Loss: 0.9960 | G Loss: 2.3251\n",
      "Epoch [459/5000] | D Loss: 0.7495 | G Loss: 2.9166\n",
      "Epoch [460/5000] | D Loss: 0.7207 | G Loss: 2.8096\n",
      "Epoch [461/5000] | D Loss: 1.2141 | G Loss: 2.3008\n",
      "Epoch [462/5000] | D Loss: 0.9070 | G Loss: 2.4251\n",
      "Epoch [463/5000] | D Loss: 0.6208 | G Loss: 3.0614\n",
      "Epoch [464/5000] | D Loss: 0.7634 | G Loss: 2.9234\n",
      "Epoch [465/5000] | D Loss: 0.7200 | G Loss: 2.5888\n",
      "Epoch [466/5000] | D Loss: 0.6836 | G Loss: 2.3092\n",
      "Epoch [467/5000] | D Loss: 0.9358 | G Loss: 2.2165\n",
      "Epoch [468/5000] | D Loss: 0.7384 | G Loss: 2.1756\n",
      "Epoch [469/5000] | D Loss: 0.7772 | G Loss: 2.4368\n",
      "Epoch [470/5000] | D Loss: 1.1912 | G Loss: 0.9498\n",
      "Epoch [471/5000] | D Loss: 0.5763 | G Loss: 3.2610\n",
      "Epoch [472/5000] | D Loss: 0.7160 | G Loss: 2.8514\n",
      "Epoch [473/5000] | D Loss: 0.7299 | G Loss: 2.2544\n",
      "Epoch [474/5000] | D Loss: 0.6228 | G Loss: 4.0798\n",
      "Epoch [475/5000] | D Loss: 0.6821 | G Loss: 2.4537\n",
      "Epoch [476/5000] | D Loss: 0.6362 | G Loss: 2.7256\n",
      "Epoch [477/5000] | D Loss: 0.7306 | G Loss: 2.2108\n",
      "Epoch [478/5000] | D Loss: 0.6803 | G Loss: 3.3555\n",
      "Epoch [479/5000] | D Loss: 0.6397 | G Loss: 2.3302\n",
      "Epoch [480/5000] | D Loss: 0.7777 | G Loss: 2.4367\n",
      "Epoch [481/5000] | D Loss: 0.8915 | G Loss: 2.1157\n",
      "Epoch [482/5000] | D Loss: 0.6387 | G Loss: 2.4798\n",
      "Epoch [483/5000] | D Loss: 0.6225 | G Loss: 2.9890\n",
      "Epoch [484/5000] | D Loss: 1.2396 | G Loss: 0.9665\n",
      "Epoch [485/5000] | D Loss: 0.6259 | G Loss: 3.0560\n",
      "Epoch [486/5000] | D Loss: 1.6920 | G Loss: 0.7900\n",
      "Epoch [487/5000] | D Loss: 0.7594 | G Loss: 1.6899\n",
      "Epoch [488/5000] | D Loss: 0.6062 | G Loss: 3.2739\n",
      "Epoch [489/5000] | D Loss: 5.9204 | G Loss: 0.7847\n",
      "Epoch [490/5000] | D Loss: 0.6728 | G Loss: 2.3184\n",
      "Epoch [491/5000] | D Loss: 0.8680 | G Loss: 2.0013\n",
      "Epoch [492/5000] | D Loss: 0.6444 | G Loss: 2.5807\n",
      "Epoch [493/5000] | D Loss: 0.8189 | G Loss: 1.8696\n",
      "Epoch [494/5000] | D Loss: 0.6878 | G Loss: 2.4376\n",
      "Epoch [495/5000] | D Loss: 0.8699 | G Loss: 1.6338\n",
      "Epoch [496/5000] | D Loss: 0.8116 | G Loss: 1.4736\n",
      "Epoch [497/5000] | D Loss: 0.9614 | G Loss: 2.3334\n",
      "Epoch [498/5000] | D Loss: 0.7671 | G Loss: 1.5517\n",
      "Epoch [499/5000] | D Loss: 0.7380 | G Loss: 2.5553\n",
      "Epoch [500/5000] | D Loss: 0.7026 | G Loss: 2.5068\n",
      "Epoch 500 FID Score: 181.3348\n",
      "Epoch [501/5000] | D Loss: 0.5936 | G Loss: 3.1714\n",
      "Epoch [502/5000] | D Loss: 0.6643 | G Loss: 2.5832\n",
      "Epoch [503/5000] | D Loss: 0.6585 | G Loss: 3.0733\n",
      "Epoch [504/5000] | D Loss: 0.6203 | G Loss: 1.4437\n",
      "Epoch [505/5000] | D Loss: 0.9033 | G Loss: 2.0495\n",
      "Epoch [506/5000] | D Loss: 0.8187 | G Loss: 1.9750\n",
      "Epoch [507/5000] | D Loss: 0.5961 | G Loss: 2.7156\n",
      "Epoch [508/5000] | D Loss: 0.7536 | G Loss: 2.2805\n",
      "Epoch [509/5000] | D Loss: 0.7811 | G Loss: 2.5994\n",
      "Epoch [510/5000] | D Loss: 0.6024 | G Loss: 2.8174\n",
      "Epoch [511/5000] | D Loss: 0.8551 | G Loss: 2.2380\n",
      "Epoch [512/5000] | D Loss: 0.7643 | G Loss: 2.3491\n",
      "Epoch [513/5000] | D Loss: 0.7454 | G Loss: 1.8111\n",
      "Epoch [514/5000] | D Loss: 0.6993 | G Loss: 2.7417\n",
      "Epoch [515/5000] | D Loss: 0.6553 | G Loss: 4.0791\n",
      "Epoch [516/5000] | D Loss: 0.5885 | G Loss: 2.8995\n",
      "Epoch [517/5000] | D Loss: 0.6385 | G Loss: 2.2450\n",
      "Epoch [518/5000] | D Loss: 0.6716 | G Loss: 2.7023\n",
      "Epoch [519/5000] | D Loss: 0.6579 | G Loss: 2.8774\n",
      "Epoch [520/5000] | D Loss: 0.7015 | G Loss: 2.4843\n",
      "Epoch [521/5000] | D Loss: 0.6016 | G Loss: 2.3321\n",
      "Epoch [522/5000] | D Loss: 0.6991 | G Loss: 2.5394\n",
      "Epoch [523/5000] | D Loss: 0.8972 | G Loss: 1.7508\n",
      "Epoch [524/5000] | D Loss: 0.5798 | G Loss: 3.2713\n",
      "Epoch [525/5000] | D Loss: 0.9864 | G Loss: 2.4942\n",
      "Epoch [526/5000] | D Loss: 0.6417 | G Loss: 3.0595\n",
      "Epoch [527/5000] | D Loss: 1.7817 | G Loss: 1.7145\n",
      "Epoch [528/5000] | D Loss: 0.9100 | G Loss: 2.4635\n",
      "Epoch [529/5000] | D Loss: 0.9256 | G Loss: 2.3573\n",
      "Epoch [530/5000] | D Loss: 0.6260 | G Loss: 2.7464\n",
      "Epoch [531/5000] | D Loss: 0.6096 | G Loss: 3.6227\n",
      "Epoch [532/5000] | D Loss: 0.6443 | G Loss: 3.0539\n",
      "Epoch [533/5000] | D Loss: 0.6883 | G Loss: 2.8889\n",
      "Epoch [534/5000] | D Loss: 0.6060 | G Loss: 2.5942\n",
      "Epoch [535/5000] | D Loss: 0.6046 | G Loss: 3.4468\n",
      "Epoch [536/5000] | D Loss: 1.1408 | G Loss: 1.3591\n",
      "Epoch [537/5000] | D Loss: 1.0454 | G Loss: 1.6937\n",
      "Epoch [538/5000] | D Loss: 0.7211 | G Loss: 2.6914\n",
      "Epoch [539/5000] | D Loss: 0.6437 | G Loss: 2.5578\n",
      "Epoch [540/5000] | D Loss: 0.6799 | G Loss: 2.8476\n",
      "Epoch [541/5000] | D Loss: 0.6099 | G Loss: 3.7314\n",
      "Epoch [542/5000] | D Loss: 0.9306 | G Loss: 2.1806\n",
      "Epoch [543/5000] | D Loss: 1.2474 | G Loss: 1.0557\n",
      "Epoch [544/5000] | D Loss: 0.6927 | G Loss: 2.5989\n",
      "Epoch [545/5000] | D Loss: 0.6462 | G Loss: 2.4153\n",
      "Epoch [546/5000] | D Loss: 0.7396 | G Loss: 2.8530\n",
      "Epoch [547/5000] | D Loss: 1.1726 | G Loss: 1.6173\n",
      "Epoch [548/5000] | D Loss: 0.7214 | G Loss: 2.1786\n",
      "Epoch [549/5000] | D Loss: 0.7100 | G Loss: 2.8289\n",
      "Epoch [550/5000] | D Loss: 0.7109 | G Loss: 2.4759\n",
      "Epoch [551/5000] | D Loss: 0.9409 | G Loss: 2.0914\n",
      "Epoch [552/5000] | D Loss: 2.0807 | G Loss: 0.5068\n",
      "Epoch [553/5000] | D Loss: 0.7846 | G Loss: 2.3851\n",
      "Epoch [554/5000] | D Loss: 0.6308 | G Loss: 2.4570\n",
      "Epoch [555/5000] | D Loss: 0.6749 | G Loss: 2.9960\n",
      "Epoch [556/5000] | D Loss: 0.6658 | G Loss: 1.4803\n",
      "Epoch [557/5000] | D Loss: 0.6616 | G Loss: 3.3845\n",
      "Epoch [558/5000] | D Loss: 0.6221 | G Loss: 2.8003\n",
      "Epoch [559/5000] | D Loss: 0.9621 | G Loss: 1.3297\n",
      "Epoch [560/5000] | D Loss: 0.9578 | G Loss: 2.0586\n",
      "Epoch [561/5000] | D Loss: 0.6294 | G Loss: 2.9477\n",
      "Epoch [562/5000] | D Loss: 0.5988 | G Loss: 3.0557\n",
      "Epoch [563/5000] | D Loss: 0.8457 | G Loss: 1.5495\n",
      "Epoch [564/5000] | D Loss: 0.6877 | G Loss: 2.9575\n",
      "Epoch [565/5000] | D Loss: 0.8184 | G Loss: 1.8352\n",
      "Epoch [566/5000] | D Loss: 2.0879 | G Loss: 0.3784\n",
      "Epoch [567/5000] | D Loss: 0.6236 | G Loss: 2.9391\n",
      "Epoch [568/5000] | D Loss: 0.8572 | G Loss: 1.7179\n",
      "Epoch [569/5000] | D Loss: 0.8126 | G Loss: 2.5692\n",
      "Epoch [570/5000] | D Loss: 0.8332 | G Loss: 1.2682\n",
      "Epoch [571/5000] | D Loss: 0.6282 | G Loss: 3.4324\n",
      "Epoch [572/5000] | D Loss: 1.0744 | G Loss: 2.8724\n",
      "Epoch [573/5000] | D Loss: 0.7855 | G Loss: 1.1802\n",
      "Epoch [574/5000] | D Loss: 0.6326 | G Loss: 3.0656\n",
      "Epoch [575/5000] | D Loss: 0.6404 | G Loss: 2.6030\n",
      "Epoch [576/5000] | D Loss: 0.8033 | G Loss: 2.7433\n",
      "Epoch [577/5000] | D Loss: 0.6466 | G Loss: 2.0014\n",
      "Epoch [578/5000] | D Loss: 1.2416 | G Loss: 1.3503\n",
      "Epoch [579/5000] | D Loss: 0.6919 | G Loss: 3.1460\n",
      "Epoch [580/5000] | D Loss: 0.6192 | G Loss: 3.3058\n",
      "Epoch [581/5000] | D Loss: 0.8526 | G Loss: 2.8286\n",
      "Epoch [582/5000] | D Loss: 1.4038 | G Loss: 0.8717\n",
      "Epoch [583/5000] | D Loss: 1.1296 | G Loss: 2.2996\n",
      "Epoch [584/5000] | D Loss: 0.7027 | G Loss: 3.4590\n",
      "Epoch [585/5000] | D Loss: 0.7621 | G Loss: 2.6629\n",
      "Epoch [586/5000] | D Loss: 1.0548 | G Loss: 1.5368\n",
      "Epoch [587/5000] | D Loss: 0.7046 | G Loss: 2.6984\n",
      "Epoch [588/5000] | D Loss: 0.6794 | G Loss: 3.2863\n",
      "Epoch [589/5000] | D Loss: 0.6168 | G Loss: 2.5354\n",
      "Epoch [590/5000] | D Loss: 0.6137 | G Loss: 2.7636\n",
      "Epoch [591/5000] | D Loss: 0.6737 | G Loss: 2.6515\n",
      "Epoch [592/5000] | D Loss: 0.5915 | G Loss: 3.1313\n",
      "Epoch [593/5000] | D Loss: 0.5838 | G Loss: 3.4461\n",
      "Epoch [594/5000] | D Loss: 0.9170 | G Loss: 2.2306\n",
      "Epoch [595/5000] | D Loss: 0.6421 | G Loss: 3.0174\n",
      "Epoch [596/5000] | D Loss: 0.6074 | G Loss: 3.8218\n",
      "Epoch [597/5000] | D Loss: 0.6211 | G Loss: 3.5433\n",
      "Epoch [598/5000] | D Loss: 0.8159 | G Loss: 1.9911\n",
      "Epoch [599/5000] | D Loss: 0.8917 | G Loss: 1.6660\n",
      "Epoch [600/5000] | D Loss: 0.8060 | G Loss: 2.1010\n",
      "Epoch 600 FID Score: 180.9289\n",
      "Epoch [601/5000] | D Loss: 0.6383 | G Loss: 2.8290\n",
      "Epoch [602/5000] | D Loss: 0.5970 | G Loss: 3.4608\n",
      "Epoch [603/5000] | D Loss: 0.6165 | G Loss: 3.3284\n",
      "Epoch [604/5000] | D Loss: 0.8231 | G Loss: 1.7607\n",
      "Epoch [605/5000] | D Loss: 0.6969 | G Loss: 3.2297\n",
      "Epoch [606/5000] | D Loss: 0.6201 | G Loss: 3.5050\n",
      "Epoch [607/5000] | D Loss: 0.6023 | G Loss: 3.1731\n",
      "Epoch [608/5000] | D Loss: 0.6367 | G Loss: 2.5993\n",
      "Epoch [609/5000] | D Loss: 0.7956 | G Loss: 2.7901\n",
      "Epoch [610/5000] | D Loss: 1.0376 | G Loss: 3.0213\n",
      "Epoch [611/5000] | D Loss: 0.8394 | G Loss: 2.6855\n",
      "Epoch [612/5000] | D Loss: 1.5649 | G Loss: 2.8035\n",
      "Epoch [613/5000] | D Loss: 0.7259 | G Loss: 2.8124\n",
      "Epoch [614/5000] | D Loss: 1.6690 | G Loss: 0.7068\n",
      "Epoch [615/5000] | D Loss: 0.9050 | G Loss: 2.1180\n",
      "Epoch [616/5000] | D Loss: 0.8039 | G Loss: 2.6368\n",
      "Epoch [617/5000] | D Loss: 0.6450 | G Loss: 3.0441\n",
      "Epoch [618/5000] | D Loss: 0.6825 | G Loss: 3.4308\n",
      "Epoch [619/5000] | D Loss: 0.7000 | G Loss: 1.9274\n",
      "Epoch [620/5000] | D Loss: 0.6652 | G Loss: 2.7080\n",
      "Epoch [621/5000] | D Loss: 0.6514 | G Loss: 3.1776\n",
      "Epoch [622/5000] | D Loss: 0.7453 | G Loss: 2.1592\n",
      "Epoch [623/5000] | D Loss: 0.7742 | G Loss: 1.9138\n",
      "Epoch [624/5000] | D Loss: 0.7118 | G Loss: 2.0717\n",
      "Epoch [625/5000] | D Loss: 0.5861 | G Loss: 2.7342\n",
      "Epoch [626/5000] | D Loss: 0.6025 | G Loss: 2.9837\n",
      "Epoch [627/5000] | D Loss: 0.5911 | G Loss: 2.8049\n",
      "Epoch [628/5000] | D Loss: 1.0588 | G Loss: 1.5043\n",
      "Epoch [629/5000] | D Loss: 0.8532 | G Loss: 2.1166\n",
      "Epoch [630/5000] | D Loss: 1.9729 | G Loss: 1.9281\n",
      "Epoch [631/5000] | D Loss: 0.7537 | G Loss: 2.2727\n",
      "Epoch [632/5000] | D Loss: 0.6658 | G Loss: 2.5862\n",
      "Epoch [633/5000] | D Loss: 0.5947 | G Loss: 2.2262\n",
      "Epoch [634/5000] | D Loss: 0.7041 | G Loss: 2.9090\n",
      "Epoch [635/5000] | D Loss: 0.7395 | G Loss: 2.9643\n",
      "Epoch [636/5000] | D Loss: 0.6631 | G Loss: 2.5688\n",
      "Epoch [637/5000] | D Loss: 0.5894 | G Loss: 2.7180\n",
      "Epoch [638/5000] | D Loss: 0.6389 | G Loss: 3.2199\n",
      "Epoch [639/5000] | D Loss: 0.8205 | G Loss: 2.1190\n",
      "Epoch [640/5000] | D Loss: 0.5692 | G Loss: 4.0425\n",
      "Epoch [641/5000] | D Loss: 2.1078 | G Loss: 1.5343\n",
      "Epoch [642/5000] | D Loss: 0.5860 | G Loss: 3.5538\n",
      "Epoch [643/5000] | D Loss: 0.6241 | G Loss: 2.9854\n",
      "Epoch [644/5000] | D Loss: 0.5830 | G Loss: 3.9920\n",
      "Epoch [645/5000] | D Loss: 0.9633 | G Loss: 2.3512\n",
      "Epoch [646/5000] | D Loss: 0.6688 | G Loss: 2.1549\n",
      "Epoch [647/5000] | D Loss: 0.6039 | G Loss: 2.5932\n",
      "Epoch [648/5000] | D Loss: 4.5303 | G Loss: 0.2669\n",
      "Epoch [649/5000] | D Loss: 0.6117 | G Loss: 2.6954\n",
      "Epoch [650/5000] | D Loss: 9.5567 | G Loss: 0.3281\n",
      "Epoch [651/5000] | D Loss: 0.8036 | G Loss: 2.8310\n",
      "Epoch [652/5000] | D Loss: 0.5909 | G Loss: 3.5925\n",
      "Epoch [653/5000] | D Loss: 0.7147 | G Loss: 2.4807\n",
      "Epoch [654/5000] | D Loss: 0.6165 | G Loss: 3.1023\n",
      "Epoch [655/5000] | D Loss: 0.6638 | G Loss: 2.2965\n",
      "Epoch [656/5000] | D Loss: 1.1205 | G Loss: 2.3383\n",
      "Epoch [657/5000] | D Loss: 0.5734 | G Loss: 3.0705\n",
      "Epoch [658/5000] | D Loss: 0.5880 | G Loss: 2.7159\n",
      "Epoch [659/5000] | D Loss: 1.4345 | G Loss: 1.2266\n",
      "Epoch [660/5000] | D Loss: 0.5975 | G Loss: 2.6351\n",
      "Epoch [661/5000] | D Loss: 1.1004 | G Loss: 1.4664\n",
      "Epoch [662/5000] | D Loss: 0.6877 | G Loss: 2.0609\n",
      "Epoch [663/5000] | D Loss: 0.6517 | G Loss: 3.2095\n",
      "Epoch [664/5000] | D Loss: 0.6184 | G Loss: 3.1350\n",
      "Epoch [665/5000] | D Loss: 0.6483 | G Loss: 2.6043\n",
      "Epoch [666/5000] | D Loss: 0.6447 | G Loss: 2.8731\n",
      "Epoch [667/5000] | D Loss: 1.8635 | G Loss: 0.3289\n",
      "Epoch [668/5000] | D Loss: 0.8549 | G Loss: 2.3487\n",
      "Epoch [669/5000] | D Loss: 0.6867 | G Loss: 2.1673\n",
      "Epoch [670/5000] | D Loss: 0.6948 | G Loss: 1.9061\n",
      "Epoch [671/5000] | D Loss: 0.5890 | G Loss: 4.0160\n",
      "Epoch [672/5000] | D Loss: 0.5779 | G Loss: 2.9256\n",
      "Epoch [673/5000] | D Loss: 0.6316 | G Loss: 4.5552\n",
      "Epoch [674/5000] | D Loss: 0.8772 | G Loss: 2.3033\n",
      "Epoch [675/5000] | D Loss: 0.8569 | G Loss: 1.5066\n",
      "Epoch [676/5000] | D Loss: 3.4780 | G Loss: 0.4258\n",
      "Epoch [677/5000] | D Loss: 0.6204 | G Loss: 2.7432\n",
      "Epoch [678/5000] | D Loss: 0.6362 | G Loss: 3.0207\n",
      "Epoch [679/5000] | D Loss: 0.8054 | G Loss: 1.9849\n",
      "Epoch [680/5000] | D Loss: 0.8327 | G Loss: 2.4661\n",
      "Epoch [681/5000] | D Loss: 0.8105 | G Loss: 1.9174\n",
      "Epoch [682/5000] | D Loss: 0.9743 | G Loss: 1.9510\n",
      "Epoch [683/5000] | D Loss: 0.6311 | G Loss: 2.7430\n",
      "Epoch [684/5000] | D Loss: 0.8530 | G Loss: 2.6548\n",
      "Epoch [685/5000] | D Loss: 0.6174 | G Loss: 2.8895\n",
      "Epoch [686/5000] | D Loss: 0.9143 | G Loss: 2.0090\n",
      "Epoch [687/5000] | D Loss: 0.6699 | G Loss: 3.1569\n",
      "Epoch [688/5000] | D Loss: 1.0851 | G Loss: 1.6673\n",
      "Epoch [689/5000] | D Loss: 0.6360 | G Loss: 2.9969\n",
      "Epoch [690/5000] | D Loss: 1.3175 | G Loss: 1.9163\n",
      "Epoch [691/5000] | D Loss: 0.6749 | G Loss: 2.4547\n",
      "Epoch [692/5000] | D Loss: 1.1651 | G Loss: 1.9941\n",
      "Epoch [693/5000] | D Loss: 0.6801 | G Loss: 3.0425\n",
      "Epoch [694/5000] | D Loss: 0.6278 | G Loss: 2.7724\n",
      "Epoch [695/5000] | D Loss: 0.6657 | G Loss: 2.9878\n",
      "Epoch [696/5000] | D Loss: 0.6288 | G Loss: 3.1511\n",
      "Epoch [697/5000] | D Loss: 0.5804 | G Loss: 3.4997\n",
      "Epoch [698/5000] | D Loss: 0.8366 | G Loss: 2.1955\n",
      "Epoch [699/5000] | D Loss: 0.6506 | G Loss: 3.2600\n",
      "Epoch [700/5000] | D Loss: 0.6802 | G Loss: 2.2147\n",
      "Epoch 700 FID Score: 182.4067\n",
      "Epoch [701/5000] | D Loss: 1.1979 | G Loss: 2.2244\n",
      "Epoch [702/5000] | D Loss: 0.6281 | G Loss: 2.0883\n",
      "Epoch [703/5000] | D Loss: 1.4147 | G Loss: 1.5113\n",
      "Epoch [704/5000] | D Loss: 0.5844 | G Loss: 3.7128\n",
      "Epoch [705/5000] | D Loss: 1.2125 | G Loss: 1.2676\n",
      "Epoch [706/5000] | D Loss: 0.7273 | G Loss: 2.1706\n",
      "Epoch [707/5000] | D Loss: 0.7100 | G Loss: 3.1335\n",
      "Epoch [708/5000] | D Loss: 0.6809 | G Loss: 2.9340\n",
      "Epoch [709/5000] | D Loss: 0.6710 | G Loss: 2.8096\n",
      "Epoch [710/5000] | D Loss: 0.6276 | G Loss: 2.5490\n",
      "Epoch [711/5000] | D Loss: 0.7720 | G Loss: 2.6612\n",
      "Epoch [712/5000] | D Loss: 0.7747 | G Loss: 2.0587\n",
      "Epoch [713/5000] | D Loss: 0.7050 | G Loss: 2.9114\n",
      "Epoch [714/5000] | D Loss: 0.6727 | G Loss: 2.3101\n",
      "Epoch [715/5000] | D Loss: 0.5997 | G Loss: 3.4678\n",
      "Epoch [716/5000] | D Loss: 0.6616 | G Loss: 2.5240\n",
      "Epoch [717/5000] | D Loss: 1.0185 | G Loss: 1.6797\n",
      "Epoch [718/5000] | D Loss: 0.6827 | G Loss: 2.7423\n",
      "Epoch [719/5000] | D Loss: 0.7016 | G Loss: 2.5557\n",
      "Epoch [720/5000] | D Loss: 0.5681 | G Loss: 2.7710\n",
      "Epoch [721/5000] | D Loss: 0.6850 | G Loss: 2.8862\n",
      "Epoch [722/5000] | D Loss: 0.5970 | G Loss: 2.3522\n",
      "Epoch [723/5000] | D Loss: 0.6271 | G Loss: 3.0617\n",
      "Epoch [724/5000] | D Loss: 1.4340 | G Loss: 2.6213\n",
      "Epoch [725/5000] | D Loss: 0.6238 | G Loss: 2.9968\n",
      "Epoch [726/5000] | D Loss: 0.7010 | G Loss: 2.2872\n",
      "Epoch [727/5000] | D Loss: 0.6765 | G Loss: 3.5532\n",
      "Epoch [728/5000] | D Loss: 0.8481 | G Loss: 2.3413\n",
      "Epoch [729/5000] | D Loss: 0.8235 | G Loss: 1.8383\n",
      "Epoch [730/5000] | D Loss: 0.6014 | G Loss: 3.1867\n",
      "Epoch [731/5000] | D Loss: 0.6268 | G Loss: 3.4856\n",
      "Epoch [732/5000] | D Loss: 0.8083 | G Loss: 2.1612\n",
      "Epoch [733/5000] | D Loss: 0.8271 | G Loss: 2.2501\n",
      "Epoch [734/5000] | D Loss: 0.6725 | G Loss: 1.7514\n",
      "Epoch [735/5000] | D Loss: 0.6907 | G Loss: 2.3430\n",
      "Epoch [736/5000] | D Loss: 0.6316 | G Loss: 2.9046\n",
      "Epoch [737/5000] | D Loss: 0.7432 | G Loss: 1.6857\n",
      "Epoch [738/5000] | D Loss: 0.6740 | G Loss: 2.9654\n",
      "Epoch [739/5000] | D Loss: 0.6279 | G Loss: 2.5233\n",
      "Epoch [740/5000] | D Loss: 0.8188 | G Loss: 2.3307\n",
      "Epoch [741/5000] | D Loss: 0.6602 | G Loss: 3.5705\n",
      "Epoch [742/5000] | D Loss: 0.7247 | G Loss: 2.3427\n",
      "Epoch [743/5000] | D Loss: 0.6060 | G Loss: 2.4389\n",
      "Epoch [744/5000] | D Loss: 0.6616 | G Loss: 2.8725\n",
      "Epoch [745/5000] | D Loss: 0.5866 | G Loss: 3.1119\n",
      "Epoch [746/5000] | D Loss: 0.8165 | G Loss: 1.9649\n",
      "Epoch [747/5000] | D Loss: 0.6816 | G Loss: 2.8046\n",
      "Epoch [748/5000] | D Loss: 0.6197 | G Loss: 2.7644\n",
      "Epoch [749/5000] | D Loss: 0.6340 | G Loss: 3.0573\n",
      "Epoch [750/5000] | D Loss: 0.6178 | G Loss: 2.6781\n",
      "Epoch [751/5000] | D Loss: 0.7933 | G Loss: 1.5002\n",
      "Epoch [752/5000] | D Loss: 0.9214 | G Loss: 2.1804\n",
      "Epoch [753/5000] | D Loss: 0.5893 | G Loss: 2.7450\n",
      "Epoch [754/5000] | D Loss: 0.7215 | G Loss: 2.0775\n",
      "Epoch [755/5000] | D Loss: 0.7290 | G Loss: 2.6229\n",
      "Epoch [756/5000] | D Loss: 0.6600 | G Loss: 2.5209\n",
      "Epoch [757/5000] | D Loss: 0.6329 | G Loss: 2.3170\n",
      "Epoch [758/5000] | D Loss: 0.6544 | G Loss: 2.5972\n",
      "Epoch [759/5000] | D Loss: 0.9471 | G Loss: 2.7752\n",
      "Epoch [760/5000] | D Loss: 1.9544 | G Loss: 0.7196\n",
      "Epoch [761/5000] | D Loss: 0.6170 | G Loss: 2.6959\n",
      "Epoch [762/5000] | D Loss: 0.6338 | G Loss: 2.2350\n",
      "Epoch [763/5000] | D Loss: 0.7260 | G Loss: 1.9803\n",
      "Epoch [764/5000] | D Loss: 0.8031 | G Loss: 1.6047\n",
      "Epoch [765/5000] | D Loss: 0.6831 | G Loss: 2.7894\n",
      "Epoch [766/5000] | D Loss: 0.6448 | G Loss: 2.9496\n",
      "Epoch [767/5000] | D Loss: 0.6549 | G Loss: 2.8609\n",
      "Epoch [768/5000] | D Loss: 0.6174 | G Loss: 3.4921\n",
      "Epoch [769/5000] | D Loss: 0.6570 | G Loss: 2.9750\n",
      "Epoch [770/5000] | D Loss: 0.5740 | G Loss: 3.3693\n",
      "Epoch [771/5000] | D Loss: 0.6126 | G Loss: 1.7131\n",
      "Epoch [772/5000] | D Loss: 0.7772 | G Loss: 2.1599\n",
      "Epoch [773/5000] | D Loss: 0.7145 | G Loss: 3.0718\n",
      "Epoch [774/5000] | D Loss: 0.5958 | G Loss: 3.0783\n",
      "Epoch [775/5000] | D Loss: 0.6493 | G Loss: 2.7984\n",
      "Epoch [776/5000] | D Loss: 0.6065 | G Loss: 3.0714\n",
      "Epoch [777/5000] | D Loss: 0.5893 | G Loss: 3.6241\n",
      "Epoch [778/5000] | D Loss: 0.6236 | G Loss: 3.8230\n",
      "Epoch [779/5000] | D Loss: 1.2967 | G Loss: 1.9805\n",
      "Epoch [780/5000] | D Loss: 0.6742 | G Loss: 3.0570\n",
      "Epoch [781/5000] | D Loss: 0.8186 | G Loss: 3.5917\n",
      "Epoch [782/5000] | D Loss: 0.7298 | G Loss: 2.1496\n",
      "Epoch [783/5000] | D Loss: 0.6610 | G Loss: 2.8964\n",
      "Epoch [784/5000] | D Loss: 0.8497 | G Loss: 2.0589\n",
      "Epoch [785/5000] | D Loss: 0.5876 | G Loss: 2.0933\n",
      "Epoch [786/5000] | D Loss: 0.6357 | G Loss: 2.4028\n",
      "Epoch [787/5000] | D Loss: 0.5999 | G Loss: 3.3316\n",
      "Epoch [788/5000] | D Loss: 0.6215 | G Loss: 3.5864\n",
      "Epoch [789/5000] | D Loss: 0.7153 | G Loss: 2.5169\n",
      "Epoch [790/5000] | D Loss: 0.6440 | G Loss: 2.6358\n",
      "Epoch [791/5000] | D Loss: 5.6630 | G Loss: 0.5464\n",
      "Epoch [792/5000] | D Loss: 0.8328 | G Loss: 2.9368\n",
      "Epoch [793/5000] | D Loss: 0.6822 | G Loss: 2.8849\n",
      "Epoch [794/5000] | D Loss: 0.6393 | G Loss: 3.4476\n",
      "Epoch [795/5000] | D Loss: 0.9974 | G Loss: 3.0346\n",
      "Epoch [796/5000] | D Loss: 0.6415 | G Loss: 3.0030\n",
      "Epoch [797/5000] | D Loss: 0.9200 | G Loss: 2.0079\n",
      "Epoch [798/5000] | D Loss: 0.7211 | G Loss: 2.4928\n",
      "Epoch [799/5000] | D Loss: 0.7381 | G Loss: 1.5150\n",
      "Epoch [800/5000] | D Loss: 0.7648 | G Loss: 2.2347\n",
      "Epoch 800 FID Score: 184.1524\n",
      "Epoch [801/5000] | D Loss: 0.7310 | G Loss: 2.4832\n",
      "Epoch [802/5000] | D Loss: 1.1716 | G Loss: 2.0429\n",
      "Epoch [803/5000] | D Loss: 1.0400 | G Loss: 1.3594\n",
      "Epoch [804/5000] | D Loss: 0.6755 | G Loss: 2.4513\n",
      "Epoch [805/5000] | D Loss: 0.7480 | G Loss: 1.9000\n",
      "Epoch [806/5000] | D Loss: 0.6558 | G Loss: 2.1634\n",
      "Epoch [807/5000] | D Loss: 0.9861 | G Loss: 2.5680\n",
      "Epoch [808/5000] | D Loss: 0.8368 | G Loss: 1.7040\n",
      "Epoch [809/5000] | D Loss: 0.6307 | G Loss: 2.9052\n",
      "Epoch [810/5000] | D Loss: 0.6074 | G Loss: 3.4414\n",
      "Epoch [811/5000] | D Loss: 0.7056 | G Loss: 2.7236\n",
      "Epoch [812/5000] | D Loss: 2.3993 | G Loss: 0.2591\n",
      "Epoch [813/5000] | D Loss: 0.6168 | G Loss: 2.6212\n",
      "Epoch [814/5000] | D Loss: 0.7911 | G Loss: 1.9687\n",
      "Epoch [815/5000] | D Loss: 0.6169 | G Loss: 2.8070\n",
      "Epoch [816/5000] | D Loss: 1.0263 | G Loss: 1.6245\n",
      "Epoch [817/5000] | D Loss: 0.7855 | G Loss: 2.5712\n",
      "Epoch [818/5000] | D Loss: 0.6858 | G Loss: 2.9978\n",
      "Epoch [819/5000] | D Loss: 0.6039 | G Loss: 3.1257\n",
      "Epoch [820/5000] | D Loss: 0.6853 | G Loss: 2.8799\n",
      "Epoch [821/5000] | D Loss: 0.7204 | G Loss: 1.6694\n",
      "Epoch [822/5000] | D Loss: 0.6199 | G Loss: 2.4120\n",
      "Epoch [823/5000] | D Loss: 0.5960 | G Loss: 5.1097\n",
      "Epoch [824/5000] | D Loss: 0.6655 | G Loss: 3.0628\n",
      "Epoch [825/5000] | D Loss: 0.8085 | G Loss: 2.5002\n",
      "Epoch [826/5000] | D Loss: 0.7521 | G Loss: 2.8788\n",
      "Epoch [827/5000] | D Loss: 0.6905 | G Loss: 2.5101\n",
      "Epoch [828/5000] | D Loss: 0.7780 | G Loss: 2.9781\n",
      "Epoch [829/5000] | D Loss: 2.7255 | G Loss: 0.3357\n",
      "Epoch [830/5000] | D Loss: 0.6026 | G Loss: 3.0283\n",
      "Epoch [831/5000] | D Loss: 0.8425 | G Loss: 1.7953\n",
      "Epoch [832/5000] | D Loss: 0.6738 | G Loss: 2.3756\n",
      "Epoch [833/5000] | D Loss: 0.8102 | G Loss: 1.9387\n",
      "Epoch [834/5000] | D Loss: 0.6293 | G Loss: 2.1673\n",
      "Epoch [835/5000] | D Loss: 0.8228 | G Loss: 3.0907\n",
      "Epoch [836/5000] | D Loss: 0.5914 | G Loss: 2.6987\n",
      "Epoch [837/5000] | D Loss: 1.4320 | G Loss: 1.1252\n",
      "Epoch [838/5000] | D Loss: 0.7167 | G Loss: 2.5113\n",
      "Epoch [839/5000] | D Loss: 0.6854 | G Loss: 2.2453\n",
      "Epoch [840/5000] | D Loss: 0.9467 | G Loss: 1.9273\n",
      "Epoch [841/5000] | D Loss: 0.7274 | G Loss: 2.8728\n",
      "Epoch [842/5000] | D Loss: 0.6778 | G Loss: 2.5578\n",
      "Epoch [843/5000] | D Loss: 0.6750 | G Loss: 3.7171\n",
      "Epoch [844/5000] | D Loss: 0.8611 | G Loss: 3.1411\n",
      "Epoch [845/5000] | D Loss: 0.6519 | G Loss: 3.1222\n",
      "Epoch [846/5000] | D Loss: 0.9885 | G Loss: 3.0685\n",
      "Epoch [847/5000] | D Loss: 2.2310 | G Loss: 1.5993\n",
      "Epoch [848/5000] | D Loss: 0.6441 | G Loss: 2.8139\n",
      "Epoch [849/5000] | D Loss: 0.6309 | G Loss: 3.2682\n",
      "Epoch [850/5000] | D Loss: 1.1399 | G Loss: 1.3097\n",
      "Epoch [851/5000] | D Loss: 0.5865 | G Loss: 2.6753\n",
      "Epoch [852/5000] | D Loss: 0.5711 | G Loss: 3.5680\n",
      "Epoch [853/5000] | D Loss: 0.7235 | G Loss: 2.2767\n",
      "Epoch [854/5000] | D Loss: 0.6625 | G Loss: 1.9201\n",
      "Epoch [855/5000] | D Loss: 0.6166 | G Loss: 3.6624\n",
      "Epoch [856/5000] | D Loss: 0.5748 | G Loss: 2.9388\n",
      "Epoch [857/5000] | D Loss: 0.6285 | G Loss: 3.0080\n",
      "Epoch [858/5000] | D Loss: 0.7859 | G Loss: 2.3943\n",
      "Epoch [859/5000] | D Loss: 0.5998 | G Loss: 4.0378\n",
      "Epoch [860/5000] | D Loss: 0.6259 | G Loss: 3.6054\n",
      "Epoch [861/5000] | D Loss: 0.5916 | G Loss: 2.9419\n",
      "Epoch [862/5000] | D Loss: 0.6851 | G Loss: 3.2936\n",
      "Epoch [863/5000] | D Loss: 0.6393 | G Loss: 3.1696\n",
      "Epoch [864/5000] | D Loss: 0.7554 | G Loss: 3.1943\n",
      "Epoch [865/5000] | D Loss: 0.6060 | G Loss: 3.4107\n",
      "Epoch [866/5000] | D Loss: 0.7938 | G Loss: 2.4664\n",
      "Epoch [867/5000] | D Loss: 0.7083 | G Loss: 2.1994\n",
      "Epoch [868/5000] | D Loss: 0.9139 | G Loss: 2.1047\n",
      "Epoch [869/5000] | D Loss: 3.1327 | G Loss: 1.6055\n",
      "Epoch [870/5000] | D Loss: 0.6012 | G Loss: 3.2269\n",
      "Epoch [871/5000] | D Loss: 0.6211 | G Loss: 3.6637\n",
      "Epoch [872/5000] | D Loss: 0.6657 | G Loss: 3.7142\n",
      "Epoch [873/5000] | D Loss: 0.6589 | G Loss: 4.1004\n",
      "Epoch [874/5000] | D Loss: 0.8160 | G Loss: 1.9202\n",
      "Epoch [875/5000] | D Loss: 0.6686 | G Loss: 3.3606\n",
      "Epoch [876/5000] | D Loss: 0.6479 | G Loss: 1.9752\n",
      "Epoch [877/5000] | D Loss: 0.6287 | G Loss: 2.2565\n",
      "Epoch [878/5000] | D Loss: 0.6334 | G Loss: 3.0266\n",
      "Epoch [879/5000] | D Loss: 0.6752 | G Loss: 3.0229\n",
      "Epoch [880/5000] | D Loss: 0.7875 | G Loss: 2.2339\n",
      "Epoch [881/5000] | D Loss: 1.6793 | G Loss: 0.6016\n",
      "Epoch [882/5000] | D Loss: 0.7256 | G Loss: 2.8547\n",
      "Epoch [883/5000] | D Loss: 0.6253 | G Loss: 3.0210\n",
      "Epoch [884/5000] | D Loss: 0.5713 | G Loss: 3.4638\n",
      "Epoch [885/5000] | D Loss: 1.6702 | G Loss: 0.6366\n",
      "Epoch [886/5000] | D Loss: 0.7364 | G Loss: 2.5687\n",
      "Epoch [887/5000] | D Loss: 0.6185 | G Loss: 2.8559\n",
      "Epoch [888/5000] | D Loss: 0.6254 | G Loss: 2.8621\n",
      "Epoch [889/5000] | D Loss: 0.6025 | G Loss: 3.2612\n",
      "Epoch [890/5000] | D Loss: 0.5824 | G Loss: 2.7991\n",
      "Epoch [891/5000] | D Loss: 0.7068 | G Loss: 2.7095\n",
      "Epoch [892/5000] | D Loss: 0.5938 | G Loss: 2.5049\n",
      "Epoch [893/5000] | D Loss: 0.8255 | G Loss: 1.8079\n",
      "Epoch [894/5000] | D Loss: 0.5937 | G Loss: 3.6577\n",
      "Epoch [895/5000] | D Loss: 0.9770 | G Loss: 2.7600\n",
      "Epoch [896/5000] | D Loss: 0.5976 | G Loss: 2.6365\n",
      "Epoch [897/5000] | D Loss: 0.6915 | G Loss: 3.1643\n",
      "Epoch [898/5000] | D Loss: 0.7436 | G Loss: 2.4918\n",
      "Epoch [899/5000] | D Loss: 0.6519 | G Loss: 1.9072\n",
      "Epoch [900/5000] | D Loss: 0.5963 | G Loss: 2.7224\n",
      "Epoch 900 FID Score: 188.0740\n",
      "Epoch [901/5000] | D Loss: 0.6719 | G Loss: 2.4510\n",
      "Epoch [902/5000] | D Loss: 0.6224 | G Loss: 3.5286\n",
      "Epoch [903/5000] | D Loss: 0.5975 | G Loss: 2.6398\n",
      "Epoch [904/5000] | D Loss: 0.6348 | G Loss: 3.1327\n",
      "Epoch [905/5000] | D Loss: 0.6147 | G Loss: 2.5977\n",
      "Epoch [906/5000] | D Loss: 0.6950 | G Loss: 2.5125\n",
      "Epoch [907/5000] | D Loss: 1.4856 | G Loss: 1.8285\n",
      "Epoch [908/5000] | D Loss: 0.7918 | G Loss: 2.4589\n",
      "Epoch [909/5000] | D Loss: 0.6742 | G Loss: 2.4553\n",
      "Epoch [910/5000] | D Loss: 0.6996 | G Loss: 2.2241\n",
      "Epoch [911/5000] | D Loss: 0.5883 | G Loss: 3.4188\n",
      "Epoch [912/5000] | D Loss: 0.6005 | G Loss: 3.5970\n",
      "Epoch [913/5000] | D Loss: 0.6684 | G Loss: 2.8268\n",
      "Epoch [914/5000] | D Loss: 0.5963 | G Loss: 2.9966\n",
      "Epoch [915/5000] | D Loss: 3.7374 | G Loss: 0.2519\n",
      "Epoch [916/5000] | D Loss: 0.6382 | G Loss: 2.9929\n",
      "Epoch [917/5000] | D Loss: 0.7017 | G Loss: 2.8546\n",
      "Epoch [918/5000] | D Loss: 0.5812 | G Loss: 3.1384\n",
      "Epoch [919/5000] | D Loss: 0.7361 | G Loss: 1.9285\n",
      "Epoch [920/5000] | D Loss: 0.6270 | G Loss: 3.3008\n",
      "Epoch [921/5000] | D Loss: 0.6063 | G Loss: 2.5954\n",
      "Epoch [922/5000] | D Loss: 0.6354 | G Loss: 3.3129\n",
      "Epoch [923/5000] | D Loss: 0.6214 | G Loss: 2.8565\n",
      "Epoch [924/5000] | D Loss: 0.6983 | G Loss: 1.9058\n",
      "Epoch [925/5000] | D Loss: 0.7404 | G Loss: 2.5097\n",
      "Epoch [926/5000] | D Loss: 0.7393 | G Loss: 2.5207\n",
      "Epoch [927/5000] | D Loss: 0.5876 | G Loss: 3.5038\n",
      "Epoch [928/5000] | D Loss: 1.0568 | G Loss: 1.9254\n",
      "Epoch [929/5000] | D Loss: 0.6991 | G Loss: 2.2746\n",
      "Epoch [930/5000] | D Loss: 0.6172 | G Loss: 3.4462\n",
      "Epoch [931/5000] | D Loss: 0.6409 | G Loss: 2.7336\n",
      "Epoch [932/5000] | D Loss: 0.5922 | G Loss: 2.4558\n",
      "Epoch [933/5000] | D Loss: 0.6199 | G Loss: 3.1464\n",
      "Epoch [934/5000] | D Loss: 0.5960 | G Loss: 3.7434\n",
      "Epoch [935/5000] | D Loss: 0.6319 | G Loss: 2.9385\n",
      "Epoch [936/5000] | D Loss: 0.8782 | G Loss: 2.1875\n",
      "Epoch [937/5000] | D Loss: 0.5850 | G Loss: 2.8180\n",
      "Epoch [938/5000] | D Loss: 0.6023 | G Loss: 2.7308\n",
      "Epoch [939/5000] | D Loss: 0.7958 | G Loss: 2.0343\n",
      "Epoch [940/5000] | D Loss: 0.5888 | G Loss: 3.4582\n",
      "Epoch [941/5000] | D Loss: 1.1383 | G Loss: 1.0866\n",
      "Epoch [942/5000] | D Loss: 0.6134 | G Loss: 3.7715\n",
      "Epoch [943/5000] | D Loss: 0.5980 | G Loss: 2.9360\n",
      "Epoch [944/5000] | D Loss: 0.7478 | G Loss: 4.0166\n",
      "Epoch [945/5000] | D Loss: 0.6217 | G Loss: 3.0390\n",
      "Epoch [946/5000] | D Loss: 0.6499 | G Loss: 3.3717\n",
      "Epoch [947/5000] | D Loss: 0.5860 | G Loss: 3.3918\n",
      "Epoch [948/5000] | D Loss: 0.6846 | G Loss: 3.0955\n",
      "Epoch [949/5000] | D Loss: 1.7868 | G Loss: 0.7408\n",
      "Epoch [950/5000] | D Loss: 0.7410 | G Loss: 1.9752\n",
      "Epoch [951/5000] | D Loss: 0.5881 | G Loss: 2.7998\n",
      "Epoch [952/5000] | D Loss: 0.6060 | G Loss: 3.0867\n",
      "Epoch [953/5000] | D Loss: 0.6634 | G Loss: 2.6698\n",
      "Epoch [954/5000] | D Loss: 0.7169 | G Loss: 1.4992\n",
      "Epoch [955/5000] | D Loss: 0.5983 | G Loss: 2.4714\n",
      "Epoch [956/5000] | D Loss: 0.6502 | G Loss: 2.7502\n",
      "Epoch [957/5000] | D Loss: 0.6950 | G Loss: 2.5521\n",
      "Epoch [958/5000] | D Loss: 0.5721 | G Loss: 3.7575\n",
      "Epoch [959/5000] | D Loss: 1.8346 | G Loss: 0.9007\n",
      "Epoch [960/5000] | D Loss: 0.7611 | G Loss: 2.4015\n",
      "Epoch [961/5000] | D Loss: 0.6958 | G Loss: 3.6443\n",
      "Epoch [962/5000] | D Loss: 1.4611 | G Loss: 2.1963\n",
      "Epoch [963/5000] | D Loss: 0.9304 | G Loss: 1.3258\n",
      "Epoch [964/5000] | D Loss: 0.6392 | G Loss: 2.2476\n",
      "Epoch [965/5000] | D Loss: 0.7347 | G Loss: 3.5249\n",
      "Epoch [966/5000] | D Loss: 0.5812 | G Loss: 3.1474\n",
      "Epoch [967/5000] | D Loss: 1.1945 | G Loss: 1.9282\n",
      "Epoch [968/5000] | D Loss: 0.6441 | G Loss: 2.4168\n",
      "Epoch [969/5000] | D Loss: 0.5966 | G Loss: 2.7673\n",
      "Epoch [970/5000] | D Loss: 0.6763 | G Loss: 3.1246\n",
      "Epoch [971/5000] | D Loss: 0.6773 | G Loss: 2.7378\n",
      "Epoch [972/5000] | D Loss: 0.7134 | G Loss: 2.2563\n",
      "Epoch [973/5000] | D Loss: 0.7236 | G Loss: 2.4990\n",
      "Epoch [974/5000] | D Loss: 0.6192 | G Loss: 3.5598\n",
      "Epoch [975/5000] | D Loss: 0.8770 | G Loss: 2.5750\n",
      "Epoch [976/5000] | D Loss: 0.6153 | G Loss: 3.2734\n",
      "Epoch [977/5000] | D Loss: 0.6033 | G Loss: 3.3267\n",
      "Epoch [978/5000] | D Loss: 0.6885 | G Loss: 2.1648\n",
      "Epoch [979/5000] | D Loss: 1.0847 | G Loss: 1.2030\n",
      "Epoch [980/5000] | D Loss: 0.6995 | G Loss: 3.2473\n",
      "Epoch [981/5000] | D Loss: 0.5787 | G Loss: 3.2644\n",
      "Epoch [982/5000] | D Loss: 0.6289 | G Loss: 1.7966\n",
      "Epoch [983/5000] | D Loss: 0.6811 | G Loss: 3.2338\n",
      "Epoch [984/5000] | D Loss: 0.6212 | G Loss: 2.5780\n",
      "Epoch [985/5000] | D Loss: 1.0731 | G Loss: 2.4582\n",
      "Epoch [986/5000] | D Loss: 0.6230 | G Loss: 3.2610\n",
      "Epoch [987/5000] | D Loss: 0.6033 | G Loss: 2.8409\n",
      "Epoch [988/5000] | D Loss: 0.6087 | G Loss: 3.2304\n",
      "Epoch [989/5000] | D Loss: 0.7268 | G Loss: 3.0089\n",
      "Epoch [990/5000] | D Loss: 0.9284 | G Loss: 1.7736\n",
      "Epoch [991/5000] | D Loss: 0.6133 | G Loss: 3.1531\n",
      "Epoch [992/5000] | D Loss: 0.6513 | G Loss: 3.7016\n",
      "Epoch [993/5000] | D Loss: 0.9167 | G Loss: 3.6078\n",
      "Epoch [994/5000] | D Loss: 0.5991 | G Loss: 3.5342\n",
      "Epoch [995/5000] | D Loss: 0.6391 | G Loss: 2.5580\n",
      "Epoch [996/5000] | D Loss: 0.6455 | G Loss: 2.3694\n",
      "Epoch [997/5000] | D Loss: 1.0191 | G Loss: 2.0510\n",
      "Epoch [998/5000] | D Loss: 0.9670 | G Loss: 1.5987\n",
      "Epoch [999/5000] | D Loss: 0.6809 | G Loss: 2.4847\n",
      "Epoch [1000/5000] | D Loss: 0.7224 | G Loss: 3.0786\n",
      "Epoch 1000 FID Score: 184.5650\n",
      "Epoch [1001/5000] | D Loss: 0.7913 | G Loss: 2.5465\n",
      "Epoch [1002/5000] | D Loss: 0.8664 | G Loss: 3.4871\n",
      "Epoch [1003/5000] | D Loss: 0.6313 | G Loss: 2.7610\n",
      "Epoch [1004/5000] | D Loss: 0.7152 | G Loss: 2.1683\n",
      "Epoch [1005/5000] | D Loss: 0.5701 | G Loss: 3.4161\n",
      "Epoch [1006/5000] | D Loss: 0.6163 | G Loss: 3.0028\n",
      "Epoch [1007/5000] | D Loss: 0.6213 | G Loss: 2.1964\n",
      "Epoch [1008/5000] | D Loss: 0.6136 | G Loss: 2.8365\n",
      "Epoch [1009/5000] | D Loss: 0.6099 | G Loss: 2.7760\n",
      "Epoch [1010/5000] | D Loss: 0.6210 | G Loss: 3.3254\n",
      "Epoch [1011/5000] | D Loss: 0.6047 | G Loss: 2.5364\n",
      "Epoch [1012/5000] | D Loss: 0.5851 | G Loss: 2.6977\n",
      "Epoch [1013/5000] | D Loss: 0.7670 | G Loss: 2.3529\n",
      "Epoch [1014/5000] | D Loss: 0.9710 | G Loss: 2.3268\n",
      "Epoch [1015/5000] | D Loss: 0.7781 | G Loss: 1.7696\n",
      "Epoch [1016/5000] | D Loss: 0.5985 | G Loss: 2.8867\n",
      "Epoch [1017/5000] | D Loss: 0.6353 | G Loss: 2.2453\n",
      "Epoch [1018/5000] | D Loss: 0.5754 | G Loss: 3.0400\n",
      "Epoch [1019/5000] | D Loss: 0.7994 | G Loss: 2.3442\n",
      "Epoch [1020/5000] | D Loss: 0.9467 | G Loss: 1.5918\n",
      "Epoch [1021/5000] | D Loss: 0.6922 | G Loss: 3.0478\n",
      "Epoch [1022/5000] | D Loss: 0.6399 | G Loss: 2.7843\n",
      "Epoch [1023/5000] | D Loss: 0.6596 | G Loss: 2.9060\n",
      "Epoch [1024/5000] | D Loss: 0.7244 | G Loss: 2.4777\n",
      "Epoch [1025/5000] | D Loss: 0.5859 | G Loss: 3.1172\n",
      "Epoch [1026/5000] | D Loss: 0.6309 | G Loss: 3.3421\n",
      "Epoch [1027/5000] | D Loss: 0.6820 | G Loss: 3.0901\n",
      "Epoch [1028/5000] | D Loss: 0.6620 | G Loss: 2.2702\n",
      "Epoch [1029/5000] | D Loss: 0.8715 | G Loss: 2.5362\n",
      "Epoch [1030/5000] | D Loss: 0.6555 | G Loss: 2.6952\n",
      "Epoch [1031/5000] | D Loss: 0.6617 | G Loss: 2.1893\n",
      "Epoch [1032/5000] | D Loss: 0.6818 | G Loss: 2.1821\n",
      "Epoch [1033/5000] | D Loss: 0.5883 | G Loss: 2.8775\n",
      "Epoch [1034/5000] | D Loss: 0.6600 | G Loss: 2.9523\n",
      "Epoch [1035/5000] | D Loss: 0.8396 | G Loss: 2.4029\n",
      "Epoch [1036/5000] | D Loss: 0.9888 | G Loss: 1.9343\n",
      "Epoch [1037/5000] | D Loss: 0.6211 | G Loss: 2.9070\n",
      "Epoch [1038/5000] | D Loss: 0.6234 | G Loss: 2.0413\n",
      "Epoch [1039/5000] | D Loss: 0.9641 | G Loss: 2.1254\n",
      "Epoch [1040/5000] | D Loss: 0.6151 | G Loss: 3.0415\n",
      "Epoch [1041/5000] | D Loss: 0.6748 | G Loss: 3.0096\n",
      "Epoch [1042/5000] | D Loss: 0.6123 | G Loss: 2.7986\n",
      "Epoch [1043/5000] | D Loss: 0.7884 | G Loss: 2.5201\n",
      "Epoch [1044/5000] | D Loss: 0.5640 | G Loss: 3.3876\n",
      "Epoch [1045/5000] | D Loss: 0.6149 | G Loss: 3.1098\n",
      "Epoch [1046/5000] | D Loss: 0.7151 | G Loss: 2.7546\n",
      "Epoch [1047/5000] | D Loss: 0.6713 | G Loss: 2.0123\n",
      "Epoch [1048/5000] | D Loss: 0.6151 | G Loss: 2.9999\n",
      "Epoch [1049/5000] | D Loss: 4.0203 | G Loss: 0.4092\n",
      "Epoch [1050/5000] | D Loss: 1.2234 | G Loss: 2.9560\n",
      "Epoch [1051/5000] | D Loss: 0.5825 | G Loss: 3.7519\n",
      "Epoch [1052/5000] | D Loss: 0.6496 | G Loss: 2.9478\n",
      "Epoch [1053/5000] | D Loss: 0.8552 | G Loss: 2.9999\n",
      "Epoch [1054/5000] | D Loss: 0.9082 | G Loss: 1.9835\n",
      "Epoch [1055/5000] | D Loss: 0.6027 | G Loss: 3.6307\n",
      "Epoch [1056/5000] | D Loss: 0.6425 | G Loss: 1.5849\n",
      "Epoch [1057/5000] | D Loss: 0.5861 | G Loss: 2.5217\n",
      "Epoch [1058/5000] | D Loss: 0.6172 | G Loss: 3.0410\n",
      "Epoch [1059/5000] | D Loss: 0.6352 | G Loss: 2.1631\n",
      "Epoch [1060/5000] | D Loss: 1.1301 | G Loss: 1.4450\n",
      "Epoch [1061/5000] | D Loss: 0.6787 | G Loss: 2.7218\n",
      "Epoch [1062/5000] | D Loss: 0.6200 | G Loss: 2.7108\n",
      "Epoch [1063/5000] | D Loss: 0.7825 | G Loss: 2.5533\n",
      "Epoch [1064/5000] | D Loss: 1.0530 | G Loss: 2.3654\n",
      "Epoch [1065/5000] | D Loss: 0.7405 | G Loss: 2.5599\n",
      "Epoch [1066/5000] | D Loss: 0.6124 | G Loss: 2.5248\n",
      "Epoch [1067/5000] | D Loss: 0.9318 | G Loss: 1.2559\n",
      "Epoch [1068/5000] | D Loss: 0.6112 | G Loss: 2.5837\n",
      "Epoch [1069/5000] | D Loss: 1.0830 | G Loss: 1.8393\n",
      "Epoch [1070/5000] | D Loss: 0.6349 | G Loss: 3.2512\n",
      "Epoch [1071/5000] | D Loss: 0.7832 | G Loss: 3.2427\n",
      "Epoch [1072/5000] | D Loss: 0.5836 | G Loss: 3.0884\n",
      "Epoch [1073/5000] | D Loss: 0.7003 | G Loss: 2.9917\n",
      "Epoch [1074/5000] | D Loss: 0.9680 | G Loss: 2.1808\n",
      "Epoch [1075/5000] | D Loss: 0.8015 | G Loss: 2.5628\n",
      "Epoch [1076/5000] | D Loss: 0.6126 | G Loss: 2.5582\n",
      "Epoch [1077/5000] | D Loss: 0.5935 | G Loss: 3.5873\n",
      "Epoch [1078/5000] | D Loss: 0.6868 | G Loss: 2.9650\n",
      "Epoch [1079/5000] | D Loss: 0.9330 | G Loss: 2.3261\n",
      "Epoch [1080/5000] | D Loss: 1.1727 | G Loss: 1.1864\n",
      "Epoch [1081/5000] | D Loss: 0.6261 | G Loss: 2.8297\n",
      "Epoch [1082/5000] | D Loss: 6.1480 | G Loss: 0.6819\n",
      "Epoch [1083/5000] | D Loss: 0.7715 | G Loss: 3.2162\n",
      "Epoch [1084/5000] | D Loss: 0.6867 | G Loss: 2.8332\n",
      "Epoch [1085/5000] | D Loss: 1.0558 | G Loss: 2.6600\n",
      "Epoch [1086/5000] | D Loss: 0.6243 | G Loss: 2.6646\n",
      "Epoch [1087/5000] | D Loss: 0.7351 | G Loss: 2.0990\n",
      "Epoch [1088/5000] | D Loss: 0.7751 | G Loss: 2.5529\n",
      "Epoch [1089/5000] | D Loss: 1.2600 | G Loss: 1.2327\n",
      "Epoch [1090/5000] | D Loss: 0.6503 | G Loss: 2.5183\n",
      "Epoch [1091/5000] | D Loss: 0.6246 | G Loss: 3.2546\n",
      "Epoch [1092/5000] | D Loss: 0.6127 | G Loss: 3.3922\n",
      "Epoch [1093/5000] | D Loss: 0.6348 | G Loss: 2.6851\n",
      "Epoch [1094/5000] | D Loss: 0.8269 | G Loss: 1.9925\n",
      "Epoch [1095/5000] | D Loss: 0.6620 | G Loss: 2.9879\n",
      "Epoch [1096/5000] | D Loss: 0.7759 | G Loss: 2.3309\n",
      "Epoch [1097/5000] | D Loss: 0.6054 | G Loss: 3.0939\n",
      "Epoch [1098/5000] | D Loss: 0.7948 | G Loss: 3.0215\n",
      "Epoch [1099/5000] | D Loss: 0.8187 | G Loss: 1.9078\n",
      "Epoch [1100/5000] | D Loss: 0.5867 | G Loss: 3.1290\n",
      "Epoch 1100 FID Score: 179.4903\n",
      "Epoch [1101/5000] | D Loss: 0.7162 | G Loss: 1.8767\n",
      "Epoch [1102/5000] | D Loss: 0.6662 | G Loss: 3.2049\n",
      "Epoch [1103/5000] | D Loss: 0.6308 | G Loss: 3.0068\n",
      "Epoch [1104/5000] | D Loss: 0.6142 | G Loss: 3.4750\n",
      "Epoch [1105/5000] | D Loss: 0.6684 | G Loss: 3.1607\n",
      "Epoch [1106/5000] | D Loss: 0.8935 | G Loss: 2.1285\n",
      "Epoch [1107/5000] | D Loss: 0.7947 | G Loss: 1.4751\n",
      "Epoch [1108/5000] | D Loss: 0.5993 | G Loss: 3.1698\n",
      "Epoch [1109/5000] | D Loss: 1.0329 | G Loss: 2.2864\n",
      "Epoch [1110/5000] | D Loss: 0.8009 | G Loss: 1.3267\n",
      "Epoch [1111/5000] | D Loss: 0.6399 | G Loss: 2.7433\n",
      "Epoch [1112/5000] | D Loss: 0.9260 | G Loss: 2.5066\n",
      "Epoch [1113/5000] | D Loss: 0.9822 | G Loss: 1.7435\n",
      "Epoch [1114/5000] | D Loss: 0.6293 | G Loss: 2.7863\n",
      "Epoch [1115/5000] | D Loss: 0.6786 | G Loss: 2.3364\n",
      "Epoch [1116/5000] | D Loss: 1.0468 | G Loss: 2.6809\n",
      "Epoch [1117/5000] | D Loss: 0.6158 | G Loss: 3.8526\n",
      "Epoch [1118/5000] | D Loss: 0.6292 | G Loss: 3.3548\n",
      "Epoch [1119/5000] | D Loss: 0.5907 | G Loss: 3.4300\n",
      "Epoch [1120/5000] | D Loss: 0.9579 | G Loss: 2.2827\n",
      "Epoch [1121/5000] | D Loss: 0.6422 | G Loss: 3.5612\n",
      "Epoch [1122/5000] | D Loss: 0.6381 | G Loss: 2.4683\n",
      "Epoch [1123/5000] | D Loss: 1.0863 | G Loss: 2.1622\n",
      "Epoch [1124/5000] | D Loss: 0.6522 | G Loss: 3.0227\n",
      "Epoch [1125/5000] | D Loss: 0.6095 | G Loss: 2.7301\n",
      "Epoch [1126/5000] | D Loss: 0.6142 | G Loss: 3.0012\n",
      "Epoch [1127/5000] | D Loss: 2.3864 | G Loss: 0.3851\n",
      "Epoch [1128/5000] | D Loss: 4.7872 | G Loss: 0.3733\n",
      "Epoch [1129/5000] | D Loss: 0.6335 | G Loss: 3.0540\n",
      "Epoch [1130/5000] | D Loss: 0.8029 | G Loss: 2.0163\n",
      "Epoch [1131/5000] | D Loss: 0.6052 | G Loss: 2.6415\n",
      "Epoch [1132/5000] | D Loss: 0.6452 | G Loss: 3.9147\n",
      "Epoch [1133/5000] | D Loss: 0.7434 | G Loss: 2.5095\n",
      "Epoch [1134/5000] | D Loss: 0.6848 | G Loss: 3.2632\n",
      "Epoch [1135/5000] | D Loss: 0.6462 | G Loss: 3.3642\n",
      "Epoch [1136/5000] | D Loss: 0.6226 | G Loss: 2.8718\n",
      "Epoch [1137/5000] | D Loss: 0.5775 | G Loss: 3.2959\n",
      "Epoch [1138/5000] | D Loss: 0.6679 | G Loss: 2.5719\n",
      "Epoch [1139/5000] | D Loss: 0.6760 | G Loss: 2.2604\n",
      "Epoch [1140/5000] | D Loss: 0.6310 | G Loss: 4.2724\n",
      "Epoch [1141/5000] | D Loss: 0.7422 | G Loss: 2.1414\n",
      "Epoch [1142/5000] | D Loss: 0.5804 | G Loss: 3.2080\n",
      "Epoch [1143/5000] | D Loss: 0.5830 | G Loss: 3.0032\n",
      "Epoch [1144/5000] | D Loss: 0.6409 | G Loss: 2.9944\n",
      "Epoch [1145/5000] | D Loss: 0.6287 | G Loss: 3.1527\n",
      "Epoch [1146/5000] | D Loss: 0.7259 | G Loss: 1.4037\n",
      "Epoch [1147/5000] | D Loss: 0.6229 | G Loss: 3.2119\n",
      "Epoch [1148/5000] | D Loss: 0.6259 | G Loss: 1.7420\n",
      "Epoch [1149/5000] | D Loss: 1.3553 | G Loss: 1.9377\n",
      "Epoch [1150/5000] | D Loss: 4.3615 | G Loss: 0.7556\n",
      "Epoch [1151/5000] | D Loss: 1.6694 | G Loss: 1.8808\n",
      "Epoch [1152/5000] | D Loss: 0.6154 | G Loss: 2.7671\n",
      "Epoch [1153/5000] | D Loss: 0.9232 | G Loss: 1.9401\n",
      "Epoch [1154/5000] | D Loss: 2.0043 | G Loss: 0.4944\n",
      "Epoch [1155/5000] | D Loss: 1.2058 | G Loss: 1.3566\n",
      "Epoch [1156/5000] | D Loss: 0.6638 | G Loss: 3.2474\n",
      "Epoch [1157/5000] | D Loss: 0.6041 | G Loss: 2.9079\n",
      "Epoch [1158/5000] | D Loss: 1.0167 | G Loss: 1.5934\n",
      "Epoch [1159/5000] | D Loss: 0.6657 | G Loss: 2.8580\n",
      "Epoch [1160/5000] | D Loss: 0.6880 | G Loss: 2.8620\n",
      "Epoch [1161/5000] | D Loss: 0.7112 | G Loss: 2.7378\n",
      "Epoch [1162/5000] | D Loss: 1.1485 | G Loss: 2.6405\n",
      "Epoch [1163/5000] | D Loss: 0.7015 | G Loss: 2.6771\n",
      "Epoch [1164/5000] | D Loss: 0.6416 | G Loss: 1.3812\n",
      "Epoch [1165/5000] | D Loss: 3.5804 | G Loss: 0.4782\n",
      "Epoch [1166/5000] | D Loss: 0.5922 | G Loss: 3.3822\n",
      "Epoch [1167/5000] | D Loss: 0.6425 | G Loss: 2.3346\n",
      "Epoch [1168/5000] | D Loss: 0.5917 | G Loss: 2.7168\n",
      "Epoch [1169/5000] | D Loss: 0.9115 | G Loss: 1.7259\n",
      "Epoch [1170/5000] | D Loss: 0.6022 | G Loss: 3.8476\n",
      "Epoch [1171/5000] | D Loss: 0.6481 | G Loss: 2.9268\n",
      "Epoch [1172/5000] | D Loss: 0.7238 | G Loss: 2.5482\n",
      "Epoch [1173/5000] | D Loss: 0.7028 | G Loss: 1.9403\n",
      "Epoch [1174/5000] | D Loss: 5.4851 | G Loss: 0.2901\n",
      "Epoch [1175/5000] | D Loss: 0.6566 | G Loss: 3.1289\n",
      "Epoch [1176/5000] | D Loss: 0.7155 | G Loss: 2.6771\n",
      "Epoch [1177/5000] | D Loss: 0.6462 | G Loss: 2.8081\n",
      "Epoch [1178/5000] | D Loss: 0.7917 | G Loss: 2.3691\n",
      "Epoch [1179/5000] | D Loss: 0.8121 | G Loss: 2.2093\n",
      "Epoch [1180/5000] | D Loss: 0.6025 | G Loss: 3.7429\n",
      "Epoch [1181/5000] | D Loss: 0.8720 | G Loss: 2.1702\n",
      "Epoch [1182/5000] | D Loss: 0.6131 | G Loss: 3.2946\n",
      "Epoch [1183/5000] | D Loss: 0.7867 | G Loss: 3.0223\n",
      "Epoch [1184/5000] | D Loss: 0.6386 | G Loss: 2.7794\n",
      "Epoch [1185/5000] | D Loss: 0.5748 | G Loss: 2.4388\n",
      "Epoch [1186/5000] | D Loss: 2.8226 | G Loss: 0.7246\n",
      "Epoch [1187/5000] | D Loss: 0.6288 | G Loss: 3.4002\n",
      "Epoch [1188/5000] | D Loss: 7.0350 | G Loss: 0.3362\n",
      "Epoch [1189/5000] | D Loss: 1.2970 | G Loss: 1.4572\n",
      "Epoch [1190/5000] | D Loss: 2.6109 | G Loss: 0.3248\n",
      "Epoch [1191/5000] | D Loss: 0.5808 | G Loss: 3.2377\n",
      "Epoch [1192/5000] | D Loss: 0.5967 | G Loss: 2.9588\n",
      "Epoch [1193/5000] | D Loss: 0.7832 | G Loss: 2.5568\n",
      "Epoch [1194/5000] | D Loss: 0.7106 | G Loss: 2.3878\n",
      "Epoch [1195/5000] | D Loss: 0.9871 | G Loss: 3.0547\n",
      "Epoch [1196/5000] | D Loss: 0.6251 | G Loss: 3.0902\n",
      "Epoch [1197/5000] | D Loss: 0.8190 | G Loss: 2.7731\n",
      "Epoch [1198/5000] | D Loss: 0.5716 | G Loss: 3.3343\n",
      "Epoch [1199/5000] | D Loss: 0.5677 | G Loss: 3.6591\n",
      "Epoch [1200/5000] | D Loss: 1.1289 | G Loss: 1.3515\n",
      "Epoch 1200 FID Score: 184.1599\n",
      "Epoch [1201/5000] | D Loss: 0.6653 | G Loss: 1.9234\n",
      "Epoch [1202/5000] | D Loss: 0.6440 | G Loss: 3.3296\n",
      "Epoch [1203/5000] | D Loss: 0.6025 | G Loss: 2.6770\n",
      "Epoch [1204/5000] | D Loss: 0.6890 | G Loss: 2.4989\n",
      "Epoch [1205/5000] | D Loss: 0.6878 | G Loss: 2.0208\n",
      "Epoch [1206/5000] | D Loss: 0.7654 | G Loss: 2.0076\n",
      "Epoch [1207/5000] | D Loss: 0.6015 | G Loss: 4.8073\n",
      "Epoch [1208/5000] | D Loss: 0.6392 | G Loss: 2.9156\n",
      "Epoch [1209/5000] | D Loss: 0.7845 | G Loss: 2.4595\n",
      "Epoch [1210/5000] | D Loss: 1.1818 | G Loss: 1.7044\n",
      "Epoch [1211/5000] | D Loss: 1.3528 | G Loss: 1.9104\n",
      "Epoch [1212/5000] | D Loss: 0.5893 | G Loss: 2.9984\n",
      "Epoch [1213/5000] | D Loss: 0.7844 | G Loss: 2.2985\n",
      "Epoch [1214/5000] | D Loss: 0.7351 | G Loss: 2.5612\n",
      "Epoch [1215/5000] | D Loss: 0.6806 | G Loss: 3.0899\n",
      "Epoch [1216/5000] | D Loss: 0.9554 | G Loss: 2.0187\n",
      "Epoch [1217/5000] | D Loss: 0.9633 | G Loss: 1.9187\n",
      "Epoch [1218/5000] | D Loss: 0.6446 | G Loss: 2.5594\n",
      "Epoch [1219/5000] | D Loss: 0.5828 | G Loss: 3.5229\n",
      "Epoch [1220/5000] | D Loss: 0.6373 | G Loss: 2.3774\n",
      "Epoch [1221/5000] | D Loss: 0.5870 | G Loss: 3.2278\n",
      "Epoch [1222/5000] | D Loss: 0.5621 | G Loss: 2.7129\n",
      "Epoch [1223/5000] | D Loss: 0.7022 | G Loss: 2.7958\n",
      "Epoch [1224/5000] | D Loss: 0.6379 | G Loss: 2.7094\n",
      "Epoch [1225/5000] | D Loss: 1.1353 | G Loss: 2.2824\n",
      "Epoch [1226/5000] | D Loss: 0.7072 | G Loss: 2.7537\n",
      "Epoch [1227/5000] | D Loss: 1.0063 | G Loss: 2.0153\n",
      "Epoch [1228/5000] | D Loss: 0.6497 | G Loss: 3.4594\n",
      "Epoch [1229/5000] | D Loss: 1.5945 | G Loss: 2.0548\n",
      "Epoch [1230/5000] | D Loss: 0.5921 | G Loss: 2.8491\n",
      "Epoch [1231/5000] | D Loss: 0.6345 | G Loss: 2.8125\n",
      "Epoch [1232/5000] | D Loss: 0.6521 | G Loss: 3.0208\n",
      "Epoch [1233/5000] | D Loss: 0.7025 | G Loss: 3.0522\n",
      "Epoch [1234/5000] | D Loss: 0.6526 | G Loss: 2.3697\n",
      "Epoch [1235/5000] | D Loss: 0.6025 | G Loss: 2.8641\n",
      "Epoch [1236/5000] | D Loss: 2.6846 | G Loss: 0.5697\n",
      "Epoch [1237/5000] | D Loss: 0.6342 | G Loss: 2.6812\n",
      "Epoch [1238/5000] | D Loss: 0.6094 | G Loss: 3.7086\n",
      "Epoch [1239/5000] | D Loss: 0.5935 | G Loss: 3.1663\n",
      "Epoch [1240/5000] | D Loss: 0.8998 | G Loss: 2.9000\n",
      "Epoch [1241/5000] | D Loss: 0.6055 | G Loss: 3.0338\n",
      "Epoch [1242/5000] | D Loss: 0.6231 | G Loss: 3.7694\n",
      "Epoch [1243/5000] | D Loss: 1.2238 | G Loss: 1.2952\n",
      "Epoch [1244/5000] | D Loss: 0.5957 | G Loss: 3.2254\n",
      "Epoch [1245/5000] | D Loss: 0.7118 | G Loss: 2.4270\n",
      "Epoch [1246/5000] | D Loss: 0.6871 | G Loss: 2.5881\n",
      "Epoch [1247/5000] | D Loss: 0.6753 | G Loss: 2.4302\n",
      "Epoch [1248/5000] | D Loss: 0.6165 | G Loss: 2.6611\n",
      "Epoch [1249/5000] | D Loss: 0.6840 | G Loss: 3.0448\n",
      "Epoch [1250/5000] | D Loss: 0.9303 | G Loss: 1.6418\n",
      "Epoch [1251/5000] | D Loss: 0.6387 | G Loss: 2.4940\n",
      "Epoch [1252/5000] | D Loss: 0.6613 | G Loss: 2.4878\n",
      "Epoch [1253/5000] | D Loss: 0.7748 | G Loss: 2.3934\n",
      "Epoch [1254/5000] | D Loss: 0.7701 | G Loss: 2.7988\n",
      "Epoch [1255/5000] | D Loss: 0.7250 | G Loss: 2.0094\n",
      "Epoch [1256/5000] | D Loss: 0.5865 | G Loss: 3.8207\n",
      "Epoch [1257/5000] | D Loss: 0.6808 | G Loss: 1.9310\n",
      "Epoch [1258/5000] | D Loss: 0.6619 | G Loss: 2.5793\n",
      "Epoch [1259/5000] | D Loss: 0.7303 | G Loss: 2.4448\n",
      "Epoch [1260/5000] | D Loss: 0.6557 | G Loss: 2.3398\n",
      "Epoch [1261/5000] | D Loss: 0.6028 | G Loss: 3.0426\n",
      "Epoch [1262/5000] | D Loss: 0.6578 | G Loss: 2.6179\n",
      "Epoch [1263/5000] | D Loss: 0.9606 | G Loss: 2.4431\n",
      "Epoch [1264/5000] | D Loss: 0.9250 | G Loss: 1.2902\n",
      "Epoch [1265/5000] | D Loss: 0.7373 | G Loss: 2.4612\n",
      "Epoch [1266/5000] | D Loss: 0.5834 | G Loss: 3.6629\n",
      "Epoch [1267/5000] | D Loss: 1.2806 | G Loss: 2.4782\n",
      "Epoch [1268/5000] | D Loss: 0.6170 | G Loss: 2.5256\n",
      "Epoch [1269/5000] | D Loss: 1.0230 | G Loss: 2.8420\n",
      "Epoch [1270/5000] | D Loss: 0.6247 | G Loss: 3.2591\n",
      "Epoch [1271/5000] | D Loss: 0.6229 | G Loss: 2.7932\n",
      "Epoch [1272/5000] | D Loss: 0.6745 | G Loss: 2.6488\n",
      "Epoch [1273/5000] | D Loss: 0.6222 | G Loss: 1.8581\n",
      "Epoch [1274/5000] | D Loss: 0.6136 | G Loss: 2.7268\n",
      "Epoch [1275/5000] | D Loss: 0.7436 | G Loss: 2.7022\n",
      "Epoch [1276/5000] | D Loss: 0.8002 | G Loss: 2.3352\n",
      "Epoch [1277/5000] | D Loss: 0.5716 | G Loss: 3.7404\n",
      "Epoch [1278/5000] | D Loss: 1.0415 | G Loss: 1.9630\n",
      "Epoch [1279/5000] | D Loss: 0.9863 | G Loss: 2.3872\n",
      "Epoch [1280/5000] | D Loss: 1.1893 | G Loss: 1.7083\n",
      "Epoch [1281/5000] | D Loss: 0.7269 | G Loss: 1.8174\n",
      "Epoch [1282/5000] | D Loss: 0.6897 | G Loss: 3.3119\n",
      "Epoch [1283/5000] | D Loss: 1.1432 | G Loss: 2.4230\n",
      "Epoch [1284/5000] | D Loss: 0.6742 | G Loss: 2.4355\n",
      "Epoch [1285/5000] | D Loss: 0.5946 | G Loss: 2.2239\n",
      "Epoch [1286/5000] | D Loss: 0.6848 | G Loss: 2.4964\n",
      "Epoch [1287/5000] | D Loss: 0.6285 | G Loss: 2.4105\n",
      "Epoch [1288/5000] | D Loss: 0.6263 | G Loss: 2.6179\n",
      "Epoch [1289/5000] | D Loss: 0.6899 | G Loss: 2.7291\n",
      "Epoch [1290/5000] | D Loss: 0.7323 | G Loss: 2.2252\n",
      "Epoch [1291/5000] | D Loss: 0.7180 | G Loss: 2.5302\n",
      "Epoch [1292/5000] | D Loss: 0.5932 | G Loss: 3.0345\n",
      "Epoch [1293/5000] | D Loss: 0.6923 | G Loss: 3.1754\n",
      "Epoch [1294/5000] | D Loss: 0.6893 | G Loss: 3.0991\n",
      "Epoch [1295/5000] | D Loss: 0.5686 | G Loss: 2.9494\n",
      "Epoch [1296/5000] | D Loss: 0.7962 | G Loss: 1.2464\n",
      "Epoch [1297/5000] | D Loss: 0.6443 | G Loss: 2.6204\n",
      "Epoch [1298/5000] | D Loss: 0.6037 | G Loss: 3.1778\n",
      "Epoch [1299/5000] | D Loss: 0.6508 | G Loss: 2.2992\n",
      "Epoch [1300/5000] | D Loss: 0.7907 | G Loss: 1.8061\n",
      "Epoch 1300 FID Score: 179.6449\n",
      "Epoch [1301/5000] | D Loss: 0.6336 | G Loss: 2.4821\n",
      "Epoch [1302/5000] | D Loss: 0.7010 | G Loss: 3.1667\n",
      "Epoch [1303/5000] | D Loss: 0.6337 | G Loss: 2.4269\n",
      "Epoch [1304/5000] | D Loss: 0.6801 | G Loss: 2.9219\n",
      "Epoch [1305/5000] | D Loss: 0.6117 | G Loss: 3.0484\n",
      "Epoch [1306/5000] | D Loss: 0.7059 | G Loss: 3.3748\n",
      "Epoch [1307/5000] | D Loss: 1.8630 | G Loss: 0.5661\n",
      "Epoch [1308/5000] | D Loss: 0.6852 | G Loss: 2.7933\n",
      "Epoch [1309/5000] | D Loss: 0.6595 | G Loss: 4.0626\n",
      "Epoch [1310/5000] | D Loss: 0.6300 | G Loss: 2.3535\n",
      "Epoch [1311/5000] | D Loss: 0.6117 | G Loss: 2.8505\n",
      "Epoch [1312/5000] | D Loss: 1.0844 | G Loss: 1.4677\n",
      "Epoch [1313/5000] | D Loss: 0.6565 | G Loss: 2.6352\n",
      "Epoch [1314/5000] | D Loss: 0.6364 | G Loss: 3.2594\n",
      "Epoch [1315/5000] | D Loss: 0.9888 | G Loss: 1.1278\n",
      "Epoch [1316/5000] | D Loss: 0.5833 | G Loss: 1.8056\n",
      "Epoch [1317/5000] | D Loss: 0.7922 | G Loss: 2.4151\n",
      "Epoch [1318/5000] | D Loss: 0.5950 | G Loss: 2.8516\n",
      "Epoch [1319/5000] | D Loss: 0.6791 | G Loss: 3.1913\n",
      "Epoch [1320/5000] | D Loss: 0.6071 | G Loss: 1.9313\n",
      "Epoch [1321/5000] | D Loss: 0.6391 | G Loss: 3.0874\n",
      "Epoch [1322/5000] | D Loss: 0.7303 | G Loss: 2.6866\n",
      "Epoch [1323/5000] | D Loss: 0.6954 | G Loss: 2.2269\n",
      "Epoch [1324/5000] | D Loss: 0.6484 | G Loss: 4.5346\n",
      "Epoch [1325/5000] | D Loss: 0.6662 | G Loss: 2.3671\n",
      "Epoch [1326/5000] | D Loss: 0.6126 | G Loss: 2.8000\n",
      "Epoch [1327/5000] | D Loss: 1.1435 | G Loss: 1.9319\n",
      "Epoch [1328/5000] | D Loss: 0.6142 | G Loss: 3.4127\n",
      "Epoch [1329/5000] | D Loss: 0.6274 | G Loss: 2.8481\n",
      "Epoch [1330/5000] | D Loss: 0.7817 | G Loss: 1.3841\n",
      "Epoch [1331/5000] | D Loss: 0.5927 | G Loss: 4.0410\n",
      "Epoch [1332/5000] | D Loss: 0.5834 | G Loss: 2.9717\n",
      "Epoch [1333/5000] | D Loss: 0.6244 | G Loss: 2.8956\n",
      "Epoch [1334/5000] | D Loss: 0.5806 | G Loss: 3.0044\n",
      "Epoch [1335/5000] | D Loss: 0.5658 | G Loss: 2.9393\n",
      "Epoch [1336/5000] | D Loss: 1.1478 | G Loss: 2.1189\n",
      "Epoch [1337/5000] | D Loss: 0.6289 | G Loss: 3.4765\n",
      "Epoch [1338/5000] | D Loss: 0.7309 | G Loss: 2.9328\n",
      "Epoch [1339/5000] | D Loss: 0.5936 | G Loss: 3.0778\n",
      "Epoch [1340/5000] | D Loss: 0.6171 | G Loss: 3.6102\n",
      "Epoch [1341/5000] | D Loss: 0.9293 | G Loss: 1.9945\n",
      "Epoch [1342/5000] | D Loss: 0.6049 | G Loss: 3.2116\n",
      "Epoch [1343/5000] | D Loss: 0.5957 | G Loss: 3.5870\n",
      "Epoch [1344/5000] | D Loss: 0.6558 | G Loss: 2.5899\n",
      "Epoch [1345/5000] | D Loss: 0.9788 | G Loss: 2.1688\n",
      "Epoch [1346/5000] | D Loss: 0.6293 | G Loss: 3.7896\n",
      "Epoch [1347/5000] | D Loss: 0.5921 | G Loss: 4.0272\n",
      "Epoch [1348/5000] | D Loss: 2.0083 | G Loss: 0.7553\n",
      "Epoch [1349/5000] | D Loss: 0.6599 | G Loss: 2.9546\n",
      "Epoch [1350/5000] | D Loss: 0.6345 | G Loss: 2.6464\n",
      "Epoch [1351/5000] | D Loss: 1.2933 | G Loss: 0.9918\n",
      "Epoch [1352/5000] | D Loss: 0.6467 | G Loss: 2.9501\n",
      "Epoch [1353/5000] | D Loss: 3.3853 | G Loss: 0.5780\n",
      "Epoch [1354/5000] | D Loss: 0.5998 | G Loss: 2.9823\n",
      "Epoch [1355/5000] | D Loss: 0.7530 | G Loss: 3.2933\n",
      "Epoch [1356/5000] | D Loss: 0.9231 | G Loss: 1.9020\n",
      "Epoch [1357/5000] | D Loss: 0.7603 | G Loss: 2.0555\n",
      "Epoch [1358/5000] | D Loss: 0.7584 | G Loss: 1.6632\n",
      "Epoch [1359/5000] | D Loss: 0.6307 | G Loss: 3.9798\n",
      "Epoch [1360/5000] | D Loss: 0.6409 | G Loss: 2.5185\n",
      "Epoch [1361/5000] | D Loss: 0.6194 | G Loss: 3.4429\n",
      "Epoch [1362/5000] | D Loss: 0.9384 | G Loss: 1.0787\n",
      "Epoch [1363/5000] | D Loss: 0.6005 | G Loss: 2.1347\n",
      "Epoch [1364/5000] | D Loss: 0.8497 | G Loss: 2.8913\n",
      "Epoch [1365/5000] | D Loss: 0.6526 | G Loss: 2.8802\n",
      "Epoch [1366/5000] | D Loss: 0.6787 | G Loss: 3.5064\n",
      "Epoch [1367/5000] | D Loss: 0.7009 | G Loss: 1.8760\n",
      "Epoch [1368/5000] | D Loss: 2.7118 | G Loss: 1.7685\n",
      "Epoch [1369/5000] | D Loss: 1.0029 | G Loss: 1.7573\n",
      "Epoch [1370/5000] | D Loss: 0.7545 | G Loss: 2.6035\n",
      "Epoch [1371/5000] | D Loss: 0.6809 | G Loss: 2.3475\n",
      "Epoch [1372/5000] | D Loss: 0.5805 | G Loss: 3.2481\n",
      "Epoch [1373/5000] | D Loss: 0.6758 | G Loss: 2.7765\n",
      "Epoch [1374/5000] | D Loss: 0.6308 | G Loss: 3.4834\n",
      "Epoch [1375/5000] | D Loss: 0.6462 | G Loss: 2.9269\n",
      "Epoch [1376/5000] | D Loss: 0.6170 | G Loss: 2.1236\n",
      "Epoch [1377/5000] | D Loss: 0.6957 | G Loss: 2.6816\n",
      "Epoch [1378/5000] | D Loss: 0.5869 | G Loss: 2.9554\n",
      "Epoch [1379/5000] | D Loss: 0.7206 | G Loss: 2.8546\n",
      "Epoch [1380/5000] | D Loss: 0.8233 | G Loss: 2.8348\n",
      "Epoch [1381/5000] | D Loss: 0.6086 | G Loss: 2.9429\n",
      "Epoch [1382/5000] | D Loss: 0.6698 | G Loss: 2.1644\n",
      "Epoch [1383/5000] | D Loss: 0.8150 | G Loss: 2.3156\n",
      "Epoch [1384/5000] | D Loss: 0.7133 | G Loss: 1.9738\n",
      "Epoch [1385/5000] | D Loss: 0.5946 | G Loss: 2.9744\n",
      "Epoch [1386/5000] | D Loss: 0.6955 | G Loss: 2.9040\n",
      "Epoch [1387/5000] | D Loss: 0.9425 | G Loss: 1.6239\n",
      "Epoch [1388/5000] | D Loss: 0.7311 | G Loss: 3.4133\n",
      "Epoch [1389/5000] | D Loss: 0.8470 | G Loss: 2.3807\n",
      "Epoch [1390/5000] | D Loss: 0.7504 | G Loss: 2.2225\n",
      "Epoch [1391/5000] | D Loss: 0.7204 | G Loss: 2.6329\n",
      "Epoch [1392/5000] | D Loss: 0.7672 | G Loss: 1.9716\n",
      "Epoch [1393/5000] | D Loss: 0.8727 | G Loss: 2.6006\n",
      "Epoch [1394/5000] | D Loss: 0.7658 | G Loss: 1.7407\n",
      "Epoch [1395/5000] | D Loss: 0.6725 | G Loss: 3.2202\n",
      "Epoch [1396/5000] | D Loss: 0.7271 | G Loss: 2.3273\n",
      "Epoch [1397/5000] | D Loss: 0.6978 | G Loss: 2.1356\n",
      "Epoch [1398/5000] | D Loss: 0.7972 | G Loss: 2.1867\n",
      "Epoch [1399/5000] | D Loss: 0.7389 | G Loss: 3.0115\n",
      "Epoch [1400/5000] | D Loss: 1.3149 | G Loss: 1.1001\n",
      "Epoch 1400 FID Score: 180.2671\n",
      "Epoch [1401/5000] | D Loss: 0.6876 | G Loss: 1.5575\n",
      "Epoch [1402/5000] | D Loss: 1.1342 | G Loss: 2.0245\n",
      "Epoch [1403/5000] | D Loss: 4.0821 | G Loss: 1.3867\n",
      "Epoch [1404/5000] | D Loss: 0.7219 | G Loss: 2.7750\n",
      "Epoch [1405/5000] | D Loss: 0.6287 | G Loss: 2.3421\n",
      "Epoch [1406/5000] | D Loss: 0.5851 | G Loss: 2.3626\n",
      "Epoch [1407/5000] | D Loss: 0.6069 | G Loss: 3.9189\n",
      "Epoch [1408/5000] | D Loss: 0.7028 | G Loss: 2.3629\n",
      "Epoch [1409/5000] | D Loss: 0.6079 | G Loss: 3.2197\n",
      "Epoch [1410/5000] | D Loss: 0.7363 | G Loss: 1.7962\n",
      "Epoch [1411/5000] | D Loss: 0.6193 | G Loss: 2.9792\n",
      "Epoch [1412/5000] | D Loss: 0.6402 | G Loss: 2.2916\n",
      "Epoch [1413/5000] | D Loss: 0.6060 | G Loss: 2.9531\n",
      "Epoch [1414/5000] | D Loss: 0.5960 | G Loss: 3.0975\n",
      "Epoch [1415/5000] | D Loss: 0.7034 | G Loss: 2.0839\n",
      "Epoch [1416/5000] | D Loss: 1.1073 | G Loss: 2.0773\n",
      "Epoch [1417/5000] | D Loss: 0.6141 | G Loss: 2.4839\n",
      "Epoch [1418/5000] | D Loss: 0.6814 | G Loss: 2.9449\n",
      "Epoch [1419/5000] | D Loss: 0.5839 | G Loss: 2.6516\n",
      "Epoch [1420/5000] | D Loss: 0.6109 | G Loss: 2.3226\n",
      "Epoch [1421/5000] | D Loss: 1.6942 | G Loss: 0.5548\n",
      "Epoch [1422/5000] | D Loss: 0.8487 | G Loss: 2.5575\n",
      "Epoch [1423/5000] | D Loss: 0.5955 | G Loss: 3.0272\n",
      "Epoch [1424/5000] | D Loss: 0.6950 | G Loss: 1.7288\n",
      "Epoch [1425/5000] | D Loss: 0.7337 | G Loss: 1.8347\n",
      "Epoch [1426/5000] | D Loss: 0.6261 | G Loss: 3.1186\n",
      "Epoch [1427/5000] | D Loss: 0.7942 | G Loss: 2.4249\n",
      "Epoch [1428/5000] | D Loss: 0.6107 | G Loss: 2.2823\n",
      "Epoch [1429/5000] | D Loss: 6.0993 | G Loss: 0.3594\n",
      "Epoch [1430/5000] | D Loss: 0.6356 | G Loss: 2.4701\n",
      "Epoch [1431/5000] | D Loss: 0.6803 | G Loss: 2.5970\n",
      "Epoch [1432/5000] | D Loss: 0.6220 | G Loss: 2.0998\n",
      "Epoch [1433/5000] | D Loss: 0.5870 | G Loss: 3.3239\n",
      "Epoch [1434/5000] | D Loss: 0.5794 | G Loss: 2.9080\n",
      "Epoch [1435/5000] | D Loss: 0.6309 | G Loss: 2.9885\n",
      "Epoch [1436/5000] | D Loss: 0.5732 | G Loss: 3.5840\n",
      "Epoch [1437/5000] | D Loss: 0.5845 | G Loss: 4.0301\n",
      "Epoch [1438/5000] | D Loss: 0.6862 | G Loss: 1.7114\n",
      "Epoch [1439/5000] | D Loss: 0.6582 | G Loss: 3.6397\n",
      "Epoch [1440/5000] | D Loss: 0.7182 | G Loss: 2.1945\n",
      "Epoch [1441/5000] | D Loss: 1.4730 | G Loss: 1.7325\n",
      "Epoch [1442/5000] | D Loss: 0.7474 | G Loss: 2.5321\n",
      "Epoch [1443/5000] | D Loss: 1.1118 | G Loss: 1.8829\n",
      "Epoch [1444/5000] | D Loss: 0.5990 | G Loss: 2.9419\n",
      "Epoch [1445/5000] | D Loss: 0.5551 | G Loss: 3.0740\n",
      "Epoch [1446/5000] | D Loss: 0.6203 | G Loss: 2.4543\n",
      "Epoch [1447/5000] | D Loss: 0.6770 | G Loss: 2.2852\n",
      "Epoch [1448/5000] | D Loss: 0.6790 | G Loss: 2.3776\n",
      "Epoch [1449/5000] | D Loss: 0.6474 | G Loss: 2.7725\n",
      "Epoch [1450/5000] | D Loss: 0.6140 | G Loss: 3.2650\n",
      "Epoch [1451/5000] | D Loss: 0.7663 | G Loss: 3.1995\n",
      "Epoch [1452/5000] | D Loss: 0.6571 | G Loss: 2.2763\n",
      "Epoch [1453/5000] | D Loss: 0.6694 | G Loss: 3.0364\n",
      "Epoch [1454/5000] | D Loss: 0.7410 | G Loss: 3.0087\n",
      "Epoch [1455/5000] | D Loss: 1.0053 | G Loss: 2.0629\n",
      "Epoch [1456/5000] | D Loss: 0.6658 | G Loss: 3.1366\n",
      "Epoch [1457/5000] | D Loss: 0.7670 | G Loss: 2.2406\n",
      "Epoch [1458/5000] | D Loss: 0.6020 | G Loss: 2.4060\n",
      "Epoch [1459/5000] | D Loss: 0.7047 | G Loss: 2.5131\n",
      "Epoch [1460/5000] | D Loss: 0.5898 | G Loss: 2.6949\n",
      "Epoch [1461/5000] | D Loss: 0.6573 | G Loss: 3.2026\n",
      "Epoch [1462/5000] | D Loss: 0.7152 | G Loss: 2.8603\n",
      "Epoch [1463/5000] | D Loss: 0.6998 | G Loss: 2.6285\n",
      "Epoch [1464/5000] | D Loss: 0.6666 | G Loss: 2.5117\n",
      "Epoch [1465/5000] | D Loss: 0.6271 | G Loss: 3.4302\n",
      "Epoch [1466/5000] | D Loss: 0.7145 | G Loss: 2.8727\n",
      "Epoch [1467/5000] | D Loss: 0.6751 | G Loss: 2.9248\n",
      "Epoch [1468/5000] | D Loss: 0.7311 | G Loss: 2.8469\n",
      "Epoch [1469/5000] | D Loss: 0.6343 | G Loss: 3.1138\n",
      "Epoch [1470/5000] | D Loss: 0.7285 | G Loss: 2.4179\n",
      "Epoch [1471/5000] | D Loss: 0.6252 | G Loss: 3.0335\n",
      "Epoch [1472/5000] | D Loss: 1.3327 | G Loss: 1.5624\n",
      "Epoch [1473/5000] | D Loss: 0.7588 | G Loss: 2.3791\n",
      "Epoch [1474/5000] | D Loss: 0.6694 | G Loss: 2.6494\n",
      "Epoch [1475/5000] | D Loss: 0.8431 | G Loss: 2.2207\n",
      "Epoch [1476/5000] | D Loss: 0.5905 | G Loss: 3.2180\n",
      "Epoch [1477/5000] | D Loss: 0.6323 | G Loss: 2.1019\n",
      "Epoch [1478/5000] | D Loss: 0.6557 | G Loss: 1.8437\n",
      "Epoch [1479/5000] | D Loss: 0.6184 | G Loss: 2.9845\n",
      "Epoch [1480/5000] | D Loss: 0.5792 | G Loss: 2.8687\n",
      "Epoch [1481/5000] | D Loss: 0.7086 | G Loss: 1.3965\n",
      "Epoch [1482/5000] | D Loss: 0.9574 | G Loss: 2.3185\n",
      "Epoch [1483/5000] | D Loss: 2.0177 | G Loss: 1.5574\n",
      "Epoch [1484/5000] | D Loss: 0.8654 | G Loss: 1.4490\n",
      "Epoch [1485/5000] | D Loss: 1.0181 | G Loss: 1.5053\n",
      "Epoch [1486/5000] | D Loss: 0.7515 | G Loss: 2.1881\n",
      "Epoch [1487/5000] | D Loss: 0.6131 | G Loss: 2.8150\n",
      "Epoch [1488/5000] | D Loss: 0.8417 | G Loss: 1.8099\n",
      "Epoch [1489/5000] | D Loss: 1.2695 | G Loss: 1.4776\n",
      "Epoch [1490/5000] | D Loss: 0.6037 | G Loss: 3.1685\n",
      "Epoch [1491/5000] | D Loss: 0.5907 | G Loss: 3.7990\n",
      "Epoch [1492/5000] | D Loss: 0.6188 | G Loss: 3.4410\n",
      "Epoch [1493/5000] | D Loss: 0.6101 | G Loss: 2.5626\n",
      "Epoch [1494/5000] | D Loss: 0.6833 | G Loss: 3.2284\n",
      "Epoch [1495/5000] | D Loss: 0.6365 | G Loss: 2.6000\n",
      "Epoch [1496/5000] | D Loss: 0.6120 | G Loss: 3.4452\n",
      "Epoch [1497/5000] | D Loss: 0.6259 | G Loss: 2.2253\n",
      "Epoch [1498/5000] | D Loss: 0.8886 | G Loss: 2.0738\n",
      "Epoch [1499/5000] | D Loss: 0.6674 | G Loss: 2.5685\n",
      "Epoch [1500/5000] | D Loss: 0.5953 | G Loss: 2.5132\n",
      "Epoch 1500 FID Score: 180.8148\n",
      "Epoch [1501/5000] | D Loss: 0.8894 | G Loss: 1.3855\n",
      "Epoch [1502/5000] | D Loss: 0.9494 | G Loss: 2.0075\n",
      "Epoch [1503/5000] | D Loss: 0.7982 | G Loss: 2.3217\n",
      "Epoch [1504/5000] | D Loss: 0.6737 | G Loss: 2.5101\n",
      "Epoch [1505/5000] | D Loss: 0.5900 | G Loss: 4.4892\n",
      "Epoch [1506/5000] | D Loss: 0.8533 | G Loss: 2.6114\n",
      "Epoch [1507/5000] | D Loss: 0.6803 | G Loss: 2.3386\n",
      "Epoch [1508/5000] | D Loss: 0.8966 | G Loss: 2.6831\n",
      "Epoch [1509/5000] | D Loss: 0.7784 | G Loss: 2.2652\n",
      "Epoch [1510/5000] | D Loss: 0.6767 | G Loss: 2.9967\n",
      "Epoch [1511/5000] | D Loss: 0.7124 | G Loss: 3.8843\n",
      "Epoch [1512/5000] | D Loss: 0.5668 | G Loss: 3.6544\n",
      "Epoch [1513/5000] | D Loss: 0.7249 | G Loss: 2.4295\n",
      "Epoch [1514/5000] | D Loss: 0.6224 | G Loss: 3.6015\n",
      "Epoch [1515/5000] | D Loss: 0.6011 | G Loss: 3.0712\n",
      "Epoch [1516/5000] | D Loss: 0.6251 | G Loss: 2.8912\n",
      "Epoch [1517/5000] | D Loss: 0.8082 | G Loss: 2.2252\n",
      "Epoch [1518/5000] | D Loss: 0.6304 | G Loss: 3.5119\n",
      "Epoch [1519/5000] | D Loss: 1.1940 | G Loss: 2.3147\n",
      "Epoch [1520/5000] | D Loss: 0.5834 | G Loss: 3.1273\n",
      "Epoch [1521/5000] | D Loss: 0.6061 | G Loss: 2.5798\n",
      "Epoch [1522/5000] | D Loss: 0.6362 | G Loss: 2.7281\n",
      "Epoch [1523/5000] | D Loss: 0.6168 | G Loss: 2.7281\n",
      "Epoch [1524/5000] | D Loss: 0.6565 | G Loss: 2.1631\n",
      "Epoch [1525/5000] | D Loss: 0.5757 | G Loss: 3.2906\n",
      "Epoch [1526/5000] | D Loss: 0.5600 | G Loss: 3.1899\n",
      "Epoch [1527/5000] | D Loss: 0.6676 | G Loss: 2.4783\n",
      "Epoch [1528/5000] | D Loss: 0.7877 | G Loss: 2.3049\n",
      "Epoch [1529/5000] | D Loss: 0.6130 | G Loss: 3.2246\n",
      "Epoch [1530/5000] | D Loss: 0.5851 | G Loss: 2.8107\n",
      "Epoch [1531/5000] | D Loss: 0.5909 | G Loss: 3.0875\n",
      "Epoch [1532/5000] | D Loss: 0.6631 | G Loss: 2.8454\n",
      "Epoch [1533/5000] | D Loss: 0.6249 | G Loss: 2.8644\n",
      "Epoch [1534/5000] | D Loss: 0.5848 | G Loss: 2.9165\n",
      "Epoch [1535/5000] | D Loss: 0.7657 | G Loss: 1.8705\n",
      "Epoch [1536/5000] | D Loss: 0.7314 | G Loss: 2.4001\n",
      "Epoch [1537/5000] | D Loss: 0.7969 | G Loss: 1.5935\n",
      "Epoch [1538/5000] | D Loss: 0.6159 | G Loss: 3.3309\n",
      "Epoch [1539/5000] | D Loss: 1.0285 | G Loss: 1.4372\n",
      "Epoch [1540/5000] | D Loss: 0.9002 | G Loss: 1.4613\n",
      "Epoch [1541/5000] | D Loss: 0.6126 | G Loss: 2.6611\n",
      "Epoch [1542/5000] | D Loss: 0.5737 | G Loss: 2.7813\n",
      "Epoch [1543/5000] | D Loss: 0.6640 | G Loss: 2.6739\n",
      "Epoch [1544/5000] | D Loss: 0.8025 | G Loss: 2.4802\n",
      "Epoch [1545/5000] | D Loss: 0.6854 | G Loss: 2.3900\n",
      "Epoch [1546/5000] | D Loss: 0.8945 | G Loss: 1.9494\n",
      "Epoch [1547/5000] | D Loss: 0.6189 | G Loss: 3.8377\n",
      "Epoch [1548/5000] | D Loss: 0.6351 | G Loss: 2.8037\n",
      "Epoch [1549/5000] | D Loss: 0.8656 | G Loss: 2.1697\n",
      "Epoch [1550/5000] | D Loss: 0.6062 | G Loss: 3.1723\n",
      "Epoch [1551/5000] | D Loss: 0.6073 | G Loss: 2.6299\n",
      "Epoch [1552/5000] | D Loss: 0.7709 | G Loss: 2.3476\n",
      "Epoch [1553/5000] | D Loss: 0.6641 | G Loss: 3.0832\n",
      "Epoch [1554/5000] | D Loss: 0.9514 | G Loss: 2.2569\n",
      "Epoch [1555/5000] | D Loss: 0.5842 | G Loss: 3.3518\n",
      "Epoch [1556/5000] | D Loss: 0.6551 | G Loss: 2.7114\n",
      "Epoch [1557/5000] | D Loss: 0.5979 | G Loss: 3.6934\n",
      "Epoch [1558/5000] | D Loss: 0.6059 | G Loss: 3.8336\n",
      "Epoch [1559/5000] | D Loss: 0.5938 | G Loss: 2.9395\n",
      "Epoch [1560/5000] | D Loss: 0.6716 | G Loss: 1.8078\n",
      "Epoch [1561/5000] | D Loss: 0.6692 | G Loss: 3.4942\n",
      "Epoch [1562/5000] | D Loss: 0.7009 | G Loss: 1.7022\n",
      "Epoch [1563/5000] | D Loss: 0.7810 | G Loss: 2.0255\n",
      "Epoch [1564/5000] | D Loss: 5.3881 | G Loss: 0.4179\n",
      "Epoch [1565/5000] | D Loss: 0.5964 | G Loss: 2.8145\n",
      "Epoch [1566/5000] | D Loss: 0.5866 | G Loss: 1.9385\n",
      "Epoch [1567/5000] | D Loss: 0.6634 | G Loss: 2.5586\n",
      "Epoch [1568/5000] | D Loss: 0.6292 | G Loss: 2.1953\n",
      "Epoch [1569/5000] | D Loss: 0.6163 | G Loss: 3.1312\n",
      "Epoch [1570/5000] | D Loss: 0.6040 | G Loss: 2.3258\n",
      "Epoch [1571/5000] | D Loss: 0.6229 | G Loss: 2.8519\n",
      "Epoch [1572/5000] | D Loss: 0.6777 | G Loss: 2.5976\n",
      "Epoch [1573/5000] | D Loss: 0.5907 | G Loss: 2.9591\n",
      "Epoch [1574/5000] | D Loss: 0.5680 | G Loss: 2.4080\n",
      "Epoch [1575/5000] | D Loss: 0.6190 | G Loss: 2.3544\n",
      "Epoch [1576/5000] | D Loss: 0.6602 | G Loss: 2.5426\n",
      "Epoch [1577/5000] | D Loss: 0.8157 | G Loss: 2.3564\n",
      "Epoch [1578/5000] | D Loss: 0.6365 | G Loss: 2.1320\n",
      "Epoch [1579/5000] | D Loss: 0.6377 | G Loss: 3.0612\n",
      "Epoch [1580/5000] | D Loss: 0.7388 | G Loss: 2.2103\n",
      "Epoch [1581/5000] | D Loss: 1.0851 | G Loss: 1.5586\n",
      "Epoch [1582/5000] | D Loss: 0.9408 | G Loss: 2.2139\n",
      "Epoch [1583/5000] | D Loss: 0.6330 | G Loss: 2.4627\n",
      "Epoch [1584/5000] | D Loss: 0.7940 | G Loss: 1.5622\n",
      "Epoch [1585/5000] | D Loss: 0.6343 | G Loss: 2.3475\n",
      "Epoch [1586/5000] | D Loss: 0.5547 | G Loss: 2.8902\n",
      "Epoch [1587/5000] | D Loss: 0.5938 | G Loss: 2.7096\n",
      "Epoch [1588/5000] | D Loss: 0.6521 | G Loss: 3.3033\n",
      "Epoch [1589/5000] | D Loss: 0.6354 | G Loss: 3.8110\n",
      "Epoch [1590/5000] | D Loss: 0.6459 | G Loss: 2.6816\n",
      "Epoch [1591/5000] | D Loss: 0.8988 | G Loss: 3.2762\n",
      "Epoch [1592/5000] | D Loss: 0.5807 | G Loss: 2.9249\n",
      "Epoch [1593/5000] | D Loss: 0.6981 | G Loss: 2.7918\n",
      "Epoch [1594/5000] | D Loss: 1.1077 | G Loss: 1.9491\n",
      "Epoch [1595/5000] | D Loss: 0.6771 | G Loss: 2.9628\n",
      "Epoch [1596/5000] | D Loss: 0.6198 | G Loss: 2.9091\n",
      "Epoch [1597/5000] | D Loss: 0.8757 | G Loss: 2.3171\n",
      "Epoch [1598/5000] | D Loss: 0.5824 | G Loss: 2.8353\n",
      "Epoch [1599/5000] | D Loss: 0.6302 | G Loss: 2.5238\n",
      "Epoch [1600/5000] | D Loss: 0.5614 | G Loss: 2.8251\n",
      "Epoch 1600 FID Score: 198.1358\n",
      "Epoch [1601/5000] | D Loss: 0.6274 | G Loss: 3.2410\n",
      "Epoch [1602/5000] | D Loss: 0.5735 | G Loss: 2.9904\n",
      "Epoch [1603/5000] | D Loss: 0.6176 | G Loss: 2.5161\n",
      "Epoch [1604/5000] | D Loss: 0.7124 | G Loss: 2.0554\n",
      "Epoch [1605/5000] | D Loss: 0.6521 | G Loss: 2.6824\n",
      "Epoch [1606/5000] | D Loss: 0.6326 | G Loss: 2.8860\n",
      "Epoch [1607/5000] | D Loss: 0.7713 | G Loss: 2.1014\n",
      "Epoch [1608/5000] | D Loss: 0.5784 | G Loss: 3.0892\n",
      "Epoch [1609/5000] | D Loss: 0.6402 | G Loss: 2.0281\n",
      "Epoch [1610/5000] | D Loss: 0.5959 | G Loss: 2.0800\n",
      "Epoch [1611/5000] | D Loss: 0.7822 | G Loss: 2.7738\n",
      "Epoch [1612/5000] | D Loss: 0.6746 | G Loss: 2.1218\n",
      "Epoch [1613/5000] | D Loss: 0.7195 | G Loss: 2.0520\n",
      "Epoch [1614/5000] | D Loss: 0.6102 | G Loss: 3.7379\n",
      "Epoch [1615/5000] | D Loss: 0.7397 | G Loss: 2.7283\n",
      "Epoch [1616/5000] | D Loss: 0.9816 | G Loss: 2.2442\n",
      "Epoch [1617/5000] | D Loss: 0.5722 | G Loss: 3.4163\n",
      "Epoch [1618/5000] | D Loss: 0.6158 | G Loss: 1.5540\n",
      "Epoch [1619/5000] | D Loss: 0.5982 | G Loss: 3.2985\n",
      "Epoch [1620/5000] | D Loss: 0.6569 | G Loss: 2.3118\n",
      "Epoch [1621/5000] | D Loss: 0.7010 | G Loss: 2.5368\n",
      "Epoch [1622/5000] | D Loss: 0.6287 | G Loss: 2.0550\n",
      "Epoch [1623/5000] | D Loss: 0.5914 | G Loss: 4.2260\n",
      "Epoch [1624/5000] | D Loss: 0.6019 | G Loss: 2.9472\n",
      "Epoch [1625/5000] | D Loss: 0.5843 | G Loss: 2.8694\n",
      "Epoch [1626/5000] | D Loss: 0.6327 | G Loss: 2.4523\n",
      "Epoch [1627/5000] | D Loss: 0.7672 | G Loss: 2.4670\n",
      "Epoch [1628/5000] | D Loss: 0.7289 | G Loss: 2.8360\n",
      "Epoch [1629/5000] | D Loss: 0.5901 | G Loss: 2.3810\n",
      "Epoch [1630/5000] | D Loss: 0.6855 | G Loss: 1.7614\n",
      "Epoch [1631/5000] | D Loss: 0.6019 | G Loss: 3.3912\n",
      "Epoch [1632/5000] | D Loss: 1.1767 | G Loss: 1.4685\n",
      "Epoch [1633/5000] | D Loss: 0.7147 | G Loss: 2.3486\n",
      "Epoch [1634/5000] | D Loss: 0.9084 | G Loss: 2.4522\n",
      "Epoch [1635/5000] | D Loss: 0.5602 | G Loss: 3.4325\n",
      "Epoch [1636/5000] | D Loss: 0.5690 | G Loss: 2.6068\n",
      "Epoch [1637/5000] | D Loss: 0.6087 | G Loss: 3.0471\n",
      "Epoch [1638/5000] | D Loss: 0.6330 | G Loss: 3.2373\n",
      "Epoch [1639/5000] | D Loss: 0.5982 | G Loss: 3.4692\n",
      "Epoch [1640/5000] | D Loss: 0.7000 | G Loss: 2.7867\n",
      "Epoch [1641/5000] | D Loss: 0.5949 | G Loss: 3.5453\n",
      "Epoch [1642/5000] | D Loss: 0.6211 | G Loss: 3.9223\n",
      "Epoch [1643/5000] | D Loss: 0.6839 | G Loss: 3.3066\n",
      "Epoch [1644/5000] | D Loss: 0.6345 | G Loss: 4.0882\n",
      "Epoch [1645/5000] | D Loss: 0.6055 | G Loss: 3.6035\n",
      "Epoch [1646/5000] | D Loss: 0.6122 | G Loss: 3.3981\n",
      "Epoch [1647/5000] | D Loss: 0.6083 | G Loss: 2.6365\n",
      "Epoch [1648/5000] | D Loss: 0.6365 | G Loss: 2.5717\n",
      "Epoch [1649/5000] | D Loss: 0.6900 | G Loss: 2.1859\n",
      "Epoch [1650/5000] | D Loss: 0.6535 | G Loss: 2.3122\n",
      "Epoch [1651/5000] | D Loss: 0.6348 | G Loss: 2.8149\n",
      "Epoch [1652/5000] | D Loss: 0.5688 | G Loss: 3.8368\n",
      "Epoch [1653/5000] | D Loss: 0.6247 | G Loss: 2.7802\n",
      "Epoch [1654/5000] | D Loss: 0.6596 | G Loss: 2.8288\n",
      "Epoch [1655/5000] | D Loss: 0.5962 | G Loss: 3.8388\n",
      "Epoch [1656/5000] | D Loss: 0.6159 | G Loss: 4.1594\n",
      "Epoch [1657/5000] | D Loss: 0.5746 | G Loss: 2.5492\n",
      "Epoch [1658/5000] | D Loss: 0.5744 | G Loss: 3.6832\n",
      "Epoch [1659/5000] | D Loss: 0.6030 | G Loss: 3.4361\n",
      "Epoch [1660/5000] | D Loss: 0.6467 | G Loss: 2.4873\n",
      "Epoch [1661/5000] | D Loss: 0.6387 | G Loss: 2.0372\n",
      "Epoch [1662/5000] | D Loss: 0.6189 | G Loss: 2.3489\n",
      "Epoch [1663/5000] | D Loss: 0.5832 | G Loss: 2.8192\n",
      "Epoch [1664/5000] | D Loss: 0.7312 | G Loss: 2.5739\n",
      "Epoch [1665/5000] | D Loss: 0.6333 | G Loss: 3.6822\n",
      "Epoch [1666/5000] | D Loss: 1.6006 | G Loss: 1.0269\n",
      "Epoch [1667/5000] | D Loss: 0.6013 | G Loss: 3.2182\n",
      "Epoch [1668/5000] | D Loss: 0.6165 | G Loss: 2.6934\n",
      "Epoch [1669/5000] | D Loss: 0.6933 | G Loss: 2.9701\n",
      "Epoch [1670/5000] | D Loss: 0.5885 | G Loss: 2.9203\n",
      "Epoch [1671/5000] | D Loss: 0.8540 | G Loss: 1.4231\n",
      "Epoch [1672/5000] | D Loss: 0.6500 | G Loss: 3.3959\n",
      "Epoch [1673/5000] | D Loss: 1.1127 | G Loss: 1.5817\n",
      "Epoch [1674/5000] | D Loss: 0.6890 | G Loss: 3.0279\n",
      "Epoch [1675/5000] | D Loss: 0.6188 | G Loss: 2.4972\n",
      "Epoch [1676/5000] | D Loss: 0.5705 | G Loss: 4.9337\n",
      "Epoch [1677/5000] | D Loss: 0.7735 | G Loss: 2.9987\n",
      "Epoch [1678/5000] | D Loss: 0.6483 | G Loss: 2.6878\n",
      "Epoch [1679/5000] | D Loss: 0.6774 | G Loss: 3.6390\n",
      "Epoch [1680/5000] | D Loss: 0.7487 | G Loss: 1.6125\n",
      "Epoch [1681/5000] | D Loss: 1.3132 | G Loss: 1.1026\n",
      "Epoch [1682/5000] | D Loss: 0.8135 | G Loss: 1.2800\n",
      "Epoch [1683/5000] | D Loss: 0.6017 | G Loss: 3.1777\n",
      "Epoch [1684/5000] | D Loss: 0.8374 | G Loss: 2.7599\n",
      "Epoch [1685/5000] | D Loss: 0.6597 | G Loss: 3.5928\n",
      "Epoch [1686/5000] | D Loss: 0.7177 | G Loss: 2.4025\n",
      "Epoch [1687/5000] | D Loss: 0.9305 | G Loss: 2.2094\n",
      "Epoch [1688/5000] | D Loss: 0.5953 | G Loss: 2.7628\n",
      "Epoch [1689/5000] | D Loss: 0.7033 | G Loss: 1.9359\n",
      "Epoch [1690/5000] | D Loss: 0.5981 | G Loss: 3.1653\n",
      "Epoch [1691/5000] | D Loss: 0.6780 | G Loss: 2.1459\n",
      "Epoch [1692/5000] | D Loss: 0.8942 | G Loss: 2.0892\n",
      "Epoch [1693/5000] | D Loss: 0.7309 | G Loss: 2.4775\n",
      "Epoch [1694/5000] | D Loss: 0.5912 | G Loss: 2.9204\n",
      "Epoch [1695/5000] | D Loss: 0.6737 | G Loss: 2.4313\n",
      "Epoch [1696/5000] | D Loss: 0.6437 | G Loss: 2.9923\n",
      "Epoch [1697/5000] | D Loss: 0.5950 | G Loss: 4.4517\n",
      "Epoch [1698/5000] | D Loss: 0.6064 | G Loss: 3.8269\n",
      "Epoch [1699/5000] | D Loss: 0.5793 | G Loss: 3.5542\n",
      "Epoch [1700/5000] | D Loss: 0.6412 | G Loss: 2.5791\n",
      "Epoch 1700 FID Score: 185.2862\n",
      "Epoch [1701/5000] | D Loss: 0.6040 | G Loss: 3.8224\n",
      "Epoch [1702/5000] | D Loss: 0.9088 | G Loss: 1.9266\n",
      "Epoch [1703/5000] | D Loss: 0.5966 | G Loss: 2.9193\n",
      "Epoch [1704/5000] | D Loss: 0.5852 | G Loss: 2.5936\n",
      "Epoch [1705/5000] | D Loss: 0.5897 | G Loss: 3.9848\n",
      "Epoch [1706/5000] | D Loss: 0.5878 | G Loss: 3.4535\n",
      "Epoch [1707/5000] | D Loss: 0.7720 | G Loss: 2.8288\n",
      "Epoch [1708/5000] | D Loss: 0.6010 | G Loss: 2.6103\n",
      "Epoch [1709/5000] | D Loss: 0.7257 | G Loss: 2.3301\n",
      "Epoch [1710/5000] | D Loss: 0.5752 | G Loss: 2.8783\n",
      "Epoch [1711/5000] | D Loss: 0.5823 | G Loss: 2.4222\n",
      "Epoch [1712/5000] | D Loss: 1.2010 | G Loss: 1.8127\n",
      "Epoch [1713/5000] | D Loss: 0.7191 | G Loss: 1.7494\n",
      "Epoch [1714/5000] | D Loss: 0.6338 | G Loss: 2.5771\n",
      "Epoch [1715/5000] | D Loss: 0.6050 | G Loss: 2.9145\n",
      "Epoch [1716/5000] | D Loss: 1.2807 | G Loss: 1.7647\n",
      "Epoch [1717/5000] | D Loss: 0.6844 | G Loss: 2.4686\n",
      "Epoch [1718/5000] | D Loss: 0.5775 | G Loss: 3.3335\n",
      "Epoch [1719/5000] | D Loss: 0.5810 | G Loss: 2.4376\n",
      "Epoch [1720/5000] | D Loss: 0.5709 | G Loss: 3.3701\n",
      "Epoch [1721/5000] | D Loss: 0.8100 | G Loss: 1.6587\n",
      "Epoch [1722/5000] | D Loss: 0.6175 | G Loss: 3.0644\n",
      "Epoch [1723/5000] | D Loss: 0.5996 | G Loss: 2.8354\n",
      "Epoch [1724/5000] | D Loss: 0.6782 | G Loss: 2.5578\n",
      "Epoch [1725/5000] | D Loss: 0.7522 | G Loss: 1.6919\n",
      "Epoch [1726/5000] | D Loss: 0.7416 | G Loss: 2.5645\n",
      "Epoch [1727/5000] | D Loss: 0.6016 | G Loss: 3.8085\n",
      "Epoch [1728/5000] | D Loss: 0.6956 | G Loss: 2.0237\n",
      "Epoch [1729/5000] | D Loss: 0.7968 | G Loss: 1.8847\n",
      "Epoch [1730/5000] | D Loss: 0.8408 | G Loss: 2.7229\n",
      "Epoch [1731/5000] | D Loss: 0.5656 | G Loss: 3.6269\n",
      "Epoch [1732/5000] | D Loss: 0.6542 | G Loss: 1.9301\n",
      "Epoch [1733/5000] | D Loss: 0.6587 | G Loss: 3.1982\n",
      "Epoch [1734/5000] | D Loss: 0.6110 | G Loss: 2.9936\n",
      "Epoch [1735/5000] | D Loss: 0.6002 | G Loss: 3.3683\n",
      "Epoch [1736/5000] | D Loss: 0.5919 | G Loss: 3.0859\n",
      "Epoch [1737/5000] | D Loss: 0.6999 | G Loss: 2.2753\n",
      "Epoch [1738/5000] | D Loss: 0.6392 | G Loss: 2.3844\n",
      "Epoch [1739/5000] | D Loss: 0.7184 | G Loss: 2.9059\n",
      "Epoch [1740/5000] | D Loss: 0.7628 | G Loss: 3.9610\n",
      "Epoch [1741/5000] | D Loss: 0.5908 | G Loss: 3.7548\n",
      "Epoch [1742/5000] | D Loss: 0.5808 | G Loss: 3.2203\n",
      "Epoch [1743/5000] | D Loss: 0.6220 | G Loss: 2.7181\n",
      "Epoch [1744/5000] | D Loss: 0.5832 | G Loss: 3.6835\n",
      "Epoch [1745/5000] | D Loss: 0.5850 | G Loss: 2.8548\n",
      "Epoch [1746/5000] | D Loss: 0.5853 | G Loss: 3.6574\n",
      "Epoch [1747/5000] | D Loss: 0.6430 | G Loss: 3.1241\n",
      "Epoch [1748/5000] | D Loss: 0.6223 | G Loss: 2.9515\n",
      "Epoch [1749/5000] | D Loss: 0.5986 | G Loss: 3.2872\n",
      "Epoch [1750/5000] | D Loss: 0.6159 | G Loss: 2.4424\n",
      "Epoch [1751/5000] | D Loss: 0.6002 | G Loss: 3.6779\n",
      "Epoch [1752/5000] | D Loss: 0.6039 | G Loss: 2.8547\n",
      "Epoch [1753/5000] | D Loss: 0.8065 | G Loss: 2.3898\n",
      "Epoch [1754/5000] | D Loss: 0.5876 | G Loss: 3.3306\n",
      "Epoch [1755/5000] | D Loss: 0.6195 | G Loss: 2.8291\n",
      "Epoch [1756/5000] | D Loss: 0.6728 | G Loss: 2.2545\n",
      "Epoch [1757/5000] | D Loss: 0.6702 | G Loss: 1.9382\n",
      "Epoch [1758/5000] | D Loss: 1.6252 | G Loss: 2.2337\n",
      "Epoch [1759/5000] | D Loss: 0.6416 | G Loss: 3.1256\n",
      "Epoch [1760/5000] | D Loss: 0.5793 | G Loss: 3.2247\n",
      "Epoch [1761/5000] | D Loss: 0.7121 | G Loss: 3.0524\n",
      "Epoch [1762/5000] | D Loss: 0.6250 | G Loss: 2.0526\n",
      "Epoch [1763/5000] | D Loss: 0.5944 | G Loss: 3.6789\n",
      "Epoch [1764/5000] | D Loss: 0.7735 | G Loss: 2.5867\n",
      "Epoch [1765/5000] | D Loss: 0.6040 | G Loss: 4.7539\n",
      "Epoch [1766/5000] | D Loss: 0.6025 | G Loss: 2.3569\n",
      "Epoch [1767/5000] | D Loss: 0.5861 | G Loss: 4.0758\n",
      "Epoch [1768/5000] | D Loss: 0.7002 | G Loss: 1.3940\n",
      "Epoch [1769/5000] | D Loss: 0.6495 | G Loss: 2.1345\n",
      "Epoch [1770/5000] | D Loss: 0.7054 | G Loss: 2.9017\n",
      "Epoch [1771/5000] | D Loss: 0.6701 | G Loss: 3.3831\n",
      "Epoch [1772/5000] | D Loss: 0.6755 | G Loss: 2.7689\n",
      "Epoch [1773/5000] | D Loss: 0.6204 | G Loss: 2.7125\n",
      "Epoch [1774/5000] | D Loss: 0.6595 | G Loss: 2.3412\n",
      "Epoch [1775/5000] | D Loss: 1.0046 | G Loss: 2.2685\n",
      "Epoch [1776/5000] | D Loss: 0.6236 | G Loss: 3.4892\n",
      "Epoch [1777/5000] | D Loss: 0.8317 | G Loss: 2.5560\n",
      "Epoch [1778/5000] | D Loss: 0.5807 | G Loss: 2.5040\n",
      "Epoch [1779/5000] | D Loss: 0.7067 | G Loss: 3.5566\n",
      "Epoch [1780/5000] | D Loss: 0.6079 | G Loss: 3.2887\n",
      "Epoch [1781/5000] | D Loss: 0.6330 | G Loss: 2.4969\n",
      "Epoch [1782/5000] | D Loss: 0.6079 | G Loss: 3.5019\n",
      "Epoch [1783/5000] | D Loss: 0.6071 | G Loss: 2.9088\n",
      "Epoch [1784/5000] | D Loss: 0.7694 | G Loss: 2.4594\n",
      "Epoch [1785/5000] | D Loss: 0.6400 | G Loss: 4.8882\n",
      "Epoch [1786/5000] | D Loss: 0.5792 | G Loss: 2.6802\n",
      "Epoch [1787/5000] | D Loss: 0.6460 | G Loss: 3.7633\n",
      "Epoch [1788/5000] | D Loss: 0.5531 | G Loss: 2.8797\n",
      "Epoch [1789/5000] | D Loss: 0.5995 | G Loss: 3.3379\n",
      "Epoch [1790/5000] | D Loss: 0.7000 | G Loss: 2.5519\n",
      "Epoch [1791/5000] | D Loss: 0.6030 | G Loss: 3.0486\n",
      "Epoch [1792/5000] | D Loss: 0.6893 | G Loss: 2.9805\n",
      "Epoch [1793/5000] | D Loss: 0.7746 | G Loss: 2.4501\n",
      "Epoch [1794/5000] | D Loss: 0.5982 | G Loss: 2.5446\n",
      "Epoch [1795/5000] | D Loss: 0.5954 | G Loss: 1.8467\n",
      "Epoch [1796/5000] | D Loss: 1.0537 | G Loss: 1.9510\n",
      "Epoch [1797/5000] | D Loss: 0.5630 | G Loss: 2.8075\n",
      "Epoch [1798/5000] | D Loss: 0.5799 | G Loss: 3.6246\n",
      "Epoch [1799/5000] | D Loss: 0.7668 | G Loss: 2.4592\n",
      "Epoch [1800/5000] | D Loss: 0.7077 | G Loss: 2.9523\n",
      "Epoch 1800 FID Score: 186.2645\n",
      "Epoch [1801/5000] | D Loss: 0.6275 | G Loss: 2.7480\n",
      "Epoch [1802/5000] | D Loss: 0.6997 | G Loss: 3.1131\n",
      "Epoch [1803/5000] | D Loss: 0.6225 | G Loss: 2.1757\n",
      "Epoch [1804/5000] | D Loss: 0.5637 | G Loss: 3.7332\n",
      "Epoch [1805/5000] | D Loss: 0.9296 | G Loss: 1.7091\n",
      "Epoch [1806/5000] | D Loss: 0.6266 | G Loss: 3.3452\n",
      "Epoch [1807/5000] | D Loss: 0.6201 | G Loss: 3.2152\n",
      "Epoch [1808/5000] | D Loss: 0.6237 | G Loss: 2.9480\n",
      "Epoch [1809/5000] | D Loss: 0.5960 | G Loss: 3.4080\n",
      "Epoch [1810/5000] | D Loss: 0.6003 | G Loss: 2.9587\n",
      "Epoch [1811/5000] | D Loss: 0.5858 | G Loss: 3.8821\n",
      "Epoch [1812/5000] | D Loss: 0.6569 | G Loss: 2.3507\n",
      "Epoch [1813/5000] | D Loss: 0.9376 | G Loss: 1.4665\n",
      "Epoch [1814/5000] | D Loss: 0.9835 | G Loss: 1.6939\n",
      "Epoch [1815/5000] | D Loss: 0.5831 | G Loss: 3.9671\n",
      "Epoch [1816/5000] | D Loss: 0.6411 | G Loss: 2.9622\n",
      "Epoch [1817/5000] | D Loss: 0.5929 | G Loss: 3.4256\n",
      "Epoch [1818/5000] | D Loss: 0.6572 | G Loss: 2.8743\n",
      "Epoch [1819/5000] | D Loss: 0.6258 | G Loss: 3.1211\n",
      "Epoch [1820/5000] | D Loss: 0.6452 | G Loss: 3.3389\n",
      "Epoch [1821/5000] | D Loss: 0.5995 | G Loss: 3.5679\n",
      "Epoch [1822/5000] | D Loss: 0.5741 | G Loss: 3.2217\n",
      "Epoch [1823/5000] | D Loss: 0.7060 | G Loss: 2.9484\n",
      "Epoch [1824/5000] | D Loss: 0.7866 | G Loss: 1.9602\n",
      "Epoch [1825/5000] | D Loss: 0.6313 | G Loss: 2.8310\n",
      "Epoch [1826/5000] | D Loss: 0.6801 | G Loss: 3.2066\n",
      "Epoch [1827/5000] | D Loss: 0.7009 | G Loss: 2.1716\n",
      "Epoch [1828/5000] | D Loss: 0.6725 | G Loss: 3.8023\n",
      "Epoch [1829/5000] | D Loss: 0.6739 | G Loss: 2.5439\n",
      "Epoch [1830/5000] | D Loss: 0.6359 | G Loss: 2.3247\n",
      "Epoch [1831/5000] | D Loss: 0.5820 | G Loss: 3.0688\n",
      "Epoch [1832/5000] | D Loss: 0.6216 | G Loss: 2.9704\n",
      "Epoch [1833/5000] | D Loss: 0.9167 | G Loss: 2.2393\n",
      "Epoch [1834/5000] | D Loss: 0.5812 | G Loss: 3.6711\n",
      "Epoch [1835/5000] | D Loss: 0.7217 | G Loss: 3.3571\n",
      "Epoch [1836/5000] | D Loss: 0.8345 | G Loss: 2.3271\n",
      "Epoch [1837/5000] | D Loss: 0.8494 | G Loss: 2.7655\n",
      "Epoch [1838/5000] | D Loss: 0.6784 | G Loss: 2.6463\n",
      "Epoch [1839/5000] | D Loss: 0.6437 | G Loss: 2.5434\n",
      "Epoch [1840/5000] | D Loss: 0.5902 | G Loss: 3.0121\n",
      "Epoch [1841/5000] | D Loss: 0.5988 | G Loss: 3.7610\n",
      "Epoch [1842/5000] | D Loss: 0.6177 | G Loss: 3.1713\n",
      "Epoch [1843/5000] | D Loss: 0.7016 | G Loss: 2.2684\n",
      "Epoch [1844/5000] | D Loss: 0.6037 | G Loss: 3.7511\n",
      "Epoch [1845/5000] | D Loss: 0.6269 | G Loss: 2.4225\n",
      "Epoch [1846/5000] | D Loss: 0.6683 | G Loss: 2.8058\n",
      "Epoch [1847/5000] | D Loss: 0.6818 | G Loss: 2.3912\n",
      "Epoch [1848/5000] | D Loss: 0.5873 | G Loss: 3.1652\n",
      "Epoch [1849/5000] | D Loss: 0.5878 | G Loss: 2.7540\n",
      "Epoch [1850/5000] | D Loss: 0.5710 | G Loss: 3.2566\n",
      "Epoch [1851/5000] | D Loss: 0.6029 | G Loss: 2.7334\n",
      "Epoch [1852/5000] | D Loss: 0.7760 | G Loss: 1.6732\n",
      "Epoch [1853/5000] | D Loss: 0.5760 | G Loss: 3.5652\n",
      "Epoch [1854/5000] | D Loss: 0.6083 | G Loss: 3.2965\n",
      "Epoch [1855/5000] | D Loss: 0.5969 | G Loss: 2.7484\n",
      "Epoch [1856/5000] | D Loss: 1.2505 | G Loss: 2.3481\n",
      "Epoch [1857/5000] | D Loss: 0.6516 | G Loss: 3.3837\n",
      "Epoch [1858/5000] | D Loss: 0.6247 | G Loss: 3.3283\n",
      "Epoch [1859/5000] | D Loss: 0.5881 | G Loss: 3.5056\n",
      "Epoch [1860/5000] | D Loss: 0.5975 | G Loss: 2.6875\n",
      "Epoch [1861/5000] | D Loss: 0.5783 | G Loss: 2.4384\n",
      "Epoch [1862/5000] | D Loss: 0.6528 | G Loss: 3.3200\n",
      "Epoch [1863/5000] | D Loss: 0.5903 | G Loss: 3.3808\n",
      "Epoch [1864/5000] | D Loss: 0.6923 | G Loss: 2.5693\n",
      "Epoch [1865/5000] | D Loss: 0.6632 | G Loss: 2.8467\n",
      "Epoch [1866/5000] | D Loss: 0.5658 | G Loss: 3.1378\n",
      "Epoch [1867/5000] | D Loss: 0.7169 | G Loss: 2.3528\n",
      "Epoch [1868/5000] | D Loss: 0.5668 | G Loss: 3.5001\n",
      "Epoch [1869/5000] | D Loss: 1.0094 | G Loss: 2.1367\n",
      "Epoch [1870/5000] | D Loss: 0.8479 | G Loss: 3.4347\n",
      "Epoch [1871/5000] | D Loss: 0.5968 | G Loss: 2.4547\n",
      "Epoch [1872/5000] | D Loss: 0.6508 | G Loss: 2.5566\n",
      "Epoch [1873/5000] | D Loss: 0.6787 | G Loss: 2.2242\n",
      "Epoch [1874/5000] | D Loss: 0.5808 | G Loss: 3.3373\n",
      "Epoch [1875/5000] | D Loss: 0.7208 | G Loss: 1.7397\n",
      "Epoch [1876/5000] | D Loss: 0.5793 | G Loss: 2.7658\n",
      "Epoch [1877/5000] | D Loss: 0.6180 | G Loss: 3.3024\n",
      "Epoch [1878/5000] | D Loss: 0.5948 | G Loss: 3.2088\n",
      "Epoch [1879/5000] | D Loss: 0.6191 | G Loss: 2.5554\n",
      "Epoch [1880/5000] | D Loss: 0.5815 | G Loss: 2.4192\n",
      "Epoch [1881/5000] | D Loss: 0.6293 | G Loss: 4.3158\n",
      "Epoch [1882/5000] | D Loss: 0.6253 | G Loss: 3.0994\n",
      "Epoch [1883/5000] | D Loss: 0.5688 | G Loss: 3.8671\n",
      "Epoch [1884/5000] | D Loss: 0.6007 | G Loss: 3.2144\n",
      "Epoch [1885/5000] | D Loss: 0.5959 | G Loss: 3.0754\n",
      "Epoch [1886/5000] | D Loss: 0.9569 | G Loss: 1.8704\n",
      "Epoch [1887/5000] | D Loss: 0.5642 | G Loss: 2.8162\n",
      "Epoch [1888/5000] | D Loss: 0.6072 | G Loss: 3.2103\n",
      "Epoch [1889/5000] | D Loss: 0.6770 | G Loss: 2.5733\n",
      "Epoch [1890/5000] | D Loss: 0.5887 | G Loss: 2.8250\n",
      "Epoch [1891/5000] | D Loss: 0.8687 | G Loss: 2.8948\n",
      "Epoch [1892/5000] | D Loss: 0.5906 | G Loss: 2.9556\n",
      "Epoch [1893/5000] | D Loss: 0.6340 | G Loss: 2.6432\n",
      "Epoch [1894/5000] | D Loss: 0.7418 | G Loss: 1.8162\n",
      "Epoch [1895/5000] | D Loss: 0.6020 | G Loss: 2.1360\n",
      "Epoch [1896/5000] | D Loss: 0.6181 | G Loss: 2.9517\n",
      "Epoch [1897/5000] | D Loss: 0.7100 | G Loss: 2.2265\n",
      "Epoch [1898/5000] | D Loss: 0.6087 | G Loss: 2.8108\n",
      "Epoch [1899/5000] | D Loss: 0.5959 | G Loss: 3.2151\n",
      "Epoch [1900/5000] | D Loss: 0.5814 | G Loss: 3.7480\n",
      "Epoch 1900 FID Score: 180.9515\n",
      "Epoch [1901/5000] | D Loss: 2.4121 | G Loss: 0.6727\n",
      "Epoch [1902/5000] | D Loss: 0.5925 | G Loss: 2.9721\n",
      "Epoch [1903/5000] | D Loss: 0.6111 | G Loss: 2.8701\n",
      "Epoch [1904/5000] | D Loss: 0.6220 | G Loss: 2.6186\n",
      "Epoch [1905/5000] | D Loss: 0.6488 | G Loss: 2.6356\n",
      "Epoch [1906/5000] | D Loss: 0.6974 | G Loss: 3.2140\n",
      "Epoch [1907/5000] | D Loss: 0.6106 | G Loss: 3.4894\n",
      "Epoch [1908/5000] | D Loss: 0.7497 | G Loss: 2.3648\n",
      "Epoch [1909/5000] | D Loss: 0.6477 | G Loss: 2.7888\n",
      "Epoch [1910/5000] | D Loss: 0.8134 | G Loss: 2.5500\n",
      "Epoch [1911/5000] | D Loss: 0.6873 | G Loss: 2.1238\n",
      "Epoch [1912/5000] | D Loss: 0.8012 | G Loss: 1.2773\n",
      "Epoch [1913/5000] | D Loss: 1.1550 | G Loss: 2.0337\n",
      "Epoch [1914/5000] | D Loss: 0.7709 | G Loss: 2.6917\n",
      "Epoch [1915/5000] | D Loss: 0.6050 | G Loss: 2.8432\n",
      "Epoch [1916/5000] | D Loss: 0.5655 | G Loss: 4.0096\n",
      "Epoch [1917/5000] | D Loss: 0.8940 | G Loss: 1.9353\n",
      "Epoch [1918/5000] | D Loss: 0.6137 | G Loss: 3.5298\n",
      "Epoch [1919/5000] | D Loss: 0.8684 | G Loss: 2.7652\n",
      "Epoch [1920/5000] | D Loss: 0.6346 | G Loss: 2.2752\n",
      "Epoch [1921/5000] | D Loss: 0.5951 | G Loss: 3.0583\n",
      "Epoch [1922/5000] | D Loss: 0.7241 | G Loss: 2.5357\n",
      "Epoch [1923/5000] | D Loss: 0.5869 | G Loss: 3.9988\n",
      "Epoch [1924/5000] | D Loss: 0.9200 | G Loss: 1.8970\n",
      "Epoch [1925/5000] | D Loss: 0.6180 | G Loss: 2.6506\n",
      "Epoch [1926/5000] | D Loss: 0.5988 | G Loss: 3.5760\n",
      "Epoch [1927/5000] | D Loss: 1.1536 | G Loss: 1.3425\n",
      "Epoch [1928/5000] | D Loss: 0.8161 | G Loss: 3.1599\n",
      "Epoch [1929/5000] | D Loss: 0.5896 | G Loss: 2.9668\n",
      "Epoch [1930/5000] | D Loss: 0.7406 | G Loss: 2.6906\n",
      "Epoch [1931/5000] | D Loss: 0.6535 | G Loss: 3.2281\n",
      "Epoch [1932/5000] | D Loss: 0.6043 | G Loss: 2.6238\n",
      "Epoch [1933/5000] | D Loss: 0.6196 | G Loss: 3.4144\n",
      "Epoch [1934/5000] | D Loss: 0.7414 | G Loss: 2.4033\n",
      "Epoch [1935/5000] | D Loss: 0.6945 | G Loss: 2.9485\n",
      "Epoch [1936/5000] | D Loss: 0.6372 | G Loss: 3.0970\n",
      "Epoch [1937/5000] | D Loss: 0.5828 | G Loss: 2.8988\n",
      "Epoch [1938/5000] | D Loss: 0.6127 | G Loss: 3.2953\n",
      "Epoch [1939/5000] | D Loss: 1.2887 | G Loss: 0.5995\n",
      "Epoch [1940/5000] | D Loss: 0.5896 | G Loss: 3.7793\n",
      "Epoch [1941/5000] | D Loss: 0.5933 | G Loss: 4.5893\n",
      "Epoch [1942/5000] | D Loss: 0.5775 | G Loss: 4.7113\n",
      "Epoch [1943/5000] | D Loss: 0.9094 | G Loss: 2.4102\n",
      "Epoch [1944/5000] | D Loss: 0.5867 | G Loss: 3.6731\n",
      "Epoch [1945/5000] | D Loss: 0.5788 | G Loss: 2.8933\n",
      "Epoch [1946/5000] | D Loss: 0.5585 | G Loss: 2.5568\n",
      "Epoch [1947/5000] | D Loss: 0.5867 | G Loss: 3.1960\n",
      "Epoch [1948/5000] | D Loss: 0.7550 | G Loss: 2.5192\n",
      "Epoch [1949/5000] | D Loss: 0.6462 | G Loss: 2.2771\n",
      "Epoch [1950/5000] | D Loss: 0.6081 | G Loss: 2.8189\n",
      "Epoch [1951/5000] | D Loss: 0.6525 | G Loss: 1.9496\n",
      "Epoch [1952/5000] | D Loss: 0.5692 | G Loss: 3.1586\n",
      "Epoch [1953/5000] | D Loss: 0.7078 | G Loss: 2.6564\n",
      "Epoch [1954/5000] | D Loss: 0.6949 | G Loss: 3.0006\n",
      "Epoch [1955/5000] | D Loss: 0.8888 | G Loss: 1.7096\n",
      "Epoch [1956/5000] | D Loss: 0.5767 | G Loss: 3.1320\n",
      "Epoch [1957/5000] | D Loss: 0.6037 | G Loss: 3.9702\n",
      "Epoch [1958/5000] | D Loss: 0.6553 | G Loss: 3.5462\n",
      "Epoch [1959/5000] | D Loss: 2.9970 | G Loss: 0.6558\n",
      "Epoch [1960/5000] | D Loss: 0.6182 | G Loss: 2.4348\n",
      "Epoch [1961/5000] | D Loss: 0.5630 | G Loss: 2.2944\n",
      "Epoch [1962/5000] | D Loss: 0.6063 | G Loss: 3.5334\n",
      "Epoch [1963/5000] | D Loss: 0.5948 | G Loss: 3.1156\n",
      "Epoch [1964/5000] | D Loss: 0.6248 | G Loss: 3.7262\n",
      "Epoch [1965/5000] | D Loss: 0.5651 | G Loss: 3.4206\n",
      "Epoch [1966/5000] | D Loss: 0.6686 | G Loss: 2.6549\n",
      "Epoch [1967/5000] | D Loss: 0.5797 | G Loss: 2.6801\n",
      "Epoch [1968/5000] | D Loss: 0.6214 | G Loss: 3.3823\n",
      "Epoch [1969/5000] | D Loss: 1.1124 | G Loss: 2.3393\n",
      "Epoch [1970/5000] | D Loss: 0.6116 | G Loss: 3.2682\n",
      "Epoch [1971/5000] | D Loss: 0.6213 | G Loss: 2.7859\n",
      "Epoch [1972/5000] | D Loss: 0.6210 | G Loss: 2.0418\n",
      "Epoch [1973/5000] | D Loss: 0.5921 | G Loss: 1.9159\n",
      "Epoch [1974/5000] | D Loss: 0.5732 | G Loss: 3.1011\n",
      "Epoch [1975/5000] | D Loss: 0.6624 | G Loss: 5.3246\n",
      "Epoch [1976/5000] | D Loss: 0.5531 | G Loss: 3.8427\n",
      "Epoch [1977/5000] | D Loss: 0.5712 | G Loss: 2.9929\n",
      "Epoch [1978/5000] | D Loss: 0.6286 | G Loss: 2.4016\n",
      "Epoch [1979/5000] | D Loss: 0.6168 | G Loss: 2.6455\n",
      "Epoch [1980/5000] | D Loss: 0.6345 | G Loss: 2.6934\n",
      "Epoch [1981/5000] | D Loss: 0.6097 | G Loss: 2.9330\n",
      "Epoch [1982/5000] | D Loss: 0.6027 | G Loss: 3.6773\n",
      "Epoch [1983/5000] | D Loss: 0.7973 | G Loss: 1.7135\n",
      "Epoch [1984/5000] | D Loss: 0.9074 | G Loss: 2.0879\n",
      "Epoch [1985/5000] | D Loss: 0.6083 | G Loss: 3.1250\n",
      "Epoch [1986/5000] | D Loss: 0.7607 | G Loss: 3.1117\n",
      "Epoch [1987/5000] | D Loss: 0.6371 | G Loss: 3.5187\n",
      "Epoch [1988/5000] | D Loss: 0.5785 | G Loss: 2.7763\n",
      "Epoch [1989/5000] | D Loss: 0.5939 | G Loss: 3.0566\n",
      "Epoch [1990/5000] | D Loss: 0.5648 | G Loss: 3.7201\n",
      "Epoch [1991/5000] | D Loss: 0.6338 | G Loss: 3.1908\n",
      "Epoch [1992/5000] | D Loss: 0.5903 | G Loss: 4.4308\n",
      "Epoch [1993/5000] | D Loss: 2.7879 | G Loss: 0.6082\n",
      "Epoch [1994/5000] | D Loss: 0.8158 | G Loss: 2.5741\n",
      "Epoch [1995/5000] | D Loss: 0.6131 | G Loss: 3.2491\n",
      "Epoch [1996/5000] | D Loss: 0.7421 | G Loss: 2.0907\n",
      "Epoch [1997/5000] | D Loss: 0.5942 | G Loss: 2.5325\n",
      "Epoch [1998/5000] | D Loss: 0.6046 | G Loss: 3.1158\n",
      "Epoch [1999/5000] | D Loss: 0.7236 | G Loss: 2.3048\n",
      "Epoch [2000/5000] | D Loss: 0.5906 | G Loss: 2.5288\n",
      "Epoch 2000 FID Score: 183.4286\n",
      "Epoch [2001/5000] | D Loss: 0.6859 | G Loss: 3.0097\n",
      "Epoch [2002/5000] | D Loss: 0.6069 | G Loss: 2.7862\n",
      "Epoch [2003/5000] | D Loss: 0.5769 | G Loss: 3.8250\n",
      "Epoch [2004/5000] | D Loss: 0.6214 | G Loss: 2.4712\n",
      "Epoch [2005/5000] | D Loss: 0.5630 | G Loss: 3.3486\n",
      "Epoch [2006/5000] | D Loss: 0.6125 | G Loss: 2.3209\n",
      "Epoch [2007/5000] | D Loss: 0.9370 | G Loss: 2.4343\n",
      "Epoch [2008/5000] | D Loss: 0.8989 | G Loss: 2.4490\n",
      "Epoch [2009/5000] | D Loss: 1.2980 | G Loss: 1.8078\n",
      "Epoch [2010/5000] | D Loss: 0.5777 | G Loss: 3.3261\n",
      "Epoch [2011/5000] | D Loss: 0.5768 | G Loss: 2.7141\n",
      "Epoch [2012/5000] | D Loss: 0.7928 | G Loss: 2.0158\n",
      "Epoch [2013/5000] | D Loss: 0.5695 | G Loss: 2.8313\n",
      "Epoch [2014/5000] | D Loss: 0.6225 | G Loss: 2.1366\n",
      "Epoch [2015/5000] | D Loss: 0.7128 | G Loss: 3.2045\n",
      "Epoch [2016/5000] | D Loss: 0.6280 | G Loss: 3.6423\n",
      "Epoch [2017/5000] | D Loss: 0.6264 | G Loss: 4.1789\n",
      "Epoch [2018/5000] | D Loss: 0.5960 | G Loss: 3.3161\n",
      "Epoch [2019/5000] | D Loss: 0.5978 | G Loss: 3.7003\n",
      "Epoch [2020/5000] | D Loss: 0.6512 | G Loss: 2.6597\n",
      "Epoch [2021/5000] | D Loss: 0.6428 | G Loss: 2.4733\n",
      "Epoch [2022/5000] | D Loss: 0.5952 | G Loss: 2.6500\n",
      "Epoch [2023/5000] | D Loss: 0.7667 | G Loss: 2.4406\n",
      "Epoch [2024/5000] | D Loss: 0.6136 | G Loss: 2.6398\n",
      "Epoch [2025/5000] | D Loss: 0.6896 | G Loss: 2.7551\n",
      "Epoch [2026/5000] | D Loss: 0.9381 | G Loss: 1.7484\n",
      "Epoch [2027/5000] | D Loss: 0.6108 | G Loss: 3.7147\n",
      "Epoch [2028/5000] | D Loss: 0.7112 | G Loss: 1.6149\n",
      "Epoch [2029/5000] | D Loss: 0.8443 | G Loss: 1.8296\n",
      "Epoch [2030/5000] | D Loss: 0.6667 | G Loss: 3.7367\n",
      "Epoch [2031/5000] | D Loss: 0.7633 | G Loss: 3.2346\n",
      "Epoch [2032/5000] | D Loss: 0.7066 | G Loss: 3.0975\n",
      "Epoch [2033/5000] | D Loss: 0.5833 | G Loss: 3.3463\n",
      "Epoch [2034/5000] | D Loss: 0.5869 | G Loss: 3.6889\n",
      "Epoch [2035/5000] | D Loss: 0.5822 | G Loss: 3.5906\n",
      "Epoch [2036/5000] | D Loss: 0.7032 | G Loss: 2.8600\n",
      "Epoch [2037/5000] | D Loss: 0.5889 | G Loss: 3.7802\n",
      "Epoch [2038/5000] | D Loss: 0.5991 | G Loss: 3.0394\n",
      "Epoch [2039/5000] | D Loss: 0.5710 | G Loss: 4.0222\n",
      "Epoch [2040/5000] | D Loss: 0.5952 | G Loss: 3.5853\n",
      "Epoch [2041/5000] | D Loss: 0.5919 | G Loss: 4.5574\n",
      "Epoch [2042/5000] | D Loss: 0.6275 | G Loss: 3.5359\n",
      "Epoch [2043/5000] | D Loss: 0.9505 | G Loss: 1.0298\n",
      "Epoch [2044/5000] | D Loss: 0.5853 | G Loss: 2.9183\n",
      "Epoch [2045/5000] | D Loss: 0.6513 | G Loss: 3.2400\n",
      "Epoch [2046/5000] | D Loss: 0.7976 | G Loss: 1.9301\n",
      "Epoch [2047/5000] | D Loss: 0.5833 | G Loss: 3.0270\n",
      "Epoch [2048/5000] | D Loss: 0.5936 | G Loss: 2.9063\n",
      "Epoch [2049/5000] | D Loss: 0.7822 | G Loss: 2.1286\n",
      "Epoch [2050/5000] | D Loss: 0.6079 | G Loss: 3.6510\n",
      "Epoch [2051/5000] | D Loss: 0.7897 | G Loss: 1.7984\n",
      "Epoch [2052/5000] | D Loss: 0.8923 | G Loss: 1.5610\n",
      "Epoch [2053/5000] | D Loss: 0.5792 | G Loss: 3.0680\n",
      "Epoch [2054/5000] | D Loss: 0.7426 | G Loss: 2.3911\n",
      "Epoch [2055/5000] | D Loss: 0.6642 | G Loss: 2.8443\n",
      "Epoch [2056/5000] | D Loss: 0.5681 | G Loss: 3.0427\n",
      "Epoch [2057/5000] | D Loss: 0.6069 | G Loss: 3.7702\n",
      "Epoch [2058/5000] | D Loss: 0.5914 | G Loss: 2.8881\n",
      "Epoch [2059/5000] | D Loss: 0.5857 | G Loss: 4.3553\n",
      "Epoch [2060/5000] | D Loss: 0.8747 | G Loss: 3.0970\n",
      "Epoch [2061/5000] | D Loss: 0.7337 | G Loss: 2.9545\n",
      "Epoch [2062/5000] | D Loss: 0.6398 | G Loss: 3.0434\n",
      "Epoch [2063/5000] | D Loss: 0.5961 | G Loss: 3.1580\n",
      "Epoch [2064/5000] | D Loss: 0.6019 | G Loss: 4.3800\n",
      "Epoch [2065/5000] | D Loss: 0.8862 | G Loss: 1.7110\n",
      "Epoch [2066/5000] | D Loss: 0.9407 | G Loss: 2.4442\n",
      "Epoch [2067/5000] | D Loss: 0.5943 | G Loss: 3.4576\n",
      "Epoch [2068/5000] | D Loss: 0.5589 | G Loss: 3.6724\n",
      "Epoch [2069/5000] | D Loss: 0.6265 | G Loss: 3.1597\n",
      "Epoch [2070/5000] | D Loss: 1.1127 | G Loss: 2.6175\n",
      "Epoch [2071/5000] | D Loss: 0.7997 | G Loss: 2.4229\n",
      "Epoch [2072/5000] | D Loss: 0.6001 | G Loss: 2.5223\n",
      "Epoch [2073/5000] | D Loss: 0.6055 | G Loss: 2.7163\n",
      "Epoch [2074/5000] | D Loss: 0.5782 | G Loss: 3.2673\n",
      "Epoch [2075/5000] | D Loss: 0.7963 | G Loss: 2.9036\n",
      "Epoch [2076/5000] | D Loss: 1.1037 | G Loss: 1.3163\n",
      "Epoch [2077/5000] | D Loss: 0.6879 | G Loss: 1.9360\n",
      "Epoch [2078/5000] | D Loss: 0.5787 | G Loss: 2.8822\n",
      "Epoch [2079/5000] | D Loss: 0.6726 | G Loss: 2.8321\n",
      "Epoch [2080/5000] | D Loss: 0.7796 | G Loss: 2.6779\n",
      "Epoch [2081/5000] | D Loss: 0.6091 | G Loss: 2.2827\n",
      "Epoch [2082/5000] | D Loss: 0.7962 | G Loss: 3.0469\n",
      "Epoch [2083/5000] | D Loss: 0.6106 | G Loss: 3.4784\n",
      "Epoch [2084/5000] | D Loss: 0.6402 | G Loss: 2.0555\n",
      "Epoch [2085/5000] | D Loss: 0.5967 | G Loss: 2.9728\n",
      "Epoch [2086/5000] | D Loss: 0.5728 | G Loss: 3.0174\n",
      "Epoch [2087/5000] | D Loss: 0.5894 | G Loss: 3.0289\n",
      "Epoch [2088/5000] | D Loss: 0.5936 | G Loss: 2.7048\n",
      "Epoch [2089/5000] | D Loss: 0.6223 | G Loss: 3.1977\n",
      "Epoch [2090/5000] | D Loss: 0.6678 | G Loss: 2.6165\n",
      "Epoch [2091/5000] | D Loss: 0.8597 | G Loss: 2.2210\n",
      "Epoch [2092/5000] | D Loss: 0.6573 | G Loss: 2.1251\n",
      "Epoch [2093/5000] | D Loss: 0.5772 | G Loss: 3.0766\n",
      "Epoch [2094/5000] | D Loss: 0.8495 | G Loss: 1.8134\n",
      "Epoch [2095/5000] | D Loss: 0.6032 | G Loss: 3.3436\n",
      "Epoch [2096/5000] | D Loss: 0.5976 | G Loss: 3.7385\n",
      "Epoch [2097/5000] | D Loss: 0.6121 | G Loss: 3.2552\n",
      "Epoch [2098/5000] | D Loss: 1.8090 | G Loss: 0.5953\n",
      "Epoch [2099/5000] | D Loss: 0.9367 | G Loss: 2.2802\n",
      "Epoch [2100/5000] | D Loss: 0.6819 | G Loss: 1.6120\n",
      "Epoch 2100 FID Score: 183.4933\n",
      "Saved improved model at Epoch 2100 with FID 183.4933\n",
      "Epoch [2101/5000] | D Loss: 0.6393 | G Loss: 2.5898\n",
      "Epoch [2102/5000] | D Loss: 0.6042 | G Loss: 2.7399\n",
      "Epoch [2103/5000] | D Loss: 0.5749 | G Loss: 3.3220\n",
      "Epoch [2104/5000] | D Loss: 0.7676 | G Loss: 1.4447\n",
      "Epoch [2105/5000] | D Loss: 0.7204 | G Loss: 1.9110\n",
      "Epoch [2106/5000] | D Loss: 0.5900 | G Loss: 4.0050\n",
      "Epoch [2107/5000] | D Loss: 1.0081 | G Loss: 1.3418\n",
      "Epoch [2108/5000] | D Loss: 0.6211 | G Loss: 2.3253\n",
      "Epoch [2109/5000] | D Loss: 0.6433 | G Loss: 3.6355\n",
      "Epoch [2110/5000] | D Loss: 0.6423 | G Loss: 3.1421\n",
      "Epoch [2111/5000] | D Loss: 0.7065 | G Loss: 2.2688\n",
      "Epoch [2112/5000] | D Loss: 0.6984 | G Loss: 4.1362\n",
      "Epoch [2113/5000] | D Loss: 0.5804 | G Loss: 3.1425\n",
      "Epoch [2114/5000] | D Loss: 0.5845 | G Loss: 2.8697\n",
      "Epoch [2115/5000] | D Loss: 0.5869 | G Loss: 3.3594\n",
      "Epoch [2116/5000] | D Loss: 0.6614 | G Loss: 3.5376\n",
      "Epoch [2117/5000] | D Loss: 0.5884 | G Loss: 2.9253\n",
      "Epoch [2118/5000] | D Loss: 0.6694 | G Loss: 2.5177\n",
      "Epoch [2119/5000] | D Loss: 0.6227 | G Loss: 2.6932\n",
      "Epoch [2120/5000] | D Loss: 1.1991 | G Loss: 0.9016\n",
      "Epoch [2121/5000] | D Loss: 0.5721 | G Loss: 3.5209\n",
      "Epoch [2122/5000] | D Loss: 0.7593 | G Loss: 2.2062\n",
      "Epoch [2123/5000] | D Loss: 0.6168 | G Loss: 2.7235\n",
      "Epoch [2124/5000] | D Loss: 0.6554 | G Loss: 2.8573\n",
      "Epoch [2125/5000] | D Loss: 0.5845 | G Loss: 3.1923\n",
      "Epoch [2126/5000] | D Loss: 0.6215 | G Loss: 3.9936\n",
      "Epoch [2127/5000] | D Loss: 0.6328 | G Loss: 3.3185\n",
      "Epoch [2128/5000] | D Loss: 0.6164 | G Loss: 4.0895\n",
      "Epoch [2129/5000] | D Loss: 0.7083 | G Loss: 2.4546\n",
      "Epoch [2130/5000] | D Loss: 0.6072 | G Loss: 3.2468\n",
      "Epoch [2131/5000] | D Loss: 0.6219 | G Loss: 3.7639\n",
      "Epoch [2132/5000] | D Loss: 0.6031 | G Loss: 2.7095\n",
      "Epoch [2133/5000] | D Loss: 0.7935 | G Loss: 1.9878\n",
      "Epoch [2134/5000] | D Loss: 0.5885 | G Loss: 3.5375\n",
      "Epoch [2135/5000] | D Loss: 0.5999 | G Loss: 3.3733\n",
      "Epoch [2136/5000] | D Loss: 0.6159 | G Loss: 2.0269\n",
      "Epoch [2137/5000] | D Loss: 0.6017 | G Loss: 2.9545\n",
      "Epoch [2138/5000] | D Loss: 0.7343 | G Loss: 2.6427\n",
      "Epoch [2139/5000] | D Loss: 0.6089 | G Loss: 2.5415\n",
      "Epoch [2140/5000] | D Loss: 0.6290 | G Loss: 2.4445\n",
      "Epoch [2141/5000] | D Loss: 0.6746 | G Loss: 2.4037\n",
      "Epoch [2142/5000] | D Loss: 0.6605 | G Loss: 1.2008\n",
      "Epoch [2143/5000] | D Loss: 0.8725 | G Loss: 2.2470\n",
      "Epoch [2144/5000] | D Loss: 0.5756 | G Loss: 3.6226\n",
      "Epoch [2145/5000] | D Loss: 0.6194 | G Loss: 2.5646\n",
      "Epoch [2146/5000] | D Loss: 0.5704 | G Loss: 3.3496\n",
      "Epoch [2147/5000] | D Loss: 0.5862 | G Loss: 3.0657\n",
      "Epoch [2148/5000] | D Loss: 0.5849 | G Loss: 2.6163\n",
      "Epoch [2149/5000] | D Loss: 0.7031 | G Loss: 2.6071\n",
      "Epoch [2150/5000] | D Loss: 0.8391 | G Loss: 2.8300\n",
      "Epoch [2151/5000] | D Loss: 0.6515 | G Loss: 3.1788\n",
      "Epoch [2152/5000] | D Loss: 0.5934 | G Loss: 2.9453\n",
      "Epoch [2153/5000] | D Loss: 0.5778 | G Loss: 3.2827\n",
      "Epoch [2154/5000] | D Loss: 0.5665 | G Loss: 3.4432\n",
      "Epoch [2155/5000] | D Loss: 0.8657 | G Loss: 2.2887\n",
      "Epoch [2156/5000] | D Loss: 0.6824 | G Loss: 3.2830\n",
      "Epoch [2157/5000] | D Loss: 0.6491 | G Loss: 2.4042\n",
      "Epoch [2158/5000] | D Loss: 0.5860 | G Loss: 2.3166\n",
      "Epoch [2159/5000] | D Loss: 0.6289 | G Loss: 3.4543\n",
      "Epoch [2160/5000] | D Loss: 0.6285 | G Loss: 3.4721\n",
      "Epoch [2161/5000] | D Loss: 0.6906 | G Loss: 3.2748\n",
      "Epoch [2162/5000] | D Loss: 0.6570 | G Loss: 2.6490\n",
      "Epoch [2163/5000] | D Loss: 0.6589 | G Loss: 2.8107\n",
      "Epoch [2164/5000] | D Loss: 1.1259 | G Loss: 1.2617\n",
      "Epoch [2165/5000] | D Loss: 0.9516 | G Loss: 1.7883\n",
      "Epoch [2166/5000] | D Loss: 0.6589 | G Loss: 3.1061\n",
      "Epoch [2167/5000] | D Loss: 0.6794 | G Loss: 2.0511\n",
      "Epoch [2168/5000] | D Loss: 0.6444 | G Loss: 2.6721\n",
      "Epoch [2169/5000] | D Loss: 0.6178 | G Loss: 2.8093\n",
      "Epoch [2170/5000] | D Loss: 0.5980 | G Loss: 2.9671\n",
      "Epoch [2171/5000] | D Loss: 0.5934 | G Loss: 3.5471\n",
      "Epoch [2172/5000] | D Loss: 0.7209 | G Loss: 2.4907\n",
      "Epoch [2173/5000] | D Loss: 0.6300 | G Loss: 3.6018\n",
      "Epoch [2174/5000] | D Loss: 0.5994 | G Loss: 2.8781\n",
      "Epoch [2175/5000] | D Loss: 0.6155 | G Loss: 2.5206\n",
      "Epoch [2176/5000] | D Loss: 0.6366 | G Loss: 3.0605\n",
      "Epoch [2177/5000] | D Loss: 0.7386 | G Loss: 2.8833\n",
      "Epoch [2178/5000] | D Loss: 0.6198 | G Loss: 4.0396\n",
      "Epoch [2179/5000] | D Loss: 0.5886 | G Loss: 2.8071\n",
      "Epoch [2180/5000] | D Loss: 0.5677 | G Loss: 2.6058\n",
      "Epoch [2181/5000] | D Loss: 1.0448 | G Loss: 2.0663\n",
      "Epoch [2182/5000] | D Loss: 0.5623 | G Loss: 3.2818\n",
      "Epoch [2183/5000] | D Loss: 0.7168 | G Loss: 2.9585\n",
      "Epoch [2184/5000] | D Loss: 0.6321 | G Loss: 2.7379\n",
      "Epoch [2185/5000] | D Loss: 0.5961 | G Loss: 3.5682\n",
      "Epoch [2186/5000] | D Loss: 0.6492 | G Loss: 3.2996\n",
      "Epoch [2187/5000] | D Loss: 1.0011 | G Loss: 2.7042\n",
      "Epoch [2188/5000] | D Loss: 0.6426 | G Loss: 2.7767\n",
      "Epoch [2189/5000] | D Loss: 0.5844 | G Loss: 3.1468\n",
      "Epoch [2190/5000] | D Loss: 1.1778 | G Loss: 0.8387\n",
      "Epoch [2191/5000] | D Loss: 0.6533 | G Loss: 1.8927\n",
      "Epoch [2192/5000] | D Loss: 0.8161 | G Loss: 2.0777\n",
      "Epoch [2193/5000] | D Loss: 0.5955 | G Loss: 2.1834\n",
      "Epoch [2194/5000] | D Loss: 0.6589 | G Loss: 3.1950\n",
      "Epoch [2195/5000] | D Loss: 0.5700 | G Loss: 3.7376\n",
      "Epoch [2196/5000] | D Loss: 0.7102 | G Loss: 1.5797\n",
      "Epoch [2197/5000] | D Loss: 0.7051 | G Loss: 3.7148\n",
      "Epoch [2198/5000] | D Loss: 0.5918 | G Loss: 2.7709\n",
      "Epoch [2199/5000] | D Loss: 0.9568 | G Loss: 2.4710\n",
      "Epoch [2200/5000] | D Loss: 0.5698 | G Loss: 3.1609\n",
      "Epoch 2200 FID Score: 184.9195\n",
      "Reducing learning rates to Generator: 5e-05, Discriminator: 5e-05\n",
      "Epoch [2201/5000] | D Loss: 0.6065 | G Loss: 2.8668\n",
      "Epoch [2202/5000] | D Loss: 0.7892 | G Loss: 1.8206\n",
      "Epoch [2203/5000] | D Loss: 0.5655 | G Loss: 3.3227\n",
      "Epoch [2204/5000] | D Loss: 0.6430 | G Loss: 2.2616\n",
      "Epoch [2205/5000] | D Loss: 0.5859 | G Loss: 2.9043\n",
      "Epoch [2206/5000] | D Loss: 0.7059 | G Loss: 3.7606\n",
      "Epoch [2207/5000] | D Loss: 0.6022 | G Loss: 2.4140\n",
      "Epoch [2208/5000] | D Loss: 0.5692 | G Loss: 3.4478\n",
      "Epoch [2209/5000] | D Loss: 0.5779 | G Loss: 3.2181\n",
      "Epoch [2210/5000] | D Loss: 0.6307 | G Loss: 2.6607\n",
      "Epoch [2211/5000] | D Loss: 0.6282 | G Loss: 2.5567\n",
      "Epoch [2212/5000] | D Loss: 0.5601 | G Loss: 3.0981\n",
      "Epoch [2213/5000] | D Loss: 1.7395 | G Loss: 0.7595\n",
      "Epoch [2214/5000] | D Loss: 0.7847 | G Loss: 2.1475\n",
      "Epoch [2215/5000] | D Loss: 0.6487 | G Loss: 2.9840\n",
      "Epoch [2216/5000] | D Loss: 0.5837 | G Loss: 3.7926\n",
      "Epoch [2217/5000] | D Loss: 0.5712 | G Loss: 3.2795\n",
      "Epoch [2218/5000] | D Loss: 0.6726 | G Loss: 2.3745\n",
      "Epoch [2219/5000] | D Loss: 0.5628 | G Loss: 3.2825\n",
      "Epoch [2220/5000] | D Loss: 0.5611 | G Loss: 2.6849\n",
      "Epoch [2221/5000] | D Loss: 0.5674 | G Loss: 3.0993\n",
      "Epoch [2222/5000] | D Loss: 0.5775 | G Loss: 2.5919\n",
      "Epoch [2223/5000] | D Loss: 0.6208 | G Loss: 3.0455\n",
      "Epoch [2224/5000] | D Loss: 0.6380 | G Loss: 2.5312\n",
      "Epoch [2225/5000] | D Loss: 0.5581 | G Loss: 2.9385\n",
      "Epoch [2226/5000] | D Loss: 0.7493 | G Loss: 2.0051\n",
      "Epoch [2227/5000] | D Loss: 0.7610 | G Loss: 2.1312\n",
      "Epoch [2228/5000] | D Loss: 0.6008 | G Loss: 2.3264\n",
      "Epoch [2229/5000] | D Loss: 0.7754 | G Loss: 1.8835\n",
      "Epoch [2230/5000] | D Loss: 0.5407 | G Loss: 3.1599\n",
      "Epoch [2231/5000] | D Loss: 0.7057 | G Loss: 2.4825\n",
      "Epoch [2232/5000] | D Loss: 0.5641 | G Loss: 3.2676\n",
      "Epoch [2233/5000] | D Loss: 0.5696 | G Loss: 3.1943\n",
      "Epoch [2234/5000] | D Loss: 0.5910 | G Loss: 2.6621\n",
      "Epoch [2235/5000] | D Loss: 0.5800 | G Loss: 3.4631\n",
      "Epoch [2236/5000] | D Loss: 0.5832 | G Loss: 2.6557\n",
      "Epoch [2237/5000] | D Loss: 0.5791 | G Loss: 4.2732\n",
      "Epoch [2238/5000] | D Loss: 0.5873 | G Loss: 2.1078\n",
      "Epoch [2239/5000] | D Loss: 0.6074 | G Loss: 3.4548\n",
      "Epoch [2240/5000] | D Loss: 0.6307 | G Loss: 2.7444\n",
      "Epoch [2241/5000] | D Loss: 0.5996 | G Loss: 3.5645\n",
      "Epoch [2242/5000] | D Loss: 0.7536 | G Loss: 2.9090\n",
      "Epoch [2243/5000] | D Loss: 0.6193 | G Loss: 2.5841\n",
      "Epoch [2244/5000] | D Loss: 0.5865 | G Loss: 2.7819\n",
      "Epoch [2245/5000] | D Loss: 0.5915 | G Loss: 3.0111\n",
      "Epoch [2246/5000] | D Loss: 0.7197 | G Loss: 2.0734\n",
      "Epoch [2247/5000] | D Loss: 0.5488 | G Loss: 2.7074\n",
      "Epoch [2248/5000] | D Loss: 0.6232 | G Loss: 3.7136\n",
      "Epoch [2249/5000] | D Loss: 0.5602 | G Loss: 3.8175\n",
      "Epoch [2250/5000] | D Loss: 0.6108 | G Loss: 2.6673\n",
      "Epoch [2251/5000] | D Loss: 1.2347 | G Loss: 1.2042\n",
      "Epoch [2252/5000] | D Loss: 0.6134 | G Loss: 3.2135\n",
      "Epoch [2253/5000] | D Loss: 0.6391 | G Loss: 3.3334\n",
      "Epoch [2254/5000] | D Loss: 0.9209 | G Loss: 2.3048\n",
      "Epoch [2255/5000] | D Loss: 0.7996 | G Loss: 2.5083\n",
      "Epoch [2256/5000] | D Loss: 0.6110 | G Loss: 3.2867\n",
      "Epoch [2257/5000] | D Loss: 0.5755 | G Loss: 3.3238\n",
      "Epoch [2258/5000] | D Loss: 0.6083 | G Loss: 2.8100\n",
      "Epoch [2259/5000] | D Loss: 0.6300 | G Loss: 2.7647\n",
      "Epoch [2260/5000] | D Loss: 0.5937 | G Loss: 2.3710\n",
      "Epoch [2261/5000] | D Loss: 0.5960 | G Loss: 3.7216\n",
      "Epoch [2262/5000] | D Loss: 0.7260 | G Loss: 2.6191\n",
      "Epoch [2263/5000] | D Loss: 0.5907 | G Loss: 3.2575\n",
      "Epoch [2264/5000] | D Loss: 0.6237 | G Loss: 2.9841\n",
      "Epoch [2265/5000] | D Loss: 0.5695 | G Loss: 3.2591\n",
      "Epoch [2266/5000] | D Loss: 0.8633 | G Loss: 1.7921\n",
      "Epoch [2267/5000] | D Loss: 0.5996 | G Loss: 3.3409\n",
      "Epoch [2268/5000] | D Loss: 0.6504 | G Loss: 3.3441\n",
      "Epoch [2269/5000] | D Loss: 0.6180 | G Loss: 2.9170\n",
      "Epoch [2270/5000] | D Loss: 0.5709 | G Loss: 3.6796\n",
      "Epoch [2271/5000] | D Loss: 0.6467 | G Loss: 2.7571\n",
      "Epoch [2272/5000] | D Loss: 0.6070 | G Loss: 2.8514\n",
      "Epoch [2273/5000] | D Loss: 0.6339 | G Loss: 3.4950\n",
      "Epoch [2274/5000] | D Loss: 0.6924 | G Loss: 2.6607\n",
      "Epoch [2275/5000] | D Loss: 0.5843 | G Loss: 3.3693\n",
      "Epoch [2276/5000] | D Loss: 0.6037 | G Loss: 2.3039\n",
      "Epoch [2277/5000] | D Loss: 0.5841 | G Loss: 2.8564\n",
      "Epoch [2278/5000] | D Loss: 0.6152 | G Loss: 2.3040\n",
      "Epoch [2279/5000] | D Loss: 0.7712 | G Loss: 1.7152\n",
      "Epoch [2280/5000] | D Loss: 0.5694 | G Loss: 2.8754\n",
      "Epoch [2281/5000] | D Loss: 0.6595 | G Loss: 2.2430\n",
      "Epoch [2282/5000] | D Loss: 0.6023 | G Loss: 2.2581\n",
      "Epoch [2283/5000] | D Loss: 0.6461 | G Loss: 2.6754\n",
      "Epoch [2284/5000] | D Loss: 0.6951 | G Loss: 2.1219\n",
      "Epoch [2285/5000] | D Loss: 0.6641 | G Loss: 3.0263\n",
      "Epoch [2286/5000] | D Loss: 0.8337 | G Loss: 2.8138\n",
      "Epoch [2287/5000] | D Loss: 0.6334 | G Loss: 2.0287\n",
      "Epoch [2288/5000] | D Loss: 0.6394 | G Loss: 3.9119\n",
      "Epoch [2289/5000] | D Loss: 0.6035 | G Loss: 2.8343\n",
      "Epoch [2290/5000] | D Loss: 0.6414 | G Loss: 2.9173\n",
      "Epoch [2291/5000] | D Loss: 0.6002 | G Loss: 3.2659\n",
      "Epoch [2292/5000] | D Loss: 0.9136 | G Loss: 0.8107\n",
      "Epoch [2293/5000] | D Loss: 0.6610 | G Loss: 2.0589\n",
      "Epoch [2294/5000] | D Loss: 0.6977 | G Loss: 2.4399\n",
      "Epoch [2295/5000] | D Loss: 0.5929 | G Loss: 2.9185\n",
      "Epoch [2296/5000] | D Loss: 0.5763 | G Loss: 3.6785\n",
      "Epoch [2297/5000] | D Loss: 0.5670 | G Loss: 3.2456\n",
      "Epoch [2298/5000] | D Loss: 0.5769 | G Loss: 4.2292\n",
      "Epoch [2299/5000] | D Loss: 1.1120 | G Loss: 0.9369\n",
      "Epoch [2300/5000] | D Loss: 0.6358 | G Loss: 4.5461\n",
      "Epoch 2300 FID Score: 188.8905\n",
      "Reducing learning rates to Generator: 2.5e-05, Discriminator: 2.5e-05\n",
      "Epoch [2301/5000] | D Loss: 0.5853 | G Loss: 3.4010\n",
      "Epoch [2302/5000] | D Loss: 0.6276 | G Loss: 2.8596\n",
      "Epoch [2303/5000] | D Loss: 0.6717 | G Loss: 2.4863\n",
      "Epoch [2304/5000] | D Loss: 0.6274 | G Loss: 3.4602\n",
      "Epoch [2305/5000] | D Loss: 0.5759 | G Loss: 2.4103\n",
      "Epoch [2306/5000] | D Loss: 0.6894 | G Loss: 3.6484\n",
      "Epoch [2307/5000] | D Loss: 0.7049 | G Loss: 1.4988\n",
      "Epoch [2308/5000] | D Loss: 0.5645 | G Loss: 2.4377\n",
      "Epoch [2309/5000] | D Loss: 0.6914 | G Loss: 2.5909\n",
      "Epoch [2310/5000] | D Loss: 0.5830 | G Loss: 2.9303\n",
      "Epoch [2311/5000] | D Loss: 0.5991 | G Loss: 3.3064\n",
      "Epoch [2312/5000] | D Loss: 0.5669 | G Loss: 2.9218\n",
      "Epoch [2313/5000] | D Loss: 0.5802 | G Loss: 3.2050\n",
      "Epoch [2314/5000] | D Loss: 0.7268 | G Loss: 2.9356\n",
      "Epoch [2315/5000] | D Loss: 0.6099 | G Loss: 3.5154\n",
      "Epoch [2316/5000] | D Loss: 0.5916 | G Loss: 2.9692\n",
      "Epoch [2317/5000] | D Loss: 0.5994 | G Loss: 3.1785\n",
      "Epoch [2318/5000] | D Loss: 0.5939 | G Loss: 2.8470\n",
      "Epoch [2319/5000] | D Loss: 0.6740 | G Loss: 2.0053\n",
      "Epoch [2320/5000] | D Loss: 0.5657 | G Loss: 2.8026\n",
      "Epoch [2321/5000] | D Loss: 0.5710 | G Loss: 2.8692\n",
      "Epoch [2322/5000] | D Loss: 0.5956 | G Loss: 3.1261\n",
      "Epoch [2323/5000] | D Loss: 0.6150 | G Loss: 2.8860\n",
      "Epoch [2324/5000] | D Loss: 0.8028 | G Loss: 2.2527\n",
      "Epoch [2325/5000] | D Loss: 0.5894 | G Loss: 2.5822\n",
      "Epoch [2326/5000] | D Loss: 0.6822 | G Loss: 3.7888\n",
      "Epoch [2327/5000] | D Loss: 0.5566 | G Loss: 3.1964\n",
      "Epoch [2328/5000] | D Loss: 0.6084 | G Loss: 2.9242\n",
      "Epoch [2329/5000] | D Loss: 0.5980 | G Loss: 2.7424\n",
      "Epoch [2330/5000] | D Loss: 0.5927 | G Loss: 3.4616\n",
      "Epoch [2331/5000] | D Loss: 0.5776 | G Loss: 2.9570\n",
      "Epoch [2332/5000] | D Loss: 0.5883 | G Loss: 2.6748\n",
      "Epoch [2333/5000] | D Loss: 0.5842 | G Loss: 3.5948\n",
      "Epoch [2334/5000] | D Loss: 0.7009 | G Loss: 2.4491\n",
      "Epoch [2335/5000] | D Loss: 0.5805 | G Loss: 2.9923\n",
      "Epoch [2336/5000] | D Loss: 0.5968 | G Loss: 2.7366\n",
      "Epoch [2337/5000] | D Loss: 0.5835 | G Loss: 3.5469\n",
      "Epoch [2338/5000] | D Loss: 0.5821 | G Loss: 3.1435\n",
      "Epoch [2339/5000] | D Loss: 0.6355 | G Loss: 2.2893\n",
      "Epoch [2340/5000] | D Loss: 0.5843 | G Loss: 2.5712\n",
      "Epoch [2341/5000] | D Loss: 0.5540 | G Loss: 2.5297\n",
      "Epoch [2342/5000] | D Loss: 0.7035 | G Loss: 1.2801\n",
      "Epoch [2343/5000] | D Loss: 0.6710 | G Loss: 2.5555\n",
      "Epoch [2344/5000] | D Loss: 0.6026 | G Loss: 2.5033\n",
      "Epoch [2345/5000] | D Loss: 0.5790 | G Loss: 4.5716\n",
      "Epoch [2346/5000] | D Loss: 0.6297 | G Loss: 2.5984\n",
      "Epoch [2347/5000] | D Loss: 0.6071 | G Loss: 2.6889\n",
      "Epoch [2348/5000] | D Loss: 0.6419 | G Loss: 2.6785\n",
      "Epoch [2349/5000] | D Loss: 0.7796 | G Loss: 1.7326\n",
      "Epoch [2350/5000] | D Loss: 0.5605 | G Loss: 3.0571\n",
      "Epoch [2351/5000] | D Loss: 0.5612 | G Loss: 3.4086\n",
      "Epoch [2352/5000] | D Loss: 0.5674 | G Loss: 3.6698\n",
      "Epoch [2353/5000] | D Loss: 0.8465 | G Loss: 2.1672\n",
      "Epoch [2354/5000] | D Loss: 0.6102 | G Loss: 2.3139\n",
      "Epoch [2355/5000] | D Loss: 0.6725 | G Loss: 3.2655\n",
      "Epoch [2356/5000] | D Loss: 0.5800 | G Loss: 2.8706\n",
      "Epoch [2357/5000] | D Loss: 0.7700 | G Loss: 2.9080\n",
      "Epoch [2358/5000] | D Loss: 0.6000 | G Loss: 2.6603\n",
      "Epoch [2359/5000] | D Loss: 0.6472 | G Loss: 2.9769\n",
      "Epoch [2360/5000] | D Loss: 0.6011 | G Loss: 2.7672\n",
      "Epoch [2361/5000] | D Loss: 0.5888 | G Loss: 2.7867\n",
      "Epoch [2362/5000] | D Loss: 0.6201 | G Loss: 2.5114\n",
      "Epoch [2363/5000] | D Loss: 0.5935 | G Loss: 2.4861\n",
      "Epoch [2364/5000] | D Loss: 0.5506 | G Loss: 3.2506\n",
      "Epoch [2365/5000] | D Loss: 0.6147 | G Loss: 3.5962\n",
      "Epoch [2366/5000] | D Loss: 0.5910 | G Loss: 3.1685\n",
      "Epoch [2367/5000] | D Loss: 0.5774 | G Loss: 4.2257\n",
      "Epoch [2368/5000] | D Loss: 0.5512 | G Loss: 3.7970\n",
      "Epoch [2369/5000] | D Loss: 0.6414 | G Loss: 2.9226\n",
      "Epoch [2370/5000] | D Loss: 0.6008 | G Loss: 2.8360\n",
      "Epoch [2371/5000] | D Loss: 0.5858 | G Loss: 2.6166\n",
      "Epoch [2372/5000] | D Loss: 0.6025 | G Loss: 3.3399\n",
      "Epoch [2373/5000] | D Loss: 0.6766 | G Loss: 3.0277\n",
      "Epoch [2374/5000] | D Loss: 0.6313 | G Loss: 4.2161\n",
      "Epoch [2375/5000] | D Loss: 0.6149 | G Loss: 2.6576\n",
      "Epoch [2376/5000] | D Loss: 0.7459 | G Loss: 3.0914\n",
      "Epoch [2377/5000] | D Loss: 0.6072 | G Loss: 3.2314\n",
      "Epoch [2378/5000] | D Loss: 1.0574 | G Loss: 2.3092\n",
      "Epoch [2379/5000] | D Loss: 0.5946 | G Loss: 2.8058\n",
      "Epoch [2380/5000] | D Loss: 0.6931 | G Loss: 1.8147\n",
      "Epoch [2381/5000] | D Loss: 0.6564 | G Loss: 1.9751\n",
      "Epoch [2382/5000] | D Loss: 0.7997 | G Loss: 2.4314\n",
      "Epoch [2383/5000] | D Loss: 0.6020 | G Loss: 2.6469\n",
      "Epoch [2384/5000] | D Loss: 0.6102 | G Loss: 1.9234\n",
      "Epoch [2385/5000] | D Loss: 0.5884 | G Loss: 4.2861\n",
      "Epoch [2386/5000] | D Loss: 0.6041 | G Loss: 3.3218\n",
      "Epoch [2387/5000] | D Loss: 0.5607 | G Loss: 2.8810\n",
      "Epoch [2388/5000] | D Loss: 0.5520 | G Loss: 2.8052\n",
      "Epoch [2389/5000] | D Loss: 0.7044 | G Loss: 2.5549\n",
      "Epoch [2390/5000] | D Loss: 0.5881 | G Loss: 2.7346\n",
      "Epoch [2391/5000] | D Loss: 1.1518 | G Loss: 2.8646\n",
      "Epoch [2392/5000] | D Loss: 0.5958 | G Loss: 3.0184\n",
      "Epoch [2393/5000] | D Loss: 0.6303 | G Loss: 2.5051\n",
      "Epoch [2394/5000] | D Loss: 0.5859 | G Loss: 2.6854\n",
      "Epoch [2395/5000] | D Loss: 0.6373 | G Loss: 2.4227\n",
      "Epoch [2396/5000] | D Loss: 0.7247 | G Loss: 2.4958\n",
      "Epoch [2397/5000] | D Loss: 0.5878 | G Loss: 3.1472\n",
      "Epoch [2398/5000] | D Loss: 0.7006 | G Loss: 2.4107\n",
      "Epoch [2399/5000] | D Loss: 0.6891 | G Loss: 2.6227\n",
      "Epoch [2400/5000] | D Loss: 0.5992 | G Loss: 2.6275\n",
      "Epoch 2400 FID Score: 177.1025\n",
      "Saved improved model at Epoch 2400 with FID 177.1025\n",
      "Epoch [2401/5000] | D Loss: 0.6728 | G Loss: 2.3529\n",
      "Epoch [2402/5000] | D Loss: 0.5884 | G Loss: 2.9321\n",
      "Epoch [2403/5000] | D Loss: 0.5856 | G Loss: 3.4318\n",
      "Epoch [2404/5000] | D Loss: 0.5599 | G Loss: 2.8584\n",
      "Epoch [2405/5000] | D Loss: 1.0001 | G Loss: 1.6253\n",
      "Epoch [2406/5000] | D Loss: 0.5841 | G Loss: 3.2151\n",
      "Epoch [2407/5000] | D Loss: 0.5772 | G Loss: 2.7375\n",
      "Epoch [2408/5000] | D Loss: 0.5818 | G Loss: 2.7190\n",
      "Epoch [2409/5000] | D Loss: 0.7259 | G Loss: 2.5379\n",
      "Epoch [2410/5000] | D Loss: 0.6192 | G Loss: 2.9779\n",
      "Epoch [2411/5000] | D Loss: 0.5745 | G Loss: 3.9544\n",
      "Epoch [2412/5000] | D Loss: 0.6022 | G Loss: 2.9647\n",
      "Epoch [2413/5000] | D Loss: 0.5900 | G Loss: 2.5162\n",
      "Epoch [2414/5000] | D Loss: 0.6122 | G Loss: 2.5469\n",
      "Epoch [2415/5000] | D Loss: 0.6038 | G Loss: 2.1965\n",
      "Epoch [2416/5000] | D Loss: 0.5641 | G Loss: 2.9353\n",
      "Epoch [2417/5000] | D Loss: 0.5734 | G Loss: 3.1401\n",
      "Epoch [2418/5000] | D Loss: 1.2411 | G Loss: 2.2678\n",
      "Epoch [2419/5000] | D Loss: 0.6867 | G Loss: 2.2767\n",
      "Epoch [2420/5000] | D Loss: 0.6240 | G Loss: 2.7377\n",
      "Epoch [2421/5000] | D Loss: 0.5796 | G Loss: 3.5087\n",
      "Epoch [2422/5000] | D Loss: 0.5717 | G Loss: 2.5130\n",
      "Epoch [2423/5000] | D Loss: 0.6766 | G Loss: 2.4985\n",
      "Epoch [2424/5000] | D Loss: 0.7853 | G Loss: 2.2914\n",
      "Epoch [2425/5000] | D Loss: 0.5671 | G Loss: 2.7438\n",
      "Epoch [2426/5000] | D Loss: 0.8155 | G Loss: 2.8127\n",
      "Epoch [2427/5000] | D Loss: 0.5678 | G Loss: 3.3387\n",
      "Epoch [2428/5000] | D Loss: 0.6041 | G Loss: 2.4907\n",
      "Epoch [2429/5000] | D Loss: 0.5772 | G Loss: 2.8040\n",
      "Epoch [2430/5000] | D Loss: 0.6208 | G Loss: 2.6342\n",
      "Epoch [2431/5000] | D Loss: 0.6421 | G Loss: 3.0510\n",
      "Epoch [2432/5000] | D Loss: 0.5942 | G Loss: 3.1714\n",
      "Epoch [2433/5000] | D Loss: 0.5712 | G Loss: 2.5786\n",
      "Epoch [2434/5000] | D Loss: 0.6098 | G Loss: 2.7321\n",
      "Epoch [2435/5000] | D Loss: 0.5592 | G Loss: 3.1987\n",
      "Epoch [2436/5000] | D Loss: 0.6267 | G Loss: 3.8132\n",
      "Epoch [2437/5000] | D Loss: 0.6339 | G Loss: 2.3045\n",
      "Epoch [2438/5000] | D Loss: 0.6128 | G Loss: 2.8911\n",
      "Epoch [2439/5000] | D Loss: 0.6479 | G Loss: 1.9189\n",
      "Epoch [2440/5000] | D Loss: 0.5657 | G Loss: 2.6643\n",
      "Epoch [2441/5000] | D Loss: 0.5531 | G Loss: 4.0915\n",
      "Epoch [2442/5000] | D Loss: 0.5912 | G Loss: 2.7769\n",
      "Epoch [2443/5000] | D Loss: 1.0526 | G Loss: 1.5290\n",
      "Epoch [2444/5000] | D Loss: 0.5804 | G Loss: 3.1748\n",
      "Epoch [2445/5000] | D Loss: 0.5789 | G Loss: 3.2245\n",
      "Epoch [2446/5000] | D Loss: 0.5813 | G Loss: 3.2173\n",
      "Epoch [2447/5000] | D Loss: 0.5508 | G Loss: 2.9152\n",
      "Epoch [2448/5000] | D Loss: 0.5933 | G Loss: 2.8266\n",
      "Epoch [2449/5000] | D Loss: 0.8373 | G Loss: 3.3517\n",
      "Epoch [2450/5000] | D Loss: 0.5539 | G Loss: 4.0937\n",
      "Epoch [2451/5000] | D Loss: 0.5867 | G Loss: 2.9291\n",
      "Epoch [2452/5000] | D Loss: 0.5718 | G Loss: 3.3273\n",
      "Epoch [2453/5000] | D Loss: 0.6192 | G Loss: 3.3277\n",
      "Epoch [2454/5000] | D Loss: 0.6622 | G Loss: 2.1366\n",
      "Epoch [2455/5000] | D Loss: 0.6483 | G Loss: 2.1700\n",
      "Epoch [2456/5000] | D Loss: 0.5602 | G Loss: 3.3918\n",
      "Epoch [2457/5000] | D Loss: 0.6260 | G Loss: 2.5461\n",
      "Epoch [2458/5000] | D Loss: 0.7139 | G Loss: 2.2228\n",
      "Epoch [2459/5000] | D Loss: 0.5806 | G Loss: 2.9071\n",
      "Epoch [2460/5000] | D Loss: 0.5898 | G Loss: 2.7907\n",
      "Epoch [2461/5000] | D Loss: 0.5662 | G Loss: 3.6958\n",
      "Epoch [2462/5000] | D Loss: 0.5796 | G Loss: 2.9081\n",
      "Epoch [2463/5000] | D Loss: 0.6236 | G Loss: 3.1934\n",
      "Epoch [2464/5000] | D Loss: 0.5702 | G Loss: 2.8522\n",
      "Epoch [2465/5000] | D Loss: 0.5532 | G Loss: 3.5455\n",
      "Epoch [2466/5000] | D Loss: 0.7697 | G Loss: 2.3228\n",
      "Epoch [2467/5000] | D Loss: 0.5786 | G Loss: 2.7958\n",
      "Epoch [2468/5000] | D Loss: 0.8439 | G Loss: 2.1183\n",
      "Epoch [2469/5000] | D Loss: 0.6785 | G Loss: 2.5680\n",
      "Epoch [2470/5000] | D Loss: 0.5888 | G Loss: 3.4663\n",
      "Epoch [2471/5000] | D Loss: 0.7657 | G Loss: 2.3436\n",
      "Epoch [2472/5000] | D Loss: 0.5923 | G Loss: 3.7172\n",
      "Epoch [2473/5000] | D Loss: 0.6152 | G Loss: 3.0440\n",
      "Epoch [2474/5000] | D Loss: 0.6097 | G Loss: 3.2203\n",
      "Epoch [2475/5000] | D Loss: 0.6268 | G Loss: 3.3941\n",
      "Epoch [2476/5000] | D Loss: 0.5999 | G Loss: 3.5838\n",
      "Epoch [2477/5000] | D Loss: 0.5952 | G Loss: 3.3475\n",
      "Epoch [2478/5000] | D Loss: 0.5739 | G Loss: 3.9918\n",
      "Epoch [2479/5000] | D Loss: 0.5621 | G Loss: 2.5395\n",
      "Epoch [2480/5000] | D Loss: 0.5673 | G Loss: 3.0156\n",
      "Epoch [2481/5000] | D Loss: 0.5633 | G Loss: 4.3405\n",
      "Epoch [2482/5000] | D Loss: 0.6042 | G Loss: 2.7124\n",
      "Epoch [2483/5000] | D Loss: 0.8072 | G Loss: 2.3484\n",
      "Epoch [2484/5000] | D Loss: 0.5916 | G Loss: 2.3668\n",
      "Epoch [2485/5000] | D Loss: 0.7816 | G Loss: 2.3639\n",
      "Epoch [2486/5000] | D Loss: 0.6098 | G Loss: 2.4157\n",
      "Epoch [2487/5000] | D Loss: 0.5629 | G Loss: 3.2035\n",
      "Epoch [2488/5000] | D Loss: 0.6017 | G Loss: 3.3393\n",
      "Epoch [2489/5000] | D Loss: 0.5739 | G Loss: 3.1653\n",
      "Epoch [2490/5000] | D Loss: 0.5805 | G Loss: 2.5738\n",
      "Epoch [2491/5000] | D Loss: 0.6170 | G Loss: 2.3865\n",
      "Epoch [2492/5000] | D Loss: 0.6489 | G Loss: 2.7645\n",
      "Epoch [2493/5000] | D Loss: 0.6428 | G Loss: 2.9052\n",
      "Epoch [2494/5000] | D Loss: 0.6202 | G Loss: 3.0118\n",
      "Epoch [2495/5000] | D Loss: 0.6097 | G Loss: 4.0344\n",
      "Epoch [2496/5000] | D Loss: 0.6877 | G Loss: 3.0842\n",
      "Epoch [2497/5000] | D Loss: 0.7421 | G Loss: 3.6317\n",
      "Epoch [2498/5000] | D Loss: 0.5781 | G Loss: 3.4378\n",
      "Epoch [2499/5000] | D Loss: 0.5993 | G Loss: 3.5492\n",
      "Epoch [2500/5000] | D Loss: 0.6115 | G Loss: 3.2276\n",
      "Epoch 2500 FID Score: 178.3145\n",
      "Reducing learning rates to Generator: 1.25e-05, Discriminator: 1.25e-05\n",
      "Epoch [2501/5000] | D Loss: 0.5860 | G Loss: 2.5663\n",
      "Epoch [2502/5000] | D Loss: 0.6753 | G Loss: 2.2443\n",
      "Epoch [2503/5000] | D Loss: 0.5573 | G Loss: 1.6527\n",
      "Epoch [2504/5000] | D Loss: 0.5816 | G Loss: 2.9265\n",
      "Epoch [2505/5000] | D Loss: 0.6330 | G Loss: 2.6970\n",
      "Epoch [2506/5000] | D Loss: 0.5848 | G Loss: 2.4282\n",
      "Epoch [2507/5000] | D Loss: 0.6212 | G Loss: 3.3081\n",
      "Epoch [2508/5000] | D Loss: 0.5749 | G Loss: 2.4996\n",
      "Epoch [2509/5000] | D Loss: 0.6352 | G Loss: 2.9045\n",
      "Epoch [2510/5000] | D Loss: 0.5741 | G Loss: 3.0967\n",
      "Epoch [2511/5000] | D Loss: 0.5717 | G Loss: 2.7831\n",
      "Epoch [2512/5000] | D Loss: 0.6076 | G Loss: 2.3556\n",
      "Epoch [2513/5000] | D Loss: 0.5757 | G Loss: 2.4026\n",
      "Epoch [2514/5000] | D Loss: 0.7115 | G Loss: 2.2255\n",
      "Epoch [2515/5000] | D Loss: 0.6963 | G Loss: 2.2751\n",
      "Epoch [2516/5000] | D Loss: 0.9141 | G Loss: 2.3921\n",
      "Epoch [2517/5000] | D Loss: 0.5911 | G Loss: 3.3755\n",
      "Epoch [2518/5000] | D Loss: 0.6289 | G Loss: 2.5607\n",
      "Epoch [2519/5000] | D Loss: 0.5858 | G Loss: 2.8377\n",
      "Epoch [2520/5000] | D Loss: 0.6252 | G Loss: 1.7638\n",
      "Epoch [2521/5000] | D Loss: 0.5646 | G Loss: 2.9822\n",
      "Epoch [2522/5000] | D Loss: 0.6687 | G Loss: 2.2496\n",
      "Epoch [2523/5000] | D Loss: 0.5858 | G Loss: 2.0646\n",
      "Epoch [2524/5000] | D Loss: 0.5742 | G Loss: 3.8331\n",
      "Epoch [2525/5000] | D Loss: 0.7293 | G Loss: 2.7730\n",
      "Epoch [2526/5000] | D Loss: 0.9200 | G Loss: 1.5487\n",
      "Epoch [2527/5000] | D Loss: 0.5837 | G Loss: 1.9557\n",
      "Epoch [2528/5000] | D Loss: 0.5577 | G Loss: 1.6911\n",
      "Epoch [2529/5000] | D Loss: 0.5702 | G Loss: 4.0228\n",
      "Epoch [2530/5000] | D Loss: 0.6365 | G Loss: 3.3859\n",
      "Epoch [2531/5000] | D Loss: 0.5784 | G Loss: 3.7822\n",
      "Epoch [2532/5000] | D Loss: 0.6225 | G Loss: 2.4162\n",
      "Epoch [2533/5000] | D Loss: 0.6089 | G Loss: 2.5478\n",
      "Epoch [2534/5000] | D Loss: 0.6061 | G Loss: 3.1341\n",
      "Epoch [2535/5000] | D Loss: 0.6750 | G Loss: 2.3066\n",
      "Epoch [2536/5000] | D Loss: 0.5818 | G Loss: 2.5135\n",
      "Epoch [2537/5000] | D Loss: 0.7781 | G Loss: 1.9402\n",
      "Epoch [2538/5000] | D Loss: 0.5954 | G Loss: 3.7916\n",
      "Epoch [2539/5000] | D Loss: 1.1084 | G Loss: 1.9395\n",
      "Epoch [2540/5000] | D Loss: 0.6763 | G Loss: 2.1511\n",
      "Epoch [2541/5000] | D Loss: 0.6114 | G Loss: 2.8087\n",
      "Epoch [2542/5000] | D Loss: 0.6857 | G Loss: 2.6080\n",
      "Epoch [2543/5000] | D Loss: 0.5649 | G Loss: 3.4918\n",
      "Epoch [2544/5000] | D Loss: 0.6082 | G Loss: 2.7002\n",
      "Epoch [2545/5000] | D Loss: 0.6296 | G Loss: 3.4873\n",
      "Epoch [2546/5000] | D Loss: 0.6193 | G Loss: 2.3987\n",
      "Epoch [2547/5000] | D Loss: 0.5792 | G Loss: 2.4994\n",
      "Epoch [2548/5000] | D Loss: 0.6297 | G Loss: 3.2485\n",
      "Epoch [2549/5000] | D Loss: 0.6093 | G Loss: 2.8871\n",
      "Epoch [2550/5000] | D Loss: 0.5700 | G Loss: 3.8602\n",
      "Epoch [2551/5000] | D Loss: 0.5814 | G Loss: 4.1811\n",
      "Epoch [2552/5000] | D Loss: 0.6108 | G Loss: 2.6179\n",
      "Epoch [2553/5000] | D Loss: 0.5759 | G Loss: 2.6493\n",
      "Epoch [2554/5000] | D Loss: 0.6355 | G Loss: 2.7234\n",
      "Epoch [2555/5000] | D Loss: 0.5642 | G Loss: 3.0235\n",
      "Epoch [2556/5000] | D Loss: 1.0078 | G Loss: 1.3279\n",
      "Epoch [2557/5000] | D Loss: 0.5764 | G Loss: 2.9342\n",
      "Epoch [2558/5000] | D Loss: 0.6472 | G Loss: 2.7205\n",
      "Epoch [2559/5000] | D Loss: 0.5819 | G Loss: 3.7884\n",
      "Epoch [2560/5000] | D Loss: 0.6123 | G Loss: 3.6274\n",
      "Epoch [2561/5000] | D Loss: 0.5800 | G Loss: 2.9758\n",
      "Epoch [2562/5000] | D Loss: 0.5967 | G Loss: 3.7034\n",
      "Epoch [2563/5000] | D Loss: 0.6028 | G Loss: 2.6345\n",
      "Epoch [2564/5000] | D Loss: 0.6775 | G Loss: 3.2302\n",
      "Epoch [2565/5000] | D Loss: 0.5870 | G Loss: 2.5811\n",
      "Epoch [2566/5000] | D Loss: 0.5698 | G Loss: 3.0969\n",
      "Epoch [2567/5000] | D Loss: 0.5879 | G Loss: 2.0466\n",
      "Epoch [2568/5000] | D Loss: 0.5740 | G Loss: 2.4257\n",
      "Epoch [2569/5000] | D Loss: 0.5732 | G Loss: 3.0397\n",
      "Epoch [2570/5000] | D Loss: 0.5739 | G Loss: 2.9735\n",
      "Epoch [2571/5000] | D Loss: 0.5741 | G Loss: 3.3276\n",
      "Epoch [2572/5000] | D Loss: 0.5824 | G Loss: 3.4532\n",
      "Epoch [2573/5000] | D Loss: 0.6754 | G Loss: 1.8138\n",
      "Epoch [2574/5000] | D Loss: 0.5795 | G Loss: 3.2501\n",
      "Epoch [2575/5000] | D Loss: 0.6104 | G Loss: 2.4237\n",
      "Epoch [2576/5000] | D Loss: 0.8155 | G Loss: 1.7309\n",
      "Epoch [2577/5000] | D Loss: 0.5808 | G Loss: 2.8268\n",
      "Epoch [2578/5000] | D Loss: 0.5778 | G Loss: 3.2508\n",
      "Epoch [2579/5000] | D Loss: 0.6765 | G Loss: 2.7199\n",
      "Epoch [2580/5000] | D Loss: 0.5860 | G Loss: 2.3683\n",
      "Epoch [2581/5000] | D Loss: 0.6066 | G Loss: 2.7659\n",
      "Epoch [2582/5000] | D Loss: 0.6384 | G Loss: 2.5988\n",
      "Epoch [2583/5000] | D Loss: 0.6205 | G Loss: 3.1817\n",
      "Epoch [2584/5000] | D Loss: 0.6089 | G Loss: 2.8783\n",
      "Epoch [2585/5000] | D Loss: 0.5917 | G Loss: 2.2325\n",
      "Epoch [2586/5000] | D Loss: 0.6389 | G Loss: 2.4272\n",
      "Epoch [2587/5000] | D Loss: 0.6177 | G Loss: 2.7449\n",
      "Epoch [2588/5000] | D Loss: 0.5514 | G Loss: 2.6534\n",
      "Epoch [2589/5000] | D Loss: 0.5626 | G Loss: 3.2426\n",
      "Epoch [2590/5000] | D Loss: 0.7404 | G Loss: 2.1197\n",
      "Epoch [2591/5000] | D Loss: 0.6269 | G Loss: 1.7482\n",
      "Epoch [2592/5000] | D Loss: 0.5832 | G Loss: 3.5917\n",
      "Epoch [2593/5000] | D Loss: 0.6012 | G Loss: 3.6198\n",
      "Epoch [2594/5000] | D Loss: 0.5702 | G Loss: 2.5985\n",
      "Epoch [2595/5000] | D Loss: 0.5833 | G Loss: 3.6592\n",
      "Epoch [2596/5000] | D Loss: 0.5675 | G Loss: 3.0026\n",
      "Epoch [2597/5000] | D Loss: 0.5963 | G Loss: 3.7757\n",
      "Epoch [2598/5000] | D Loss: 0.7342 | G Loss: 2.3882\n",
      "Epoch [2599/5000] | D Loss: 0.5858 | G Loss: 3.0105\n",
      "Epoch [2600/5000] | D Loss: 0.5537 | G Loss: 2.9530\n",
      "Epoch 2600 FID Score: 191.5618\n",
      "Reducing learning rates to Generator: 6.25e-06, Discriminator: 6.25e-06\n",
      "Epoch [2601/5000] | D Loss: 0.5598 | G Loss: 3.4807\n",
      "Epoch [2602/5000] | D Loss: 0.6902 | G Loss: 2.8368\n",
      "Epoch [2603/5000] | D Loss: 0.6077 | G Loss: 2.5382\n",
      "Epoch [2604/5000] | D Loss: 0.5808 | G Loss: 2.5225\n",
      "Epoch [2605/5000] | D Loss: 0.6002 | G Loss: 2.4905\n",
      "Epoch [2606/5000] | D Loss: 0.6122 | G Loss: 3.1048\n",
      "Epoch [2607/5000] | D Loss: 0.5947 | G Loss: 3.7925\n",
      "Epoch [2608/5000] | D Loss: 0.6323 | G Loss: 2.4659\n",
      "Epoch [2609/5000] | D Loss: 0.5648 | G Loss: 2.4494\n",
      "Epoch [2610/5000] | D Loss: 0.5744 | G Loss: 3.5082\n",
      "Epoch [2611/5000] | D Loss: 0.6493 | G Loss: 1.4551\n",
      "Epoch [2612/5000] | D Loss: 0.6077 | G Loss: 2.4030\n",
      "Epoch [2613/5000] | D Loss: 0.5759 | G Loss: 3.4527\n",
      "Epoch [2614/5000] | D Loss: 0.6119 | G Loss: 2.7464\n",
      "Epoch [2615/5000] | D Loss: 0.5646 | G Loss: 2.3723\n",
      "Epoch [2616/5000] | D Loss: 0.5989 | G Loss: 2.9291\n",
      "Epoch [2617/5000] | D Loss: 0.5591 | G Loss: 3.2529\n",
      "Epoch [2618/5000] | D Loss: 0.6012 | G Loss: 3.8261\n",
      "Epoch [2619/5000] | D Loss: 0.6271 | G Loss: 2.1334\n",
      "Epoch [2620/5000] | D Loss: 0.5808 | G Loss: 2.9388\n",
      "Epoch [2621/5000] | D Loss: 0.5832 | G Loss: 3.1475\n",
      "Epoch [2622/5000] | D Loss: 0.5566 | G Loss: 2.8037\n",
      "Epoch [2623/5000] | D Loss: 0.5633 | G Loss: 3.0392\n",
      "Epoch [2624/5000] | D Loss: 0.5858 | G Loss: 2.2001\n",
      "Epoch [2625/5000] | D Loss: 0.5583 | G Loss: 3.5570\n",
      "Epoch [2626/5000] | D Loss: 0.5892 | G Loss: 2.7419\n",
      "Epoch [2627/5000] | D Loss: 0.6693 | G Loss: 2.0521\n",
      "Epoch [2628/5000] | D Loss: 0.5935 | G Loss: 2.4764\n",
      "Epoch [2629/5000] | D Loss: 0.6524 | G Loss: 2.9027\n",
      "Epoch [2630/5000] | D Loss: 0.5787 | G Loss: 3.2157\n",
      "Epoch [2631/5000] | D Loss: 0.5465 | G Loss: 2.8567\n",
      "Epoch [2632/5000] | D Loss: 0.5482 | G Loss: 2.3886\n",
      "Epoch [2633/5000] | D Loss: 0.6976 | G Loss: 3.4320\n",
      "Epoch [2634/5000] | D Loss: 0.5607 | G Loss: 4.4567\n",
      "Epoch [2635/5000] | D Loss: 0.5741 | G Loss: 2.5644\n",
      "Epoch [2636/5000] | D Loss: 0.6299 | G Loss: 2.4609\n",
      "Epoch [2637/5000] | D Loss: 0.6186 | G Loss: 2.6243\n",
      "Epoch [2638/5000] | D Loss: 0.6571 | G Loss: 3.0861\n",
      "Epoch [2639/5000] | D Loss: 0.5769 | G Loss: 3.2530\n",
      "Epoch [2640/5000] | D Loss: 0.5781 | G Loss: 3.0526\n",
      "Epoch [2641/5000] | D Loss: 0.5600 | G Loss: 2.4163\n",
      "Epoch [2642/5000] | D Loss: 0.5640 | G Loss: 3.3565\n",
      "Epoch [2643/5000] | D Loss: 0.5962 | G Loss: 3.1521\n",
      "Epoch [2644/5000] | D Loss: 0.5833 | G Loss: 3.0157\n",
      "Epoch [2645/5000] | D Loss: 0.5555 | G Loss: 3.1355\n",
      "Epoch [2646/5000] | D Loss: 1.0491 | G Loss: 2.9020\n",
      "Epoch [2647/5000] | D Loss: 0.5660 | G Loss: 3.2331\n",
      "Epoch [2648/5000] | D Loss: 0.5623 | G Loss: 2.9849\n",
      "Epoch [2649/5000] | D Loss: 0.7054 | G Loss: 2.6542\n",
      "Epoch [2650/5000] | D Loss: 0.5754 | G Loss: 3.3368\n",
      "Epoch [2651/5000] | D Loss: 0.6512 | G Loss: 2.6256\n",
      "Epoch [2652/5000] | D Loss: 0.5666 | G Loss: 3.4072\n",
      "Epoch [2653/5000] | D Loss: 0.6027 | G Loss: 2.9824\n",
      "Epoch [2654/5000] | D Loss: 0.5891 | G Loss: 4.1272\n",
      "Epoch [2655/5000] | D Loss: 0.5684 | G Loss: 3.5837\n",
      "Epoch [2656/5000] | D Loss: 0.7787 | G Loss: 2.3576\n",
      "Epoch [2657/5000] | D Loss: 0.5661 | G Loss: 3.0491\n",
      "Epoch [2658/5000] | D Loss: 0.5730 | G Loss: 3.5489\n",
      "Epoch [2659/5000] | D Loss: 0.5613 | G Loss: 3.3343\n",
      "Epoch [2660/5000] | D Loss: 0.5791 | G Loss: 2.8470\n",
      "Epoch [2661/5000] | D Loss: 0.5643 | G Loss: 3.0461\n",
      "Epoch [2662/5000] | D Loss: 0.5580 | G Loss: 2.7419\n",
      "Epoch [2663/5000] | D Loss: 0.6014 | G Loss: 2.3757\n",
      "Epoch [2664/5000] | D Loss: 0.7114 | G Loss: 2.7048\n",
      "Epoch [2665/5000] | D Loss: 0.5767 | G Loss: 2.8033\n",
      "Epoch [2666/5000] | D Loss: 0.6168 | G Loss: 2.8887\n",
      "Epoch [2667/5000] | D Loss: 0.9973 | G Loss: 3.8837\n",
      "Epoch [2668/5000] | D Loss: 0.6538 | G Loss: 2.1703\n",
      "Epoch [2669/5000] | D Loss: 0.5825 | G Loss: 3.8288\n",
      "Epoch [2670/5000] | D Loss: 0.6669 | G Loss: 3.0446\n",
      "Epoch [2671/5000] | D Loss: 0.5747 | G Loss: 3.0446\n",
      "Epoch [2672/5000] | D Loss: 0.8252 | G Loss: 2.9246\n",
      "Epoch [2673/5000] | D Loss: 0.5740 | G Loss: 4.0483\n",
      "Epoch [2674/5000] | D Loss: 0.6126 | G Loss: 2.6889\n",
      "Epoch [2675/5000] | D Loss: 0.6408 | G Loss: 3.4179\n",
      "Epoch [2676/5000] | D Loss: 0.6004 | G Loss: 1.9443\n",
      "Epoch [2677/5000] | D Loss: 0.5650 | G Loss: 3.2986\n",
      "Epoch [2678/5000] | D Loss: 0.5774 | G Loss: 3.4168\n",
      "Epoch [2679/5000] | D Loss: 0.5634 | G Loss: 2.8362\n",
      "Epoch [2680/5000] | D Loss: 0.5537 | G Loss: 2.8652\n",
      "Epoch [2681/5000] | D Loss: 0.5781 | G Loss: 2.7264\n",
      "Epoch [2682/5000] | D Loss: 0.5957 | G Loss: 3.1283\n",
      "Epoch [2683/5000] | D Loss: 0.7842 | G Loss: 2.4362\n",
      "Epoch [2684/5000] | D Loss: 0.5748 | G Loss: 3.3669\n",
      "Epoch [2685/5000] | D Loss: 0.6047 | G Loss: 2.2673\n",
      "Epoch [2686/5000] | D Loss: 0.6003 | G Loss: 2.1645\n",
      "Epoch [2687/5000] | D Loss: 0.6672 | G Loss: 2.8700\n",
      "Epoch [2688/5000] | D Loss: 0.5646 | G Loss: 2.8919\n",
      "Epoch [2689/5000] | D Loss: 0.5557 | G Loss: 3.0830\n",
      "Epoch [2690/5000] | D Loss: 0.6058 | G Loss: 1.5208\n",
      "Epoch [2691/5000] | D Loss: 0.6283 | G Loss: 2.9097\n",
      "Epoch [2692/5000] | D Loss: 0.5647 | G Loss: 2.5292\n",
      "Epoch [2693/5000] | D Loss: 0.5837 | G Loss: 2.3295\n",
      "Epoch [2694/5000] | D Loss: 0.5980 | G Loss: 3.9119\n",
      "Epoch [2695/5000] | D Loss: 0.5856 | G Loss: 3.4616\n",
      "Epoch [2696/5000] | D Loss: 0.5785 | G Loss: 2.8207\n",
      "Epoch [2697/5000] | D Loss: 0.5759 | G Loss: 2.5897\n",
      "Epoch [2698/5000] | D Loss: 0.6009 | G Loss: 2.1406\n",
      "Epoch [2699/5000] | D Loss: 0.5691 | G Loss: 2.6969\n",
      "Epoch [2700/5000] | D Loss: 0.5959 | G Loss: 4.3070\n",
      "Epoch 2700 FID Score: 182.4427\n",
      "Reducing learning rates to Generator: 3.125e-06, Discriminator: 3.125e-06\n",
      "Epoch [2701/5000] | D Loss: 0.6291 | G Loss: 2.2488\n",
      "Epoch [2702/5000] | D Loss: 0.6016 | G Loss: 3.1401\n",
      "Epoch [2703/5000] | D Loss: 0.5665 | G Loss: 3.7896\n",
      "Epoch [2704/5000] | D Loss: 0.5754 | G Loss: 3.4852\n",
      "Epoch [2705/5000] | D Loss: 0.5619 | G Loss: 3.2120\n",
      "Epoch [2706/5000] | D Loss: 0.5939 | G Loss: 2.4533\n",
      "Epoch [2707/5000] | D Loss: 0.5479 | G Loss: 3.3577\n",
      "Epoch [2708/5000] | D Loss: 0.5666 | G Loss: 3.2679\n",
      "Epoch [2709/5000] | D Loss: 0.7189 | G Loss: 2.4968\n",
      "Epoch [2710/5000] | D Loss: 0.5755 | G Loss: 3.1493\n",
      "Epoch [2711/5000] | D Loss: 0.5969 | G Loss: 3.0680\n",
      "Epoch [2712/5000] | D Loss: 0.5669 | G Loss: 2.6463\n",
      "Epoch [2713/5000] | D Loss: 0.5857 | G Loss: 3.8485\n",
      "Epoch [2714/5000] | D Loss: 0.5751 | G Loss: 2.9761\n",
      "Epoch [2715/5000] | D Loss: 0.6396 | G Loss: 2.1666\n",
      "Epoch [2716/5000] | D Loss: 0.5659 | G Loss: 2.9648\n",
      "Epoch [2717/5000] | D Loss: 0.6261 | G Loss: 3.1375\n",
      "Epoch [2718/5000] | D Loss: 0.5865 | G Loss: 3.4042\n",
      "Epoch [2719/5000] | D Loss: 0.5630 | G Loss: 3.3125\n",
      "Epoch [2720/5000] | D Loss: 0.5636 | G Loss: 2.8393\n",
      "Epoch [2721/5000] | D Loss: 0.5683 | G Loss: 2.4648\n",
      "Epoch [2722/5000] | D Loss: 0.5701 | G Loss: 2.5933\n",
      "Epoch [2723/5000] | D Loss: 0.5944 | G Loss: 2.9215\n",
      "Epoch [2724/5000] | D Loss: 0.5665 | G Loss: 2.8160\n",
      "Epoch [2725/5000] | D Loss: 0.5880 | G Loss: 1.5220\n",
      "Epoch [2726/5000] | D Loss: 0.5650 | G Loss: 3.2823\n",
      "Epoch [2727/5000] | D Loss: 0.5727 | G Loss: 2.8137\n",
      "Epoch [2728/5000] | D Loss: 0.5562 | G Loss: 3.4062\n",
      "Epoch [2729/5000] | D Loss: 0.5799 | G Loss: 3.9425\n",
      "Epoch [2730/5000] | D Loss: 0.6337 | G Loss: 2.1406\n",
      "Epoch [2731/5000] | D Loss: 0.5797 | G Loss: 2.7275\n",
      "Epoch [2732/5000] | D Loss: 0.5975 | G Loss: 2.8117\n",
      "Epoch [2733/5000] | D Loss: 0.5861 | G Loss: 2.9300\n",
      "Epoch [2734/5000] | D Loss: 0.5962 | G Loss: 2.1707\n",
      "Epoch [2735/5000] | D Loss: 0.5665 | G Loss: 2.6401\n",
      "Epoch [2736/5000] | D Loss: 0.5717 | G Loss: 3.4389\n",
      "Epoch [2737/5000] | D Loss: 0.5665 | G Loss: 3.8828\n",
      "Epoch [2738/5000] | D Loss: 0.6580 | G Loss: 2.7637\n",
      "Epoch [2739/5000] | D Loss: 0.5802 | G Loss: 3.3120\n",
      "Epoch [2740/5000] | D Loss: 0.5645 | G Loss: 4.0415\n",
      "Epoch [2741/5000] | D Loss: 0.5750 | G Loss: 4.3255\n",
      "Epoch [2742/5000] | D Loss: 0.5740 | G Loss: 2.3662\n",
      "Epoch [2743/5000] | D Loss: 0.5655 | G Loss: 3.3505\n",
      "Epoch [2744/5000] | D Loss: 0.5664 | G Loss: 3.4306\n",
      "Epoch [2745/5000] | D Loss: 0.5534 | G Loss: 2.7788\n",
      "Epoch [2746/5000] | D Loss: 0.5808 | G Loss: 3.9048\n",
      "Epoch [2747/5000] | D Loss: 0.5743 | G Loss: 2.5365\n",
      "Epoch [2748/5000] | D Loss: 0.6294 | G Loss: 2.6005\n",
      "Epoch [2749/5000] | D Loss: 0.5673 | G Loss: 2.5115\n",
      "Epoch [2750/5000] | D Loss: 0.5765 | G Loss: 2.7517\n",
      "Epoch [2751/5000] | D Loss: 0.5722 | G Loss: 4.3119\n",
      "Epoch [2752/5000] | D Loss: 0.6867 | G Loss: 2.9111\n",
      "Epoch [2753/5000] | D Loss: 0.5799 | G Loss: 3.5529\n",
      "Epoch [2754/5000] | D Loss: 0.6640 | G Loss: 2.7521\n",
      "Epoch [2755/5000] | D Loss: 0.6187 | G Loss: 3.2092\n",
      "Epoch [2756/5000] | D Loss: 0.5615 | G Loss: 3.0235\n",
      "Epoch [2757/5000] | D Loss: 0.6194 | G Loss: 2.3063\n",
      "Epoch [2758/5000] | D Loss: 0.5639 | G Loss: 3.5612\n",
      "Epoch [2759/5000] | D Loss: 0.5871 | G Loss: 3.0736\n",
      "Epoch [2760/5000] | D Loss: 0.5763 | G Loss: 2.8804\n",
      "Epoch [2761/5000] | D Loss: 0.5614 | G Loss: 3.2490\n",
      "Epoch [2762/5000] | D Loss: 0.6238 | G Loss: 3.0609\n",
      "Epoch [2763/5000] | D Loss: 0.5711 | G Loss: 2.5342\n",
      "Epoch [2764/5000] | D Loss: 0.5681 | G Loss: 3.2524\n",
      "Epoch [2765/5000] | D Loss: 0.5847 | G Loss: 3.1852\n",
      "Epoch [2766/5000] | D Loss: 0.5831 | G Loss: 3.7714\n",
      "Epoch [2767/5000] | D Loss: 0.5629 | G Loss: 3.6442\n",
      "Epoch [2768/5000] | D Loss: 0.5541 | G Loss: 3.0532\n",
      "Epoch [2769/5000] | D Loss: 0.6113 | G Loss: 2.7545\n",
      "Epoch [2770/5000] | D Loss: 0.5697 | G Loss: 2.5091\n",
      "Epoch [2771/5000] | D Loss: 0.6136 | G Loss: 2.4251\n",
      "Epoch [2772/5000] | D Loss: 0.9058 | G Loss: 4.3388\n",
      "Epoch [2773/5000] | D Loss: 0.5770 | G Loss: 2.7090\n",
      "Epoch [2774/5000] | D Loss: 0.6335 | G Loss: 2.8954\n",
      "Epoch [2775/5000] | D Loss: 0.6928 | G Loss: 3.1590\n",
      "Epoch [2776/5000] | D Loss: 0.6490 | G Loss: 3.3738\n",
      "Epoch [2777/5000] | D Loss: 0.6155 | G Loss: 2.9077\n",
      "Epoch [2778/5000] | D Loss: 0.5796 | G Loss: 2.6723\n",
      "Epoch [2779/5000] | D Loss: 0.5624 | G Loss: 3.1223\n",
      "Epoch [2780/5000] | D Loss: 0.6692 | G Loss: 2.6601\n",
      "Epoch [2781/5000] | D Loss: 0.6745 | G Loss: 1.9632\n",
      "Epoch [2782/5000] | D Loss: 0.5636 | G Loss: 2.7739\n",
      "Epoch [2783/5000] | D Loss: 0.6208 | G Loss: 3.5429\n",
      "Epoch [2784/5000] | D Loss: 0.5747 | G Loss: 2.8708\n",
      "Epoch [2785/5000] | D Loss: 0.6102 | G Loss: 2.5393\n",
      "Epoch [2786/5000] | D Loss: 0.5978 | G Loss: 2.5135\n",
      "Epoch [2787/5000] | D Loss: 0.6315 | G Loss: 1.8565\n",
      "Epoch [2788/5000] | D Loss: 0.6024 | G Loss: 3.2915\n",
      "Epoch [2789/5000] | D Loss: 0.5685 | G Loss: 2.9717\n",
      "Epoch [2790/5000] | D Loss: 0.6791 | G Loss: 2.8624\n",
      "Epoch [2791/5000] | D Loss: 0.7766 | G Loss: 2.2144\n",
      "Epoch [2792/5000] | D Loss: 0.5512 | G Loss: 3.1797\n",
      "Epoch [2793/5000] | D Loss: 0.5596 | G Loss: 2.3820\n",
      "Epoch [2794/5000] | D Loss: 0.5735 | G Loss: 2.9159\n",
      "Epoch [2795/5000] | D Loss: 0.5600 | G Loss: 2.1281\n",
      "Epoch [2796/5000] | D Loss: 0.5778 | G Loss: 3.6451\n",
      "Epoch [2797/5000] | D Loss: 0.6123 | G Loss: 3.7189\n",
      "Epoch [2798/5000] | D Loss: 0.5514 | G Loss: 2.9622\n",
      "Epoch [2799/5000] | D Loss: 0.5599 | G Loss: 4.3280\n",
      "Epoch [2800/5000] | D Loss: 0.6033 | G Loss: 2.3304\n",
      "Epoch 2800 FID Score: 183.5500\n",
      "Reducing learning rates to Generator: 1.5625e-06, Discriminator: 1.5625e-06\n",
      "Epoch [2801/5000] | D Loss: 0.6163 | G Loss: 3.0155\n",
      "Epoch [2802/5000] | D Loss: 1.3324 | G Loss: 3.0375\n",
      "Epoch [2803/5000] | D Loss: 0.5828 | G Loss: 2.4998\n",
      "Epoch [2804/5000] | D Loss: 0.5830 | G Loss: 2.6177\n",
      "Epoch [2805/5000] | D Loss: 0.7835 | G Loss: 2.6239\n",
      "Epoch [2806/5000] | D Loss: 0.5738 | G Loss: 2.5177\n",
      "Epoch [2807/5000] | D Loss: 0.5656 | G Loss: 2.3244\n",
      "Epoch [2808/5000] | D Loss: 0.5614 | G Loss: 2.0941\n",
      "Epoch [2809/5000] | D Loss: 0.5610 | G Loss: 2.9759\n",
      "Epoch [2810/5000] | D Loss: 0.6206 | G Loss: 2.7162\n",
      "Epoch [2811/5000] | D Loss: 0.6436 | G Loss: 3.5404\n",
      "Epoch [2812/5000] | D Loss: 0.5673 | G Loss: 3.2717\n",
      "Epoch [2813/5000] | D Loss: 0.6935 | G Loss: 2.5558\n",
      "Epoch [2814/5000] | D Loss: 0.5543 | G Loss: 3.0831\n",
      "Epoch [2815/5000] | D Loss: 0.6858 | G Loss: 2.9201\n",
      "Epoch [2816/5000] | D Loss: 0.7935 | G Loss: 2.8983\n",
      "Epoch [2817/5000] | D Loss: 0.5642 | G Loss: 2.2280\n",
      "Epoch [2818/5000] | D Loss: 0.6458 | G Loss: 2.5957\n",
      "Epoch [2819/5000] | D Loss: 0.6262 | G Loss: 3.3617\n",
      "Epoch [2820/5000] | D Loss: 0.5810 | G Loss: 2.8527\n",
      "Epoch [2821/5000] | D Loss: 0.5781 | G Loss: 3.3598\n",
      "Epoch [2822/5000] | D Loss: 0.5530 | G Loss: 3.6183\n",
      "Epoch [2823/5000] | D Loss: 0.7530 | G Loss: 2.8754\n",
      "Epoch [2824/5000] | D Loss: 0.5994 | G Loss: 3.0114\n",
      "Epoch [2825/5000] | D Loss: 0.6258 | G Loss: 3.9739\n",
      "Epoch [2826/5000] | D Loss: 0.6208 | G Loss: 3.0763\n",
      "Epoch [2827/5000] | D Loss: 0.5723 | G Loss: 2.4864\n",
      "Epoch [2828/5000] | D Loss: 0.6259 | G Loss: 2.1491\n",
      "Epoch [2829/5000] | D Loss: 0.6129 | G Loss: 2.5100\n",
      "Epoch [2830/5000] | D Loss: 0.5472 | G Loss: 3.4754\n",
      "Epoch [2831/5000] | D Loss: 0.5501 | G Loss: 2.5843\n",
      "Epoch [2832/5000] | D Loss: 0.6084 | G Loss: 3.2548\n",
      "Epoch [2833/5000] | D Loss: 0.6727 | G Loss: 2.3801\n",
      "Epoch [2834/5000] | D Loss: 0.7438 | G Loss: 2.3085\n",
      "Epoch [2835/5000] | D Loss: 0.5703 | G Loss: 2.7389\n",
      "Epoch [2836/5000] | D Loss: 0.5696 | G Loss: 2.8953\n",
      "Epoch [2837/5000] | D Loss: 0.6012 | G Loss: 2.5016\n",
      "Epoch [2838/5000] | D Loss: 0.5720 | G Loss: 3.1537\n",
      "Epoch [2839/5000] | D Loss: 0.5954 | G Loss: 2.4506\n",
      "Epoch [2840/5000] | D Loss: 0.5792 | G Loss: 3.6265\n",
      "Epoch [2841/5000] | D Loss: 0.5680 | G Loss: 2.8073\n",
      "Epoch [2842/5000] | D Loss: 0.5858 | G Loss: 3.7234\n",
      "Epoch [2843/5000] | D Loss: 0.5451 | G Loss: 3.4178\n",
      "Epoch [2844/5000] | D Loss: 0.5740 | G Loss: 3.1663\n",
      "Epoch [2845/5000] | D Loss: 0.5769 | G Loss: 2.9674\n",
      "Epoch [2846/5000] | D Loss: 0.5624 | G Loss: 2.9515\n",
      "Epoch [2847/5000] | D Loss: 0.6005 | G Loss: 2.2882\n",
      "Epoch [2848/5000] | D Loss: 0.5843 | G Loss: 2.7509\n",
      "Epoch [2849/5000] | D Loss: 0.5830 | G Loss: 2.7407\n",
      "Epoch [2850/5000] | D Loss: 0.5802 | G Loss: 2.8400\n",
      "Epoch [2851/5000] | D Loss: 0.5758 | G Loss: 2.2724\n",
      "Epoch [2852/5000] | D Loss: 0.6303 | G Loss: 2.5430\n",
      "Epoch [2853/5000] | D Loss: 0.6094 | G Loss: 2.9675\n",
      "Epoch [2854/5000] | D Loss: 0.5886 | G Loss: 3.0201\n",
      "Epoch [2855/5000] | D Loss: 0.5726 | G Loss: 2.5241\n",
      "Epoch [2856/5000] | D Loss: 0.5395 | G Loss: 2.8091\n",
      "Epoch [2857/5000] | D Loss: 0.5776 | G Loss: 3.2936\n",
      "Epoch [2858/5000] | D Loss: 0.5753 | G Loss: 3.9816\n",
      "Epoch [2859/5000] | D Loss: 0.5857 | G Loss: 3.7250\n",
      "Epoch [2860/5000] | D Loss: 0.5537 | G Loss: 2.5748\n",
      "Epoch [2861/5000] | D Loss: 0.5674 | G Loss: 3.3470\n",
      "Epoch [2862/5000] | D Loss: 0.5786 | G Loss: 3.7072\n",
      "Epoch [2863/5000] | D Loss: 0.5977 | G Loss: 4.2191\n",
      "Epoch [2864/5000] | D Loss: 0.5930 | G Loss: 2.7643\n",
      "Epoch [2865/5000] | D Loss: 0.6002 | G Loss: 2.8426\n",
      "Epoch [2866/5000] | D Loss: 0.5802 | G Loss: 3.5268\n",
      "Epoch [2867/5000] | D Loss: 0.6059 | G Loss: 3.5088\n",
      "Epoch [2868/5000] | D Loss: 0.6609 | G Loss: 1.8785\n",
      "Epoch [2869/5000] | D Loss: 0.6042 | G Loss: 2.6330\n",
      "Epoch [2870/5000] | D Loss: 0.6098 | G Loss: 2.2926\n",
      "Epoch [2871/5000] | D Loss: 1.1647 | G Loss: 3.0247\n",
      "Epoch [2872/5000] | D Loss: 0.6207 | G Loss: 2.9881\n",
      "Epoch [2873/5000] | D Loss: 0.5859 | G Loss: 2.6632\n",
      "Epoch [2874/5000] | D Loss: 0.6112 | G Loss: 3.2085\n",
      "Epoch [2875/5000] | D Loss: 0.5837 | G Loss: 2.9058\n",
      "Epoch [2876/5000] | D Loss: 0.6182 | G Loss: 3.1385\n",
      "Epoch [2877/5000] | D Loss: 0.5748 | G Loss: 3.4286\n",
      "Epoch [2878/5000] | D Loss: 0.5595 | G Loss: 3.2766\n",
      "Epoch [2879/5000] | D Loss: 0.5548 | G Loss: 2.9217\n",
      "Epoch [2880/5000] | D Loss: 0.6195 | G Loss: 2.5869\n",
      "Epoch [2881/5000] | D Loss: 0.5926 | G Loss: 2.7917\n",
      "Epoch [2882/5000] | D Loss: 0.6382 | G Loss: 2.7300\n",
      "Epoch [2883/5000] | D Loss: 0.6612 | G Loss: 2.4605\n",
      "Epoch [2884/5000] | D Loss: 0.5383 | G Loss: 3.2670\n",
      "Epoch [2885/5000] | D Loss: 0.6468 | G Loss: 2.3502\n",
      "Epoch [2886/5000] | D Loss: 0.7055 | G Loss: 3.3632\n",
      "Epoch [2887/5000] | D Loss: 0.5677 | G Loss: 3.2857\n",
      "Epoch [2888/5000] | D Loss: 0.5758 | G Loss: 3.4691\n",
      "Epoch [2889/5000] | D Loss: 0.5720 | G Loss: 2.2129\n",
      "Epoch [2890/5000] | D Loss: 0.5752 | G Loss: 3.2230\n",
      "Epoch [2891/5000] | D Loss: 0.5719 | G Loss: 3.3448\n",
      "Epoch [2892/5000] | D Loss: 0.5753 | G Loss: 2.2239\n",
      "Epoch [2893/5000] | D Loss: 0.7468 | G Loss: 2.9521\n",
      "Epoch [2894/5000] | D Loss: 0.7870 | G Loss: 2.3738\n",
      "Epoch [2895/5000] | D Loss: 0.6326 | G Loss: 2.5911\n",
      "Epoch [2896/5000] | D Loss: 0.5771 | G Loss: 3.8688\n",
      "Epoch [2897/5000] | D Loss: 0.5813 | G Loss: 2.2357\n",
      "Epoch [2898/5000] | D Loss: 0.5583 | G Loss: 2.5536\n",
      "Epoch [2899/5000] | D Loss: 0.5535 | G Loss: 2.4872\n",
      "Epoch [2900/5000] | D Loss: 0.6382 | G Loss: 2.0549\n",
      "Epoch 2900 FID Score: 185.2926\n",
      "Reducing learning rates to Generator: 1e-06, Discriminator: 1e-06\n",
      "No improvement in FID for 5 intervals. Stopping early at Epoch 2900.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import models, transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.models import Inception_V3_Weights\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "\n",
    "# Device Configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Using NVIDIA GPU (CUDA)')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print('Using Mac GPU (MPS)')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using CPU')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_parquet('../data/processed_sticker_dataset.parquet')\n",
    "df[\"combined_embedding\"] = df[\"combined_embedding\"].apply(lambda x: np.array(x, dtype=np.float32))\n",
    "\n",
    "# Custom Dataset Class\n",
    "class StickerDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        embedding = torch.tensor(self.dataframe.iloc[idx]['combined_embedding']).float()\n",
    "        image_tensor = torch.load(self.dataframe.iloc[idx]['image_path']).float()\n",
    "        return embedding, image_tensor\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = StickerDataset(df)\n",
    "# Splitting data to training and testing sets\n",
    "train_samples = int(round(len(dataset)*0.90))\n",
    "train_set, val_set = random_split(dataset, [train_samples, len(dataset) - train_samples])\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=64, shuffle=False)\n",
    "\n",
    "# Directories to save\n",
    "os.makedirs(\"../evaluation/sticker_cgan/train_output\", exist_ok=True)\n",
    "os.makedirs(\"../evaluation/sticker_cgan/val_output\", exist_ok=True)\n",
    "os.makedirs(\"../saved_models\", exist_ok=True)\n",
    "train_output_dir = \"../evaluation/sticker_cgan/train_output\"\n",
    "val_output_dir = \"../evaluation/sticker_cgan/val_output\"\n",
    "val_models_dir = \"../saved_models\"\n",
    "\n",
    "\"\"\"\n",
    "Reference: https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html\n",
    "Learning ConvTranspose2d layers for upsampling\n",
    "\"\"\"\n",
    "# Generator Model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, embedding_dim, image_channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.noise_fc = nn.Sequential(\n",
    "            nn.Linear(noise_dim, 256 * 4 * 4),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.embed_fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 256 * 4 * 4),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, image_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, noise, embed):\n",
    "        noise_features = self.noise_fc(noise).view(noise.size(0), 256, 4, 4)\n",
    "        embed_features = self.embed_fc(embed).view(embed.size(0), 256, 4, 4)\n",
    "        x = noise_features + embed_features\n",
    "        x = self.conv_blocks(x)\n",
    "        return x\n",
    "\n",
    "\"\"\"\n",
    "Reference: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "Learning Conv2d layers for downsampling\n",
    "\"\"\"\n",
    "# Discriminator Model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, embedding_dim, image_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.Conv2d(image_channels + 1, 64, kernel_size=4, stride=2, padding=1),  # added +1\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.1),  # Dropout to weaken discriminator\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.1),  # Dropout to weaken discriminator\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.1),  # Dropout to weaken discriminator\n",
    "        )\n",
    "        self.fc = nn.Linear(256 * 4 * 4, 1)\n",
    "        # Project embedding into a spatial format\n",
    "        self.embed_fc = nn.Linear(embedding_dim, 4 * 4)  # Map embeddings to 4x4 spatial size\n",
    "        \n",
    "    def forward(self, img, embed):\n",
    "        batch_size = img.size(0)\n",
    "        # Convert embedding into spatial form\n",
    "        embed_features = self.embed_fc(embed).view(batch_size, 1, 4, 4)\n",
    "        # Resize embedding map to match image dimensions\n",
    "        embed_features = torch.nn.functional.interpolate(embed_features, size=(img.shape[2], img.shape[3]))\n",
    "        # Concatenate embeddings as an extra channel\n",
    "        x = torch.cat((img, embed_features), dim=1)\n",
    "        x = self.conv_blocks(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        return torch.sigmoid(self.fc(x))\n",
    "\n",
    "# Model Initialization\n",
    "noise_dim = 100\n",
    "embedding_dim = len(df['combined_embedding'][0])\n",
    "generator = Generator(noise_dim, embedding_dim).to(device)\n",
    "discriminator = Discriminator(embedding_dim).to(device)\n",
    "gamma = 10.0  # R1 regularization coefficient\n",
    "\n",
    "patience = 5  # Number of epochs to wait before early stopping\n",
    "lr_patience = 1  # Number of epochs to wait before reducing learning rate\n",
    "lr_factor = 0.5  # Factor to reduce learning rate by\n",
    "min_lr = 1e-6  # Minimum learning rate threshold\n",
    "\n",
    "best_fid_score = float('inf')\n",
    "epochs_since_improvement = 0\n",
    "epochs_since_lr_reduce = 0\n",
    "\n",
    "# Loss and Optimizers\n",
    "criterion = nn.BCELoss()\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "\n",
    "# Prepare InceptionV3 model for FID calculation using the new weights API.\n",
    "inception_model = models.inception_v3(weights=Inception_V3_Weights.IMAGENET1K_V1,\n",
    "                                        transform_input=False,\n",
    "                                        aux_logits=True).to(device)\n",
    "# Replace the final fully connected layer with an identity so that we get features\n",
    "inception_model.fc = nn.Identity()\n",
    "inception_model.eval()\n",
    "\n",
    "def get_inception_features(images, model):\n",
    "    \"\"\"\n",
    "    Resizes images to 299x299, normalizes them with Inception's mean and std,\n",
    "    and returns the features from the model.\n",
    "    \"\"\"\n",
    "    # Resize to InceptionV3 expected input size\n",
    "    images = torch.nn.functional.interpolate(images, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "    # If images are from generator (range [-1, 1]), convert them to [0, 1]\n",
    "    images = (images + 1) / 2\n",
    "    # Normalize with ImageNet statistics\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    images = torch.stack([normalize(img) for img in images])\n",
    "    with torch.no_grad():\n",
    "        features = model(images.to(device))\n",
    "        # If model returns a tuple (due to aux logits), take the first element.\n",
    "        if isinstance(features, tuple):\n",
    "            features = features[0]\n",
    "        features = features.detach().cpu().numpy()\n",
    "    return features\n",
    "\n",
    "def compute_fid(real_images, generated_images, model):\n",
    "    \"\"\"\n",
    "    Computes the Frechet Inception Distance (FID) between two sets of images.\n",
    "    \"\"\"\n",
    "    # Get inception features for real and generated images\n",
    "    real_features = get_inception_features(real_images, model)\n",
    "    fake_features = get_inception_features(generated_images, model)\n",
    "    \n",
    "    # Compute mean and covariance statistics\n",
    "    mu_real = np.mean(real_features, axis=0)\n",
    "    sigma_real = np.cov(real_features, rowvar=False)\n",
    "    mu_fake = np.mean(fake_features, axis=0)\n",
    "    sigma_fake = np.cov(fake_features, rowvar=False)\n",
    "    \n",
    "    # Compute squared difference between means\n",
    "    diff = mu_real - mu_fake\n",
    "    diff_squared = diff.dot(diff)\n",
    "    \n",
    "    # Compute sqrt of product of covariance matrices\n",
    "    covmean, _ = sqrtm(sigma_real.dot(sigma_fake), disp=False)\n",
    "    # If the product is almost singular, sqrtm may return complex numbers\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    \n",
    "    fid = diff_squared + np.trace(sigma_real + sigma_fake - 2 * covmean)\n",
    "    return fid\n",
    "\n",
    "num_epochs = 5000\n",
    "save_interval = int(num_epochs / 50)\n",
    "d_losses, g_losses, d_fake_losses, d_real_losses = [], [], [], []\n",
    "fid_scores = []\n",
    "fid_epochs = []\n",
    "all_fid_scores = []\n",
    "all_generated_images = []\n",
    "all_real_images = []\n",
    "all_epochs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    for i, (combined_embeddings, real_images) in enumerate(train_loader):\n",
    "        combined_embeddings = combined_embeddings.to(device)\n",
    "        real_images = real_images.to(device)\n",
    "        # Ensure real images require gradients for R1 penalty computation.\n",
    "        real_images.requires_grad_()\n",
    "        \n",
    "        # Train Discriminator\n",
    "        noise = torch.randn(real_images.size(0), noise_dim).to(device)\n",
    "        fake_images = generator(noise, combined_embeddings)\n",
    "        \n",
    "        real_labels = torch.full((real_images.size(0), 1), 0.95).to(device)\n",
    "        fake_labels = torch.full((real_images.size(0), 1), 0.05).to(device)\n",
    "        \n",
    "        # Forward pass on real images.\n",
    "        real_outputs = discriminator(real_images, combined_embeddings)\n",
    "        d_loss_real = criterion(real_outputs, real_labels)\n",
    "        \n",
    "        # Compute R1 regularization: gradient penalty on real images.\n",
    "        grad_real = torch.autograd.grad(\n",
    "            outputs=real_outputs.sum(), \n",
    "            inputs=real_images, \n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True\n",
    "        )[0]\n",
    "        grad_penalty = grad_real.view(grad_real.size(0), -1).pow(2).sum(1).mean()\n",
    "        d_loss_real = d_loss_real + (gamma / 2) * grad_penalty\n",
    "        \n",
    "        # Forward pass on fake images.\n",
    "        fake_outputs = discriminator(fake_images.detach(), combined_embeddings)\n",
    "        d_loss_fake = criterion(fake_outputs, fake_labels)\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        \n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # Train Generator\n",
    "        fake_outputs = discriminator(fake_images, combined_embeddings)\n",
    "        g_loss = criterion(fake_outputs, real_labels)\n",
    "        \n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        # Turn off gradients for real_images after update.\n",
    "        real_images.requires_grad_(False)\n",
    "    \n",
    "    # Store loss values\n",
    "    d_losses.append(d_loss.item())\n",
    "    g_losses.append(g_loss.item())\n",
    "    d_real_losses.append(d_loss_real.item())\n",
    "    d_fake_losses.append(d_loss_fake.item())\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] | D Loss: {d_loss:.4f} | G Loss: {g_loss.item():.4f}\")\n",
    "    \n",
    "    # Save evaluation every save_interval\n",
    "    if (epoch + 1) % save_interval == 0 or (epoch + 1) == num_epochs:\n",
    "        os.makedirs(train_output_dir, exist_ok=True)\n",
    "        os.makedirs(val_output_dir, exist_ok=True)\n",
    "\n",
    "        generator.eval()\n",
    "\n",
    "        # --- Evaluate on Training Set ---\n",
    "        train_batch = next(iter(train_loader))\n",
    "        train_embeddings, train_images = train_batch\n",
    "        train_embeddings = train_embeddings.to(device)\n",
    "        noise_train = torch.randn(train_images.size(0), noise_dim).to(device)\n",
    "        with torch.no_grad():\n",
    "            generated_train = generator(noise_train, train_embeddings).cpu()\n",
    "        grid_train = make_grid(generated_train, nrow=8, normalize=True)\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(np.transpose(grid_train.numpy(), (1, 2, 0)))\n",
    "        plt.title(f\"Train Set Generated Images at Epoch {epoch+1}\")\n",
    "        plt.axis(\"off\")\n",
    "        train_image_path = os.path.join(train_output_dir, f\"generated_train_epoch_{epoch+1}.png\")\n",
    "        plt.savefig(train_image_path)\n",
    "        plt.close()\n",
    "\n",
    "        # --- Plot Loss Curves ---\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, len(g_losses) + 1), g_losses, label=\"G Loss\")\n",
    "        plt.plot(range(1, len(d_losses) + 1), d_losses, label=\"D Loss\")\n",
    "        plt.xlabel(\"Epoch (save interval count)\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(f\"Training Losses up to Epoch {epoch+1}\")\n",
    "        plt.legend()\n",
    "        loss_plot_path = os.path.join(train_output_dir, f\"loss_plot_epoch_{epoch+1}.png\")\n",
    "        plt.savefig(loss_plot_path)\n",
    "        plt.close()\n",
    "\n",
    "        # --- Evaluate on Validation Set ---\n",
    "        val_batch = next(iter(val_loader))\n",
    "        val_embeddings, val_images = val_batch\n",
    "        val_embeddings = val_embeddings.to(device)\n",
    "        noise_val = torch.randn(val_images.size(0), noise_dim).to(device)\n",
    "        with torch.no_grad():\n",
    "            generated_val = generator(noise_val, val_embeddings).cpu()\n",
    "        grid_val = make_grid(generated_val, nrow=8, normalize=True)\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(np.transpose(grid_val.numpy(), (1, 2, 0)))\n",
    "        plt.title(f\"Validation Set Generated Images at Epoch {epoch+1}\")\n",
    "        plt.axis(\"off\")\n",
    "        val_image_path = os.path.join(val_output_dir, f\"generated_val_epoch_{epoch+1}.png\")\n",
    "        plt.savefig(val_image_path)\n",
    "        plt.close()\n",
    "\n",
    "        # --- Compute FID Score on Validation Set ---\n",
    "        # Note: using the current batch from the validation set for demonstration.\n",
    "        fid_score = compute_fid(val_images.cpu(), generated_val, inception_model)\n",
    "        fid_scores.append(fid_score)\n",
    "        fid_epochs.append(epoch+1)\n",
    "        print(f\"Epoch {epoch+1} FID Score: {fid_score:.4f}\")\n",
    "\n",
    "        all_fid_scores.append(fid_score)\n",
    "        all_generated_images.append(generated_val.cpu())\n",
    "        all_real_images.append(val_images.cpu())\n",
    "        all_epochs.append(epoch + 1)\n",
    "        \n",
    "        # Early Stopping and LR Reduction Logic\n",
    "        if fid_score < best_fid_score and epoch > 2000:\n",
    "            best_fid_score = fid_score\n",
    "            epochs_since_improvement = 0\n",
    "            epochs_since_lr_reduce = 0\n",
    "    \n",
    "            # Save the best models\n",
    "            torch.save(generator.state_dict(), os.path.join(val_models_dir, \"cgan_sticker_generator.pth\"))\n",
    "            torch.save(discriminator.state_dict(), os.path.join(val_models_dir, \"cgan_sticker_discriminator.pth\"))\n",
    "            print(f\"Saved improved model at Epoch {epoch+1} with FID {fid_score:.4f}\")\n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "            epochs_since_lr_reduce += 1\n",
    "            \n",
    "        # Reduce learning rate if no improvement for 'lr_patience' epochs\n",
    "        if epochs_since_lr_reduce >= lr_patience and epoch > 2000:\n",
    "            new_g_lr = max(g_optimizer.param_groups[0]['lr'] * lr_factor, min_lr)\n",
    "            new_d_lr = max(d_optimizer.param_groups[0]['lr'] * lr_factor, min_lr)\n",
    "    \n",
    "            for param_group in g_optimizer.param_groups:\n",
    "                param_group['lr'] = new_g_lr\n",
    "            for param_group in d_optimizer.param_groups:\n",
    "                param_group['lr'] = new_d_lr\n",
    "    \n",
    "            print(f\"Reducing learning rates to Generator: {new_g_lr}, Discriminator: {new_d_lr}\")\n",
    "    \n",
    "            epochs_since_lr_reduce = 0\n",
    "    \n",
    "        # Early stopping if no improvement for 'patience' epochs\n",
    "        if epochs_since_improvement >= patience and epoch > 2000:\n",
    "            print(f\"No improvement in FID for {patience} intervals. Stopping early at Epoch {epoch+1}.\")\n",
    "            break\n",
    "\n",
    "        generator.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c180ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NVIDIA GPU (CUDA)\n",
      "Saved generated Stickers plot at: ../evaluation/sticker_cgan/val_output\\stickers_from_prompts.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import models, transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "# Generator model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, embedding_dim=512, image_channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.noise_fc = nn.Sequential(\n",
    "            nn.Linear(noise_dim, 256 * 4 * 4),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # embed_transform layer should accept embedding_dim=512 from CLIP\n",
    "        self.embed_transform = nn.Linear(embedding_dim, 384)  # embedding_dim = 512 to match CLIP output\n",
    "\n",
    "        self.embed_fc = nn.Sequential(\n",
    "            nn.Linear(384, 256 * 4 * 4),  # Adjusted to match embed_transform output size\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, image_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, noise, embed):\n",
    "        noise_features = self.noise_fc(noise).view(noise.size(0), 256, 4, 4)\n",
    "        embed = self.embed_transform(embed)  # Apply transformation\n",
    "        embed_features = self.embed_fc(embed).view(embed.size(0), 256, 4, 4)\n",
    "        x = noise_features + embed_features\n",
    "        x = self.conv_blocks(x)\n",
    "        return x\n",
    "\n",
    "# Device Configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Using NVIDIA GPU (CUDA)')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print('Using Mac GPU (MPS)')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using CPU')\n",
    "\n",
    "# Define noise dimension (100 as in the original model)\n",
    "noise_dim = 100\n",
    "\n",
    "# Define your prompts\n",
    "prompts = [\n",
    "    \"A cute smiling cat\",\n",
    "    \"A colorful rainbow\",\n",
    "    \"A fun pineapple with sunglasses\",\n",
    "    \"A cartoonish cactus with flowers\",\n",
    "    \"A happy ice cream cone with sprinkles\"\n",
    "]\n",
    "\n",
    "generated_images = []\n",
    "\n",
    "# Load CLIP's tokenizer and text model\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "\n",
    "clip_tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model = clip_model.to(device)\n",
    "clip_model.eval()\n",
    "\n",
    "# Load the pre-trained generator model with strict=False to ignore missing layers\n",
    "generator = Generator(noise_dim, embedding_dim=512).to(device)\n",
    "generator.load_state_dict(torch.load(\"../saved_models/cgan_sticker_generator.pth\"), strict=False)\n",
    "\n",
    "# Initialize missing weights for embed_transform manually\n",
    "init.kaiming_normal_(generator.embed_transform.weight, a=0.2, mode='fan_in', nonlinearity='leaky_relu')\n",
    "if generator.embed_transform.bias is not None:\n",
    "    init.zeros_(generator.embed_transform.bias)\n",
    "\n",
    "generator.eval()  # Set the generator to evaluation mode\n",
    "\n",
    "# Function to mean pool token embeddings (used in CLIP)\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    \"\"\"Mean pool the token embeddings.\"\"\"\n",
    "    token_embeddings = model_output.last_hidden_state  # (batch_size, sequence_length, hidden_dim)\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, dim=1) / torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
    "\n",
    "# Function to get text embeddings using CLIP\n",
    "def embed_text(text):\n",
    "    if pd.isna(text) or text.strip() == \"\":\n",
    "        # Adjust the zero vector size to match CLIP's output dimension (512 for clip-vit-base-patch32)\n",
    "        return np.zeros(512, dtype=np.float32)\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    inputs = clip_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=77)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    \n",
    "    # Disable gradients for inference\n",
    "    with torch.no_grad():\n",
    "        output = clip_model(**inputs)\n",
    "    \n",
    "    # Pool the token embeddings (mean pooling)\n",
    "    pooled_embedding = mean_pooling(output, inputs[\"attention_mask\"])\n",
    "    \n",
    "    # Optionally, you might want to L2 normalize the pooled embedding:\n",
    "    pooled_embedding = torch.nn.functional.normalize(pooled_embedding, p=2, dim=-1)\n",
    "    \n",
    "    return pooled_embedding.squeeze().cpu().numpy().astype(np.float32)\n",
    "\n",
    "# Generate an emoji for each prompt using the Hugging Face CLIP resources\n",
    "for prompt in prompts:\n",
    "    # Tokenize the prompt using the CLIPTokenizer and get the embedding as a numpy array\n",
    "    text_embedding = embed_text(prompt)\n",
    "    \n",
    "    # Convert the numpy array back to a torch tensor and add the batch dimension\n",
    "    text_embedding = torch.tensor(text_embedding).to(device).unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    # Generate a random noise vector\n",
    "    noise = torch.randn(1, noise_dim).to(device)\n",
    "    \n",
    "    # Generate the emoji image using the generator\n",
    "    with torch.no_grad():\n",
    "        gen_image = generator(noise, text_embedding)  # Feed text embedding into the generator\n",
    "        gen_image = gen_image.cpu()  # move to CPU for plotting\n",
    "    \n",
    "    generated_images.append(gen_image)\n",
    "\n",
    "# Function to convert a tensor image to a numpy image (assumes [1, C, H, W] in range [-1, 1])\n",
    "def tensor_to_image(tensor):\n",
    "    image = tensor.squeeze(0)          # remove batch dimension\n",
    "    image = (image + 1) / 2            # scale to [0, 1]\n",
    "    image = image.permute(1, 2, 0).numpy()  # convert to HWC format\n",
    "    return np.clip(image, 0, 1)\n",
    "\n",
    "# Plot the generated emojis along with their corresponding prompts\n",
    "fig, axs = plt.subplots(1, len(prompts), figsize=(20, 4))\n",
    "for i, (prompt, gen_img) in enumerate(zip(prompts, generated_images)):\n",
    "    img_np = tensor_to_image(gen_img)\n",
    "    axs[i].imshow(img_np)\n",
    "    axs[i].set_title(prompt, fontsize=10)\n",
    "    axs[i].axis(\"off\")\n",
    "plt.suptitle(\"Stickers Generated from Text Prompts\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "# Save the figure to disk\n",
    "save_path = os.path.join(\"../evaluation/sticker_cgan/val_output\", \"stickers_from_prompts.png\")\n",
    "plt.savefig(save_path)\n",
    "print(f\"Saved generated Stickers plot at: {save_path}\")\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
